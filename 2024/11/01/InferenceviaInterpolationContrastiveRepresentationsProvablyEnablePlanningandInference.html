<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference</title><!-- Begin Jekyll SEO tag v2.7.1 -->
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference" />
<meta name="author" content="Benjamin Eysenbach, Vivek Myers, Ruslan Salakhutdinov, Sergey Levine" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Given time series data, how can we answer questions like “what will happen in the future?” and “how did we get here?” These sorts of probabilistic inference questions are challenging when observations are high-dimensional. In this paper, we show how these questions can have compact, closed form solutions in terms of learned representations. The key idea is to apply a variant of contrastive learning to time series data. Prior work already shows that the representations learned by contrastive learning encode a probability ratio. By extending prior work to show that the marginal distribution over representations is Gaussian, we can then prove that joint distribution of representations is also Gaussian. Taken together, these results show that representations learned via temporal contrastive learning follow a Gauss-Markov chain, a graphical model where inference (e.g., prediction, planning) over representations corresponds to inverting a low-dimensional matrix. In one special case, inferring intermediate representations will be equivalent to interpolating between the learned representations. We validate our theory using numerical simulations on tasks up to 46-dimensions." />
<meta property="og:description" content="Given time series data, how can we answer questions like “what will happen in the future?” and “how did we get here?” These sorts of probabilistic inference questions are challenging when observations are high-dimensional. In this paper, we show how these questions can have compact, closed form solutions in terms of learned representations. The key idea is to apply a variant of contrastive learning to time series data. Prior work already shows that the representations learned by contrastive learning encode a probability ratio. By extending prior work to show that the marginal distribution over representations is Gaussian, we can then prove that joint distribution of representations is also Gaussian. Taken together, these results show that representations learned via temporal contrastive learning follow a Gauss-Markov chain, a graphical model where inference (e.g., prediction, planning) over representations corresponds to inverting a low-dimensional matrix. In one special case, inferring intermediate representations will be equivalent to interpolating between the learned representations. We validate our theory using numerical simulations on tasks up to 46-dimensions." />
<link rel="canonical" href="https://dedzago.github.io/arxiv_rss/2024/11/01/InferenceviaInterpolationContrastiveRepresentationsProvablyEnablePlanningandInference.html" />
<meta property="og:url" content="https://dedzago.github.io/arxiv_rss/2024/11/01/InferenceviaInterpolationContrastiveRepresentationsProvablyEnablePlanningandInference.html" />
<meta property="og:site_name" content="Stat Arxiv of Today" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-11-01T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference" />
<script type="application/ld+json">
{"description":"Given time series data, how can we answer questions like “what will happen in the future?” and “how did we get here?” These sorts of probabilistic inference questions are challenging when observations are high-dimensional. In this paper, we show how these questions can have compact, closed form solutions in terms of learned representations. The key idea is to apply a variant of contrastive learning to time series data. Prior work already shows that the representations learned by contrastive learning encode a probability ratio. By extending prior work to show that the marginal distribution over representations is Gaussian, we can then prove that joint distribution of representations is also Gaussian. Taken together, these results show that representations learned via temporal contrastive learning follow a Gauss-Markov chain, a graphical model where inference (e.g., prediction, planning) over representations corresponds to inverting a low-dimensional matrix. In one special case, inferring intermediate representations will be equivalent to interpolating between the learned representations. We validate our theory using numerical simulations on tasks up to 46-dimensions.","mainEntityOfPage":{"@type":"WebPage","@id":"https://dedzago.github.io/arxiv_rss/2024/11/01/InferenceviaInterpolationContrastiveRepresentationsProvablyEnablePlanningandInference.html"},"@type":"BlogPosting","author":{"@type":"Person","name":"Benjamin Eysenbach, Vivek Myers, Ruslan Salakhutdinov, Sergey Levine"},"headline":"Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference","url":"https://dedzago.github.io/arxiv_rss/2024/11/01/InferenceviaInterpolationContrastiveRepresentationsProvablyEnablePlanningandInference.html","dateModified":"2024-11-01T00:00:00+00:00","datePublished":"2024-11-01T00:00:00+00:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://dedzago.github.io/arxiv_rss/feed.xml" title="Stat Arxiv of Today" /><link rel="shortcut icon" type="image/x-icon" href="" />
  <link rel="stylesheet" href="/arxiv_rss/assets/css/main.css" />


<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</head>
<body a="light">
    <main class="page-content" aria-label="Content">
      <div class="w">
        <a href="/arxiv_rss/">..</a><article>
  <p class="post-meta">
    <time datetime="2024-11-01 00:00:00 +0000">11-01</time>
  </p>
  
  <h1>Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference</h1>
  <br>Benjamin Eysenbach, Vivek Myers, Ruslan Salakhutdinov, Sergey Levine</h3>
  <br> [stat.ML]

  <p>Given time series data, how can we answer questions like “what will happen in the future?” and “how did we get here?” These sorts of probabilistic inference questions are challenging when observations are high-dimensional. In this paper, we show how these questions can have compact, closed form solutions in terms of learned representations. The key idea is to apply a variant of contrastive learning to time series data. Prior work already shows that the representations learned by contrastive learning encode a probability ratio. By extending prior work to show that the marginal distribution over representations is Gaussian, we can then prove that joint distribution of representations is also Gaussian. Taken together, these results show that representations learned via temporal contrastive learning follow a Gauss-Markov chain, a graphical model where inference (e.g., prediction, planning) over representations corresponds to inverting a low-dimensional matrix. In one special case, inferring intermediate representations will be equivalent to interpolating between the learned representations. We validate our theory using numerical simulations on tasks up to 46-dimensions.</p>

<p><a href="https://arxiv.org/abs/2403.04082">Read more</a></p>

</article>

      </div>
    </main>
  </body>
</html>