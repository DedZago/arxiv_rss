<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Large-scale cloze evaluation reveals that token prediction tasks are neither lexically nor semantically aligned</title><!-- Begin Jekyll SEO tag v2.7.1 -->
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Large-scale cloze evaluation reveals that token prediction tasks are neither lexically nor semantically aligned" />
<meta name="author" content="Cassandra L. Jacobs, Loïc Grobol, Alvin Tsang" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In this work we compare the generative behavior at the next token prediction level in several language models by comparing them to human productions in the cloze task. We find that while large models trained for longer are typically better estimators of human productions, but they reliably under-estimate the probabilities of human responses, over-rank rare responses, under-rank top responses, and produce highly distinct semantic spaces. Altogether, this work demonstrates in a tractable, interpretable domain that LM generations can not be used as replacements of or models of the cloze task." />
<meta property="og:description" content="In this work we compare the generative behavior at the next token prediction level in several language models by comparing them to human productions in the cloze task. We find that while large models trained for longer are typically better estimators of human productions, but they reliably under-estimate the probabilities of human responses, over-rank rare responses, under-rank top responses, and produce highly distinct semantic spaces. Altogether, this work demonstrates in a tractable, interpretable domain that LM generations can not be used as replacements of or models of the cloze task." />
<link rel="canonical" href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/Largescaleclozeevaluationrevealsthattokenpredictiontasksareneitherlexicallynorsemanticallyaligned.html" />
<meta property="og:url" content="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/Largescaleclozeevaluationrevealsthattokenpredictiontasksareneitherlexicallynorsemanticallyaligned.html" />
<meta property="og:site_name" content="Stat Arxiv of Today" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-10-29T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Large-scale cloze evaluation reveals that token prediction tasks are neither lexically nor semantically aligned" />
<script type="application/ld+json">
{"description":"In this work we compare the generative behavior at the next token prediction level in several language models by comparing them to human productions in the cloze task. We find that while large models trained for longer are typically better estimators of human productions, but they reliably under-estimate the probabilities of human responses, over-rank rare responses, under-rank top responses, and produce highly distinct semantic spaces. Altogether, this work demonstrates in a tractable, interpretable domain that LM generations can not be used as replacements of or models of the cloze task.","author":{"@type":"Person","name":"Cassandra L. Jacobs, Loïc Grobol, Alvin Tsang"},"datePublished":"2024-10-29T00:00:00+00:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/Largescaleclozeevaluationrevealsthattokenpredictiontasksareneitherlexicallynorsemanticallyaligned.html"},"url":"https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/Largescaleclozeevaluationrevealsthattokenpredictiontasksareneitherlexicallynorsemanticallyaligned.html","headline":"Large-scale cloze evaluation reveals that token prediction tasks are neither lexically nor semantically aligned","@type":"BlogPosting","dateModified":"2024-10-29T00:00:00+00:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" title="Stat Arxiv of Today" /><link rel="shortcut icon" type="image/x-icon" href="" />
  <link rel="stylesheet" href="/arxiv_rss/assets/css/main.css" />


<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</head>
<body a="light">
    <main class="page-content" aria-label="Content">
      <div class="w">
        <a href="/arxiv_rss/">..</a><article>
  <p class="post-meta">
    <time datetime="2024-10-29 00:00:00 +0000">10-29</time>
  </p>
  
  <h1>Large-scale cloze evaluation reveals that token prediction tasks are neither lexically nor semantically aligned</h1>
  <br>Cassandra L. Jacobs, Loïc Grobol, Alvin Tsang</h3>
  <br> []

  <p>In this work we compare the generative behavior at the next token prediction level in several language models by comparing them to human productions in the cloze task. We find that while large models trained for longer are typically better estimators of human productions, but they reliably under-estimate the probabilities of human responses, over-rank rare responses, under-rank top responses, and produce highly distinct semantic spaces. Altogether, this work demonstrates in a tractable, interpretable domain that LM generations can not be used as replacements of or models of the cloze task.</p>

<p><a href="https://arxiv.org/abs/2410.12057">Read more</a></p>

</article>

      </div>
    </main>
  </body>
</html>