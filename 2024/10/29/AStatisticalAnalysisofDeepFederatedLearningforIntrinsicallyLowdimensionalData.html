<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>A Statistical Analysis of Deep Federated Learning for Intrinsically Low-dimensional Data</title><!-- Begin Jekyll SEO tag v2.7.1 -->
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="A Statistical Analysis of Deep Federated Learning for Intrinsically Low-dimensional Data" />
<meta name="author" content="Saptarshi Chakraborty, Peter L. Bartlett" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Federated Learning (FL) has emerged as a groundbreaking paradigm in collaborative machine learning, emphasizing decentralized model training to address data privacy concerns. While significant progress has been made in optimizing federated learning, the exploration of generalization error, particularly in heterogeneous settings, has been limited, focusing mainly on parametric cases. This paper investigates the generalization properties of deep federated regression within a two-stage sampling model. Our findings highlight that the intrinsic dimension, defined by the entropic dimension, is crucial for determining convergence rates when appropriate network sizes are used. Specifically, if the true relationship between response and explanatory variables is charecterized by a $\beta$-H&quot;older function and there are $n$ independent and identically distributed (i.i.d.) samples from $m$ participating clients, the error rate for participating clients scales at most as $\tilde{O}\left((mn)^{-2\beta/(2\beta + \bar{d}{2\beta}(\lambda))}\right)$, and for non-participating clients, it scales as $\tilde{O}\left(\Delta \cdot m^{-2\beta/(2\beta + \bar{d}{2\beta}(\lambda))} + (mn)^{-2\beta/(2\beta + \bar{d}{2\beta}(\lambda))}\right)$. Here, $\bar{d}{2\beta}(\lambda)$ represents the $2\beta$-entropic dimension of $\lambda$, the marginal distribution of the explanatory variables, and $\Delta$ characterizes the dependence between the sampling stages. Our results explicitly account for the “closeness” of clients, demonstrating that the convergence rates of deep federated learners depend on intrinsic rather than nominal high-dimensionality." />
<meta property="og:description" content="Federated Learning (FL) has emerged as a groundbreaking paradigm in collaborative machine learning, emphasizing decentralized model training to address data privacy concerns. While significant progress has been made in optimizing federated learning, the exploration of generalization error, particularly in heterogeneous settings, has been limited, focusing mainly on parametric cases. This paper investigates the generalization properties of deep federated regression within a two-stage sampling model. Our findings highlight that the intrinsic dimension, defined by the entropic dimension, is crucial for determining convergence rates when appropriate network sizes are used. Specifically, if the true relationship between response and explanatory variables is charecterized by a $\beta$-H&quot;older function and there are $n$ independent and identically distributed (i.i.d.) samples from $m$ participating clients, the error rate for participating clients scales at most as $\tilde{O}\left((mn)^{-2\beta/(2\beta + \bar{d}{2\beta}(\lambda))}\right)$, and for non-participating clients, it scales as $\tilde{O}\left(\Delta \cdot m^{-2\beta/(2\beta + \bar{d}{2\beta}(\lambda))} + (mn)^{-2\beta/(2\beta + \bar{d}{2\beta}(\lambda))}\right)$. Here, $\bar{d}{2\beta}(\lambda)$ represents the $2\beta$-entropic dimension of $\lambda$, the marginal distribution of the explanatory variables, and $\Delta$ characterizes the dependence between the sampling stages. Our results explicitly account for the “closeness” of clients, demonstrating that the convergence rates of deep federated learners depend on intrinsic rather than nominal high-dimensionality." />
<link rel="canonical" href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AStatisticalAnalysisofDeepFederatedLearningforIntrinsicallyLowdimensionalData.html" />
<meta property="og:url" content="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AStatisticalAnalysisofDeepFederatedLearningforIntrinsicallyLowdimensionalData.html" />
<meta property="og:site_name" content="Stat Arxiv of Today" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-10-29T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="A Statistical Analysis of Deep Federated Learning for Intrinsically Low-dimensional Data" />
<script type="application/ld+json">
{"description":"Federated Learning (FL) has emerged as a groundbreaking paradigm in collaborative machine learning, emphasizing decentralized model training to address data privacy concerns. While significant progress has been made in optimizing federated learning, the exploration of generalization error, particularly in heterogeneous settings, has been limited, focusing mainly on parametric cases. This paper investigates the generalization properties of deep federated regression within a two-stage sampling model. Our findings highlight that the intrinsic dimension, defined by the entropic dimension, is crucial for determining convergence rates when appropriate network sizes are used. Specifically, if the true relationship between response and explanatory variables is charecterized by a $\\beta$-H&quot;older function and there are $n$ independent and identically distributed (i.i.d.) samples from $m$ participating clients, the error rate for participating clients scales at most as $\\tilde{O}\\left((mn)^{-2\\beta/(2\\beta + \\bar{d}{2\\beta}(\\lambda))}\\right)$, and for non-participating clients, it scales as $\\tilde{O}\\left(\\Delta \\cdot m^{-2\\beta/(2\\beta + \\bar{d}{2\\beta}(\\lambda))} + (mn)^{-2\\beta/(2\\beta + \\bar{d}{2\\beta}(\\lambda))}\\right)$. Here, $\\bar{d}{2\\beta}(\\lambda)$ represents the $2\\beta$-entropic dimension of $\\lambda$, the marginal distribution of the explanatory variables, and $\\Delta$ characterizes the dependence between the sampling stages. Our results explicitly account for the “closeness” of clients, demonstrating that the convergence rates of deep federated learners depend on intrinsic rather than nominal high-dimensionality.","author":{"@type":"Person","name":"Saptarshi Chakraborty, Peter L. Bartlett"},"datePublished":"2024-10-29T00:00:00+00:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AStatisticalAnalysisofDeepFederatedLearningforIntrinsicallyLowdimensionalData.html"},"url":"https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AStatisticalAnalysisofDeepFederatedLearningforIntrinsicallyLowdimensionalData.html","headline":"A Statistical Analysis of Deep Federated Learning for Intrinsically Low-dimensional Data","@type":"BlogPosting","dateModified":"2024-10-29T00:00:00+00:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" title="Stat Arxiv of Today" /><link rel="shortcut icon" type="image/x-icon" href="" />
  <link rel="stylesheet" href="/arxiv_rss/assets/css/main.css" />


<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</head>
<body a="light">
    <main class="page-content" aria-label="Content">
      <div class="w">
        <a href="/arxiv_rss/">..</a><article>
  <p class="post-meta">
    <time datetime="2024-10-29 00:00:00 +0000">10-29</time>
  </p>
  
  <h1>A Statistical Analysis of Deep Federated Learning for Intrinsically Low-dimensional Data</h1>
  <br>Saptarshi Chakraborty, Peter L. Bartlett</h3>
  <br> [stat.ML,stat.TH]

  <p>Federated Learning (FL) has emerged as a groundbreaking paradigm in collaborative machine learning, emphasizing decentralized model training to address data privacy concerns. While significant progress has been made in optimizing federated learning, the exploration of generalization error, particularly in heterogeneous settings, has been limited, focusing mainly on parametric cases. This paper investigates the generalization properties of deep federated regression within a two-stage sampling model. Our findings highlight that the intrinsic dimension, defined by the entropic dimension, is crucial for determining convergence rates when appropriate network sizes are used. Specifically, if the true relationship between response and explanatory variables is charecterized by a $\beta$-H"older function and there are $n$ independent and identically distributed (i.i.d.) samples from $m$ participating clients, the error rate for participating clients scales at most as $\tilde{O}\left((mn)^{-2\beta/(2\beta + \bar{d}<em>{2\beta}(\lambda))}\right)$, and for non-participating clients, it scales as $\tilde{O}\left(\Delta \cdot m^{-2\beta/(2\beta + \bar{d}</em>{2\beta}(\lambda))} + (mn)^{-2\beta/(2\beta + \bar{d}<em>{2\beta}(\lambda))}\right)$. Here, $\bar{d}</em>{2\beta}(\lambda)$ represents the $2\beta$-entropic dimension of $\lambda$, the marginal distribution of the explanatory variables, and $\Delta$ characterizes the dependence between the sampling stages. Our results explicitly account for the “closeness” of clients, demonstrating that the convergence rates of deep federated learners depend on intrinsic rather than nominal high-dimensionality.</p>

<p><a href="https://arxiv.org/abs/2410.20659">Read more</a></p>

</article>

      </div>
    </main>
  </body>
</html>