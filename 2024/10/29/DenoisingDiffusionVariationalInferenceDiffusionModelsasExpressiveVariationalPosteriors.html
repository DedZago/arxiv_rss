<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Denoising Diffusion Variational Inference: Diffusion Models as Expressive Variational Posteriors</title><!-- Begin Jekyll SEO tag v2.7.1 -->
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Denoising Diffusion Variational Inference: Diffusion Models as Expressive Variational Posteriors" />
<meta name="author" content="Top Piriyakulkij, Yingheng Wang, Volodymyr Kuleshov" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="We propose denoising diffusion variational inference (DDVI), a black-box variational inference algorithm for latent variable models which relies on diffusion models as flexible approximate posteriors. Specifically, our method introduces an expressive class of diffusion-based variational posteriors that perform iterative refinement in latent space; we train these posteriors with a novel regularized evidence lower bound (ELBO) on the marginal likelihood inspired by the wake-sleep algorithm. Our method is easy to implement (it fits a regularized extension of the ELBO), is compatible with black-box variational inference, and outperforms alternative classes of approximate posteriors based on normalizing flows or adversarial networks. We find that DDVI improves inference and learning in deep latent variable models across common benchmarks as well as on a motivating task in biology – inferring latent ancestry from human genomes – where it outperforms strong baselines on the Thousand Genomes dataset." />
<meta property="og:description" content="We propose denoising diffusion variational inference (DDVI), a black-box variational inference algorithm for latent variable models which relies on diffusion models as flexible approximate posteriors. Specifically, our method introduces an expressive class of diffusion-based variational posteriors that perform iterative refinement in latent space; we train these posteriors with a novel regularized evidence lower bound (ELBO) on the marginal likelihood inspired by the wake-sleep algorithm. Our method is easy to implement (it fits a regularized extension of the ELBO), is compatible with black-box variational inference, and outperforms alternative classes of approximate posteriors based on normalizing flows or adversarial networks. We find that DDVI improves inference and learning in deep latent variable models across common benchmarks as well as on a motivating task in biology – inferring latent ancestry from human genomes – where it outperforms strong baselines on the Thousand Genomes dataset." />
<link rel="canonical" href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/DenoisingDiffusionVariationalInferenceDiffusionModelsasExpressiveVariationalPosteriors.html" />
<meta property="og:url" content="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/DenoisingDiffusionVariationalInferenceDiffusionModelsasExpressiveVariationalPosteriors.html" />
<meta property="og:site_name" content="Stat Arxiv of Today" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-10-29T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Denoising Diffusion Variational Inference: Diffusion Models as Expressive Variational Posteriors" />
<script type="application/ld+json">
{"description":"We propose denoising diffusion variational inference (DDVI), a black-box variational inference algorithm for latent variable models which relies on diffusion models as flexible approximate posteriors. Specifically, our method introduces an expressive class of diffusion-based variational posteriors that perform iterative refinement in latent space; we train these posteriors with a novel regularized evidence lower bound (ELBO) on the marginal likelihood inspired by the wake-sleep algorithm. Our method is easy to implement (it fits a regularized extension of the ELBO), is compatible with black-box variational inference, and outperforms alternative classes of approximate posteriors based on normalizing flows or adversarial networks. We find that DDVI improves inference and learning in deep latent variable models across common benchmarks as well as on a motivating task in biology – inferring latent ancestry from human genomes – where it outperforms strong baselines on the Thousand Genomes dataset.","author":{"@type":"Person","name":"Top Piriyakulkij, Yingheng Wang, Volodymyr Kuleshov"},"datePublished":"2024-10-29T00:00:00+00:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/DenoisingDiffusionVariationalInferenceDiffusionModelsasExpressiveVariationalPosteriors.html"},"url":"https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/DenoisingDiffusionVariationalInferenceDiffusionModelsasExpressiveVariationalPosteriors.html","headline":"Denoising Diffusion Variational Inference: Diffusion Models as Expressive Variational Posteriors","@type":"BlogPosting","dateModified":"2024-10-29T00:00:00+00:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" title="Stat Arxiv of Today" /><link rel="shortcut icon" type="image/x-icon" href="" />
  <link rel="stylesheet" href="/arxiv_rss/assets/css/main.css" />


<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</head>
<body a="light">
    <main class="page-content" aria-label="Content">
      <div class="w">
        <a href="/arxiv_rss/">..</a><article>
  <p class="post-meta">
    <time datetime="2024-10-29 00:00:00 +0000">10-29</time>
  </p>
  
  <h1>Denoising Diffusion Variational Inference: Diffusion Models as Expressive Variational Posteriors</h1>
  <br>Top Piriyakulkij, Yingheng Wang, Volodymyr Kuleshov</h3>
  <br> [stat.ML]

  <p>We propose denoising diffusion variational inference (DDVI), a black-box variational inference algorithm for latent variable models which relies on diffusion models as flexible approximate posteriors. Specifically, our method introduces an expressive class of diffusion-based variational posteriors that perform iterative refinement in latent space; we train these posteriors with a novel regularized evidence lower bound (ELBO) on the marginal likelihood inspired by the wake-sleep algorithm. Our method is easy to implement (it fits a regularized extension of the ELBO), is compatible with black-box variational inference, and outperforms alternative classes of approximate posteriors based on normalizing flows or adversarial networks. We find that DDVI improves inference and learning in deep latent variable models across common benchmarks as well as on a motivating task in biology – inferring latent ancestry from human genomes – where it outperforms strong baselines on the Thousand Genomes dataset.</p>

<p><a href="https://arxiv.org/abs/2401.02739">Read more</a></p>

</article>

      </div>
    </main>
  </body>
</html>