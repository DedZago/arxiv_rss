<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Hierarchical Universal Value Function Approximators</title><!-- Begin Jekyll SEO tag v2.7.1 -->
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Hierarchical Universal Value Function Approximators" />
<meta name="author" content="Rushiv Arora" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="There have been key advancements to building universal approximators for multi-goal collections of reinforcement learning value functions – key elements in estimating long-term returns of states in a parameterized manner. We extend this to hierarchical reinforcement learning, using the options framework, by introducing hierarchical universal value function approximators (H-UVFAs). This allows us to leverage the added benefits of scaling, planning, and generalization expected in temporal abstraction settings. We develop supervised and reinforcement learning methods for learning embeddings of the states, goals, options, and actions in the two hierarchical value functions: $Q(s, g, o; \theta)$ and $Q(s, g, o, a; \theta)$. Finally we demonstrate generalization of the HUVFAs and show they outperform corresponding UVFAs." />
<meta property="og:description" content="There have been key advancements to building universal approximators for multi-goal collections of reinforcement learning value functions – key elements in estimating long-term returns of states in a parameterized manner. We extend this to hierarchical reinforcement learning, using the options framework, by introducing hierarchical universal value function approximators (H-UVFAs). This allows us to leverage the added benefits of scaling, planning, and generalization expected in temporal abstraction settings. We develop supervised and reinforcement learning methods for learning embeddings of the states, goals, options, and actions in the two hierarchical value functions: $Q(s, g, o; \theta)$ and $Q(s, g, o, a; \theta)$. Finally we demonstrate generalization of the HUVFAs and show they outperform corresponding UVFAs." />
<link rel="canonical" href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/HierarchicalUniversalValueFunctionApproximators.html" />
<meta property="og:url" content="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/HierarchicalUniversalValueFunctionApproximators.html" />
<meta property="og:site_name" content="Stat Arxiv of Today" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-10-29T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Hierarchical Universal Value Function Approximators" />
<script type="application/ld+json">
{"description":"There have been key advancements to building universal approximators for multi-goal collections of reinforcement learning value functions – key elements in estimating long-term returns of states in a parameterized manner. We extend this to hierarchical reinforcement learning, using the options framework, by introducing hierarchical universal value function approximators (H-UVFAs). This allows us to leverage the added benefits of scaling, planning, and generalization expected in temporal abstraction settings. We develop supervised and reinforcement learning methods for learning embeddings of the states, goals, options, and actions in the two hierarchical value functions: $Q(s, g, o; \\theta)$ and $Q(s, g, o, a; \\theta)$. Finally we demonstrate generalization of the HUVFAs and show they outperform corresponding UVFAs.","author":{"@type":"Person","name":"Rushiv Arora"},"datePublished":"2024-10-29T00:00:00+00:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/HierarchicalUniversalValueFunctionApproximators.html"},"url":"https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/HierarchicalUniversalValueFunctionApproximators.html","headline":"Hierarchical Universal Value Function Approximators","@type":"BlogPosting","dateModified":"2024-10-29T00:00:00+00:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" title="Stat Arxiv of Today" /><link rel="shortcut icon" type="image/x-icon" href="" />
  <link rel="stylesheet" href="/arxiv_rss/assets/css/main.css" />


<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</head>
<body a="light">
    <main class="page-content" aria-label="Content">
      <div class="w">
        <a href="/arxiv_rss/">..</a><article>
  <p class="post-meta">
    <time datetime="2024-10-29 00:00:00 +0000">10-29</time>
  </p>
  
  <h1>Hierarchical Universal Value Function Approximators</h1>
  <br>Rushiv Arora</h3>
  <br> [stat.ML]

  <p>There have been key advancements to building universal approximators for multi-goal collections of reinforcement learning value functions – key elements in estimating long-term returns of states in a parameterized manner. We extend this to hierarchical reinforcement learning, using the options framework, by introducing hierarchical universal value function approximators (H-UVFAs). This allows us to leverage the added benefits of scaling, planning, and generalization expected in temporal abstraction settings. We develop supervised and reinforcement learning methods for learning embeddings of the states, goals, options, and actions in the two hierarchical value functions: $Q(s, g, o; \theta)$ and $Q(s, g, o, a; \theta)$. Finally we demonstrate generalization of the HUVFAs and show they outperform corresponding UVFAs.</p>

<p><a href="https://arxiv.org/abs/2410.08997">Read more</a></p>

</article>

      </div>
    </main>
  </body>
</html>