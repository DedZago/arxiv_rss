<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Error estimates of the Euler's method for stochastic differential equations with multiplicative noise via relative entropy</title><!-- Begin Jekyll SEO tag v2.7.1 -->
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Error estimates of the Euler’s method for stochastic differential equations with multiplicative noise via relative entropy" />
<meta name="author" content="Lei Li, Mengchao Wang, Yuliang Wang" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="We investigate the sharp error estimate of the density under the relative entropy (or Kullback-Leibler divergence) for the traditional Euler-Maruyama discretization of stochastic differential equations (SDEs) with multiplicative noise. The foundation of the proof is the estimates of the derivatives for the logarithmic numerical density. The key technique is to adopt the Malliavin calculus to get the expressions of the derivatives of the logarithmic Green’s function and to obtain an estimate for the inverse Malliavin matrix. The estimate of relative entropy then naturally gives sharp error bounds under total variation distance and Wasserstein distances. Compared to the usual weak error estimate for SDEs, such estimate can give an error bound for a family of test functions instead of one test function." />
<meta property="og:description" content="We investigate the sharp error estimate of the density under the relative entropy (or Kullback-Leibler divergence) for the traditional Euler-Maruyama discretization of stochastic differential equations (SDEs) with multiplicative noise. The foundation of the proof is the estimates of the derivatives for the logarithmic numerical density. The key technique is to adopt the Malliavin calculus to get the expressions of the derivatives of the logarithmic Green’s function and to obtain an estimate for the inverse Malliavin matrix. The estimate of relative entropy then naturally gives sharp error bounds under total variation distance and Wasserstein distances. Compared to the usual weak error estimate for SDEs, such estimate can give an error bound for a family of test functions instead of one test function." />
<link rel="canonical" href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ErrorestimatesoftheEulersmethodforstochasticdifferentialequationswithmultiplicativenoiseviarelativeentropy.html" />
<meta property="og:url" content="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ErrorestimatesoftheEulersmethodforstochasticdifferentialequationswithmultiplicativenoiseviarelativeentropy.html" />
<meta property="og:site_name" content="Stat Arxiv of Today" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-10-29T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Error estimates of the Euler’s method for stochastic differential equations with multiplicative noise via relative entropy" />
<script type="application/ld+json">
{"description":"We investigate the sharp error estimate of the density under the relative entropy (or Kullback-Leibler divergence) for the traditional Euler-Maruyama discretization of stochastic differential equations (SDEs) with multiplicative noise. The foundation of the proof is the estimates of the derivatives for the logarithmic numerical density. The key technique is to adopt the Malliavin calculus to get the expressions of the derivatives of the logarithmic Green’s function and to obtain an estimate for the inverse Malliavin matrix. The estimate of relative entropy then naturally gives sharp error bounds under total variation distance and Wasserstein distances. Compared to the usual weak error estimate for SDEs, such estimate can give an error bound for a family of test functions instead of one test function.","author":{"@type":"Person","name":"Lei Li, Mengchao Wang, Yuliang Wang"},"datePublished":"2024-10-29T00:00:00+00:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ErrorestimatesoftheEulersmethodforstochasticdifferentialequationswithmultiplicativenoiseviarelativeentropy.html"},"url":"https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ErrorestimatesoftheEulersmethodforstochasticdifferentialequationswithmultiplicativenoiseviarelativeentropy.html","headline":"Error estimates of the Euler’s method for stochastic differential equations with multiplicative noise via relative entropy","@type":"BlogPosting","dateModified":"2024-10-29T00:00:00+00:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" title="Stat Arxiv of Today" /><link rel="shortcut icon" type="image/x-icon" href="" />
  <link rel="stylesheet" href="/arxiv_rss/assets/css/main.css" />


<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</head>
<body a="light">
    <main class="page-content" aria-label="Content">
      <div class="w">
        <a href="/arxiv_rss/">..</a><article>
  <p class="post-meta">
    <time datetime="2024-10-29 00:00:00 +0000">10-29</time>
  </p>
  
  <h1>Error estimates of the Euler's method for stochastic differential equations with multiplicative noise via relative entropy</h1>
  <br>Lei Li, Mengchao Wang, Yuliang Wang</h3>
  <br> []

  <p>We investigate the sharp error estimate of the density under the relative entropy (or Kullback-Leibler divergence) for the traditional Euler-Maruyama discretization of stochastic differential equations (SDEs) with multiplicative noise. The foundation of the proof is the estimates of the derivatives for the logarithmic numerical density. The key technique is to adopt the Malliavin calculus to get the expressions of the derivatives of the logarithmic Green’s function and to obtain an estimate for the inverse Malliavin matrix. The estimate of relative entropy then naturally gives sharp error bounds under total variation distance and Wasserstein distances. Compared to the usual weak error estimate for SDEs, such estimate can give an error bound for a family of test functions instead of one test function.</p>

<p><a href="https://arxiv.org/abs/2409.04991">Read more</a></p>

</article>

      </div>
    </main>
  </body>
</html>