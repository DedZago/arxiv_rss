<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Multilingual Multi-Aspect Explainability Analyses on Machine Reading Comprehension Models</title><!-- Begin Jekyll SEO tag v2.7.1 -->
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Multilingual Multi-Aspect Explainability Analyses on Machine Reading Comprehension Models" />
<meta name="author" content="Yiming Cui, Wei-Nan Zhang, Wanxiang Che, Ting Liu, Zhigang Chen, Shijin Wang" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Achieving human-level performance on some of the Machine Reading Comprehension (MRC) datasets is no longer challenging with the help of powerful Pre-trained Language Models (PLMs). However, the internal mechanism of these artifacts remains unclear, placing an obstacle for further understanding these models. This paper focuses on conducting a series of analytical experiments to examine the relations between the multi-head self-attention and the final MRC system performance, revealing the potential explainability in PLM-based MRC models. To ensure the robustness of the analyses, we perform our experiments in a multilingual way on top of various PLMs. We discover that passage-to-question and passage understanding attentions are the most important ones in the question answering process, showing strong correlations to the final performance than other parts. Through comprehensive visualizations and case studies, we also observe several general findings on the attention maps, which can be helpful to understand how these models solve the questions." />
<meta property="og:description" content="Achieving human-level performance on some of the Machine Reading Comprehension (MRC) datasets is no longer challenging with the help of powerful Pre-trained Language Models (PLMs). However, the internal mechanism of these artifacts remains unclear, placing an obstacle for further understanding these models. This paper focuses on conducting a series of analytical experiments to examine the relations between the multi-head self-attention and the final MRC system performance, revealing the potential explainability in PLM-based MRC models. To ensure the robustness of the analyses, we perform our experiments in a multilingual way on top of various PLMs. We discover that passage-to-question and passage understanding attentions are the most important ones in the question answering process, showing strong correlations to the final performance than other parts. Through comprehensive visualizations and case studies, we also observe several general findings on the attention maps, which can be helpful to understand how these models solve the questions." />
<link rel="canonical" href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/MultilingualMultiAspectExplainabilityAnalysesonMachineReadingComprehensionModels.html" />
<meta property="og:url" content="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/MultilingualMultiAspectExplainabilityAnalysesonMachineReadingComprehensionModels.html" />
<meta property="og:site_name" content="Stat Arxiv of Today" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-10-29T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Multilingual Multi-Aspect Explainability Analyses on Machine Reading Comprehension Models" />
<script type="application/ld+json">
{"description":"Achieving human-level performance on some of the Machine Reading Comprehension (MRC) datasets is no longer challenging with the help of powerful Pre-trained Language Models (PLMs). However, the internal mechanism of these artifacts remains unclear, placing an obstacle for further understanding these models. This paper focuses on conducting a series of analytical experiments to examine the relations between the multi-head self-attention and the final MRC system performance, revealing the potential explainability in PLM-based MRC models. To ensure the robustness of the analyses, we perform our experiments in a multilingual way on top of various PLMs. We discover that passage-to-question and passage understanding attentions are the most important ones in the question answering process, showing strong correlations to the final performance than other parts. Through comprehensive visualizations and case studies, we also observe several general findings on the attention maps, which can be helpful to understand how these models solve the questions.","author":{"@type":"Person","name":"Yiming Cui, Wei-Nan Zhang, Wanxiang Che, Ting Liu, Zhigang Chen, Shijin Wang"},"datePublished":"2024-10-29T00:00:00+00:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/MultilingualMultiAspectExplainabilityAnalysesonMachineReadingComprehensionModels.html"},"url":"https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/MultilingualMultiAspectExplainabilityAnalysesonMachineReadingComprehensionModels.html","headline":"Multilingual Multi-Aspect Explainability Analyses on Machine Reading Comprehension Models","@type":"BlogPosting","dateModified":"2024-10-29T00:00:00+00:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" title="Stat Arxiv of Today" /><link rel="shortcut icon" type="image/x-icon" href="" />
  <link rel="stylesheet" href="/arxiv_rss/assets/css/main.css" />


<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</head>
<body a="light">
    <main class="page-content" aria-label="Content">
      <div class="w">
        <a href="/arxiv_rss/">..</a><article>
  <p class="post-meta">
    <time datetime="2024-10-29 00:00:00 +0000">10-29</time>
  </p>
  
  <h1>Multilingual Multi-Aspect Explainability Analyses on Machine Reading Comprehension Models</h1>
  <br>Yiming Cui, Wei-Nan Zhang, Wanxiang Che, Ting Liu, Zhigang Chen, Shijin Wang</h3>
  <br> []

  <p>Achieving human-level performance on some of the Machine Reading Comprehension (MRC) datasets is no longer challenging with the help of powerful Pre-trained Language Models (PLMs). However, the internal mechanism of these artifacts remains unclear, placing an obstacle for further understanding these models. This paper focuses on conducting a series of analytical experiments to examine the relations between the multi-head self-attention and the final MRC system performance, revealing the potential explainability in PLM-based MRC models. To ensure the robustness of the analyses, we perform our experiments in a multilingual way on top of various PLMs. We discover that passage-to-question and passage understanding attentions are the most important ones in the question answering process, showing strong correlations to the final performance than other parts. Through comprehensive visualizations and case studies, we also observe several general findings on the attention maps, which can be helpful to understand how these models solve the questions.</p>

<p><a href="https://arxiv.org/abs/2108.11574">Read more</a></p>

</article>

      </div>
    </main>
  </body>
</html>