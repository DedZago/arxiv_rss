---
layout: post
title: "Reducing Investigator Bias in Sampling-Based Land Cover Classification by Integrating Multiple Investigators' Maps Using a Multiple Classifier System"
date: 2024-04-27
author: Narumasa Tsutsumida, Akira Kato
tags: stat.AP
---

Land cover classification plays a pivotal role in describing Earth's surface characteristics. However, these thematic classifications can be affected by uncertainties introduced by an investigator's bias. While land cover classification mapping is becoming easier for us due to the emergence of cloud geospatial platforms such as Google Earth Engine, such uncertainty is often overlooked. Thus, this study aimed to create a robust land cover classification map by reducing investigator-induced uncertainty from independent investigators' maps using a multiple classifier system. In Saitama City, Japan, as a case study, 44 investigators used a point-based visual interpretation method via Google Earth Engine to collect stratified reference samples across four different land cover classes: forest, agriculture, urban, and water. These samples were then used to train a random forest classifier, ultimately resulting in the creation of individual classification maps. We quantified pixel-level discrepancies in these maps, which came from inherent investigator-induced variability. To tackle these uncertainties, we developed a multiple classifier system incorporating K-Medoids to group the most reliable maps and minimize discrepancies. We further applied Bayesian analysis to these grouped maps to produce a unified, accurate classification map. This yielded an overall accuracy of 92.5\% for 400 independent validation samples. We discuss how our approach can also reduce salt-and-pepper noise, which is often found in individual classification maps. This research underscores the intrinsic uncertainties present in land cover classification maps attributable to investigator variations and introduces a potential solution to attenuate these variations.

[Read more](https://arxiv.org/abs/2403.15720)