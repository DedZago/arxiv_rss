<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-10-29T11:26:32+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html"></title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/2024-10-29-DoLLMsknowinternallywhentheyfollowinstructions.html" rel="alternate" type="text/html" title="" /><published>2024-10-29T11:26:32+00:00</published><updated>2024-10-29T11:26:32+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/2024-10-29-DoLLMsknowinternallywhentheyfollowinstructions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/2024-10-29-DoLLMsknowinternallywhentheyfollowinstructions.html">&lt;p&gt;Instruction-following is crucial for building AI agents with large language models (LLMs), as these models must adhere strictly to user-provided constraints and guidelines. However, LLMs often fail to follow even simple and clear instructions. To improve instruction-following behavior and prevent undesirable outputs, a deeper understanding of how LLMs’ internal states relate to these outcomes is required. Our analysis of LLM internal states reveal a dimension in the input embedding space linked to successful instruction-following. We demonstrate that modifying representations along this dimension improves instruction-following success rates compared to random changes, without compromising response quality. Further investigation reveals that this dimension is more closely related to the phrasing of prompts rather than the inherent difficulty of the task or instructions. This discovery also suggests explanations for why LLMs sometimes fail to follow clear instructions and why prompt engineering is often effective, even when the content remains largely unchanged. This work provides insight into the internal workings of LLMs’ instruction-following, paving the way for reliable LLM agents.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.14516&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Emanuele Aliverti</name></author></entry><entry><title type="html"></title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/2024-10-29-FedECAAFederatedExternalControlArmMethodforCausalInferencewithTimeToEventDatainDistributedSettings.html" rel="alternate" type="text/html" title="" /><published>2024-10-29T11:26:32+00:00</published><updated>2024-10-29T11:26:32+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/2024-10-29-FedECAAFederatedExternalControlArmMethodforCausalInferencewithTimeToEventDatainDistributedSettings</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/2024-10-29-FedECAAFederatedExternalControlArmMethodforCausalInferencewithTimeToEventDatainDistributedSettings.html">&lt;p&gt;External control arms (ECA) can inform the early clinical development of experimental drugs and provide efficacy evidence for regulatory approval. However, the main challenge in implementing ECA lies in accessing real-world or historical clinical trials data. Indeed, regulations protecting patients’ rights by strictly controlling data processing make pooling data from multiple sources in a central server often difficult. To address these limitations, we develop a new method, ‘FedECA’ that leverages federated learning (FL) to enable inverse probability of treatment weighting (IPTW) for time-to-event outcomes on separate cohorts without needing to pool data. To showcase the potential of FedECA, we apply it in different settings of increasing complexity culminating with a real-world use-case in which FedECA provides evidence for a differential effect between two drugs that would have otherwise gone unnoticed. By sharing our code, we hope FedECA will foster the creation of federated research networks and thus accelerate drug development.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.16984&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Emanuele Aliverti</name></author></entry><entry><title type="html"></title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/2024-10-29-GPT4oSystemCard.html" rel="alternate" type="text/html" title="" /><published>2024-10-29T11:26:32+00:00</published><updated>2024-10-29T11:26:32+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/2024-10-29-GPT4oSystemCard</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/2024-10-29-GPT4oSystemCard.html">&lt;p&gt;GPT-4o is an autoregressive omni model that accepts as input any combination of text, audio, image, and video, and generates any combination of text, audio, and image outputs. It’s trained end-to-end across text, vision, and audio, meaning all inputs and outputs are processed by the same neural network. GPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time in conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50\% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models. In line with our commitment to building AI safely and consistent with our voluntary commitments to the White House, we are sharing the GPT-4o System Card, which includes our Preparedness Framework evaluations. In this System Card, we provide a detailed look at GPT-4o’s capabilities, limitations, and safety evaluations across multiple categories, focusing on speech-to-speech while also evaluating text and image capabilities, and measures we’ve implemented to ensure the model is safe and aligned. We also include third-party assessments on dangerous capabilities, as well as discussion of potential societal impacts of GPT-4o’s text and vision capabilities.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.21276&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Emanuele Aliverti</name></author></entry><entry><title type="html"></title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/2024-10-29-LibraryLearningDoesntTheCuriousCaseoftheSingleUseLibrary.html" rel="alternate" type="text/html" title="" /><published>2024-10-29T11:26:32+00:00</published><updated>2024-10-29T11:26:32+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/2024-10-29-LibraryLearningDoesntTheCuriousCaseoftheSingleUseLibrary</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/2024-10-29-LibraryLearningDoesntTheCuriousCaseoftheSingleUseLibrary.html">&lt;p&gt;Advances in Large Language Models (LLMs) have spurred a wave of LLM library learning systems for mathematical reasoning. These systems aim to learn a reusable library of tools, such as formal Isabelle lemmas or Python programs that are tailored to a family of tasks. Many of these systems are inspired by the human structuring of knowledge into reusable and extendable concepts, but do current methods actually learn reusable libraries of tools?
  We study two library learning systems for mathematics which both reported increased accuracy: LEGO-Prover and TroVE. We find that function reuse is extremely infrequent on miniF2F and MATH. Our followup ablation experiments suggest that, rather than reuse, self-correction and self-consistency are the primary drivers of the observed performance gains. Our code and data are available at https://github.com/ikb-a/curious-case&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20274&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Emanuele Aliverti</name></author></entry><entry><title type="html"></title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/2024-10-29-ShowMeWhatsWrongCombiningChartsandTexttoGuideDataAnalysis.html" rel="alternate" type="text/html" title="" /><published>2024-10-29T11:26:32+00:00</published><updated>2024-10-29T11:26:32+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/2024-10-29-ShowMeWhatsWrongCombiningChartsandTexttoGuideDataAnalysis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/2024-10-29-ShowMeWhatsWrongCombiningChartsandTexttoGuideDataAnalysis.html">&lt;p&gt;Analyzing and finding anomalies in multi-dimensional datasets is a cumbersome but vital task across different domains. In the context of financial fraud detection, analysts must quickly identify suspicious activity among transactional data. This is an iterative process made of complex exploratory tasks such as recognizing patterns, grouping, and comparing. To mitigate the information overload inherent to these steps, we present a tool combining automated information highlights, Large Language Model generated textual insights, and visual analytics, facilitating exploration at different levels of detail. We perform a segmentation of the data per analysis area and visually represent each one, making use of automated visual cues to signal which require more attention. Upon user selection of an area, our system provides textual and graphical summaries. The text, acting as a link between the high-level and detailed views of the chosen segment, allows for a quick understanding of relevant details. A thorough exploration of the data comprising the selection can be done through graphical representations. The feedback gathered in a study performed with seven domain experts suggests our tool effectively supports and guides exploratory analysis, easing the identification of suspicious information.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.00727&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Emanuele Aliverti</name></author></entry><entry><title type="html">4-bit Shampoo for Memory-Efficient Network Training</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/4bitShampooforMemoryEfficientNetworkTraining.html" rel="alternate" type="text/html" title="4-bit Shampoo for Memory-Efficient Network Training" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/4bitShampooforMemoryEfficientNetworkTraining</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/4bitShampooforMemoryEfficientNetworkTraining.html">&lt;p&gt;Second-order optimizers, maintaining a matrix termed a preconditioner, are superior to first-order optimizers in both theory and practice. The states forming the preconditioner and its inverse root restrict the maximum size of models trained by second-order optimizers. To address this, compressing 32-bit optimizer states to lower bitwidths has shown promise in reducing memory usage. However, current approaches only pertain to first-order optimizers. In this paper, we propose the first 4-bit second-order optimizers, exemplified by 4-bit Shampoo, maintaining performance similar to that of 32-bit ones. We show that quantizing the eigenvector matrix of the preconditioner in 4-bit Shampoo is remarkably better than quantizing the preconditioner itself both theoretically and experimentally. By rectifying the orthogonality of the quantized eigenvector matrix, we enhance the approximation of the preconditioner’s eigenvector matrix, which also benefits the computation of its inverse 4-th root. Besides, we find that linear square quantization slightly outperforms dynamic tree quantization when quantizing second-order optimizer states. Evaluation on various networks for image classification and natural language modeling demonstrates that our 4-bit Shampoo achieves comparable performance to its 32-bit counterpart while being more memory-efficient.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18144&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sike Wang, Pan Zhou, Jia Li, Hua Huang</name></author><summary type="html">Second-order optimizers, maintaining a matrix termed a preconditioner, are superior to first-order optimizers in both theory and practice. The states forming the preconditioner and its inverse root restrict the maximum size of models trained by second-order optimizers. To address this, compressing 32-bit optimizer states to lower bitwidths has shown promise in reducing memory usage. However, current approaches only pertain to first-order optimizers. In this paper, we propose the first 4-bit second-order optimizers, exemplified by 4-bit Shampoo, maintaining performance similar to that of 32-bit ones. We show that quantizing the eigenvector matrix of the preconditioner in 4-bit Shampoo is remarkably better than quantizing the preconditioner itself both theoretically and experimentally. By rectifying the orthogonality of the quantized eigenvector matrix, we enhance the approximation of the preconditioner’s eigenvector matrix, which also benefits the computation of its inverse 4-th root. Besides, we find that linear square quantization slightly outperforms dynamic tree quantization when quantizing second-order optimizer states. Evaluation on various networks for image classification and natural language modeling demonstrates that our 4-bit Shampoo achieves comparable performance to its 32-bit counterpart while being more memory-efficient.</summary></entry><entry><title type="html">A Bayesian Generalized Bridge Regression Approach to Covariance Estimation in the Presence of Covariates</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ABayesianGeneralizedBridgeRegressionApproachtoCovarianceEstimationinthePresenceofCovariates.html" rel="alternate" type="text/html" title="A Bayesian Generalized Bridge Regression Approach to Covariance Estimation in the Presence of Covariates" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ABayesianGeneralizedBridgeRegressionApproachtoCovarianceEstimationinthePresenceofCovariates</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ABayesianGeneralizedBridgeRegressionApproachtoCovarianceEstimationinthePresenceofCovariates.html">&lt;p&gt;A hierarchical Bayesian approach that permits simultaneous inference for the regression coefficient matrix and the error precision (inverse covariance) matrix in the multivariate linear model is proposed. Assuming a natural ordering of the elements of the response, the precision matrix is reparameterized so it can be estimated with univariate-response linear regression techniques. A novel generalized bridge regression prior that accommodates both sparse and dense settings and is competitive with alternative methods for univariate-response regression is proposed and used in this framework. Two component-wise Markov chain Monte Carlo algorithms are developed for sampling, including a data augmentation algorithm based on a scale mixture of normals representation. Numerical examples demonstrate that the proposed method is competitive with comparable joint mean-covariance models, particularly in estimation of the precision matrix. The method is also used to estimate the 253 by 253 precision matrix of 90,670 spectra extracted from images taken by the Hubble Space Telescope, demonstrating its computational feasibility for problems with large n and q.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00906&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Christina Zhao , Ding Xiang , Galin L. Jones , Adam J. Rothman</name></author><category term="stat.ME" /><summary type="html">A hierarchical Bayesian approach that permits simultaneous inference for the regression coefficient matrix and the error precision (inverse covariance) matrix in the multivariate linear model is proposed. Assuming a natural ordering of the elements of the response, the precision matrix is reparameterized so it can be estimated with univariate-response linear regression techniques. A novel generalized bridge regression prior that accommodates both sparse and dense settings and is competitive with alternative methods for univariate-response regression is proposed and used in this framework. Two component-wise Markov chain Monte Carlo algorithms are developed for sampling, including a data augmentation algorithm based on a scale mixture of normals representation. Numerical examples demonstrate that the proposed method is competitive with comparable joint mean-covariance models, particularly in estimation of the precision matrix. The method is also used to estimate the 253 by 253 precision matrix of 90,670 spectra extracted from images taken by the Hubble Space Telescope, demonstrating its computational feasibility for problems with large n and q.</summary></entry><entry><title type="html">A Canonicalization Perspective on Invariant and Equivariant Learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ACanonicalizationPerspectiveonInvariantandEquivariantLearning.html" rel="alternate" type="text/html" title="A Canonicalization Perspective on Invariant and Equivariant Learning" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ACanonicalizationPerspectiveonInvariantandEquivariantLearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ACanonicalizationPerspectiveonInvariantandEquivariantLearning.html">&lt;p&gt;In many applications, we desire neural networks to exhibit invariance or equivariance to certain groups due to symmetries inherent in the data. Recently, frame-averaging methods emerged to be a unified framework for attaining symmetries efficiently by averaging over input-dependent subsets of the group, i.e., frames. What we currently lack is a principled understanding of the design of frames. In this work, we introduce a canonicalization perspective that provides an essential and complete view of the design of frames. Canonicalization is a classic approach for attaining invariance by mapping inputs to their canonical forms. We show that there exists an inherent connection between frames and canonical forms. Leveraging this connection, we can efficiently compare the complexity of frames as well as determine the optimality of certain frames. Guided by this principle, we design novel frames for eigenvectors that are strictly superior to existing methods – some are even optimal – both theoretically and empirically. The reduction to the canonicalization perspective further uncovers equivalences between previous methods. These observations suggest that canonicalization provides a fundamental understanding of existing frame-averaging methods and unifies existing equivariant and invariant learning methods. Code is available at https://github.com/GeorgeMLP/canonicalization.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18378&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>George Ma, Yifei Wang, Derek Lim, Stefanie Jegelka, Yisen Wang</name></author><summary type="html">In many applications, we desire neural networks to exhibit invariance or equivariance to certain groups due to symmetries inherent in the data. Recently, frame-averaging methods emerged to be a unified framework for attaining symmetries efficiently by averaging over input-dependent subsets of the group, i.e., frames. What we currently lack is a principled understanding of the design of frames. In this work, we introduce a canonicalization perspective that provides an essential and complete view of the design of frames. Canonicalization is a classic approach for attaining invariance by mapping inputs to their canonical forms. We show that there exists an inherent connection between frames and canonical forms. Leveraging this connection, we can efficiently compare the complexity of frames as well as determine the optimality of certain frames. Guided by this principle, we design novel frames for eigenvectors that are strictly superior to existing methods – some are even optimal – both theoretically and empirically. The reduction to the canonicalization perspective further uncovers equivalences between previous methods. These observations suggest that canonicalization provides a fundamental understanding of existing frame-averaging methods and unifies existing equivariant and invariant learning methods. Code is available at https://github.com/GeorgeMLP/canonicalization.</summary></entry><entry><title type="html">A Comparative Analysis of Wealth Index Predictions in Africa between three Multi-Source Inference Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AComparativeAnalysisofWealthIndexPredictionsinAfricabetweenthreeMultiSourceInferenceModels.html" rel="alternate" type="text/html" title="A Comparative Analysis of Wealth Index Predictions in Africa between three Multi-Source Inference Models" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AComparativeAnalysisofWealthIndexPredictionsinAfricabetweenthreeMultiSourceInferenceModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AComparativeAnalysisofWealthIndexPredictionsinAfricabetweenthreeMultiSourceInferenceModels.html">&lt;p&gt;Poverty map inference has become a critical focus of research, utilizing both traditional and modern techniques, ranging from regression models to convolutional neural networks applied to tabular data, satellite imagery, and networks. While much attention has been given to validating models during the training phase, the final predictions have received less scrutiny. In this study, we analyze the International Wealth Index (IWI) predicted by Lee and Braithwaite (2022) and Esp&apos;in-Noboa et al. (2023), alongside the Relative Wealth Index (RWI) inferred by Chi et al. (2022), across six Sub-Saharan African countries. Our analysis reveals trends and discrepancies in wealth predictions between these models. In particular, significant and unexpected discrepancies between the predictions of Lee and Braithwaite and Esp&apos;in-Noboa et al., even after accounting for differences in training data. In contrast, the shape of the wealth distributions predicted by Esp&apos;in-Noboa et al. and Chi et al. are more closely aligned, suggesting similar levels of skewness. These findings raise concerns about the validity of certain models and emphasize the importance of rigorous audits for wealth prediction algorithms used in policy-making. Continuous validation and refinement are essential to ensure the reliability of these models, particularly when they inform poverty alleviation strategies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2408.01631&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Márton Karsai, János Kertész, Lisette Espín-Noboa</name></author><summary type="html">Poverty map inference has become a critical focus of research, utilizing both traditional and modern techniques, ranging from regression models to convolutional neural networks applied to tabular data, satellite imagery, and networks. While much attention has been given to validating models during the training phase, the final predictions have received less scrutiny. In this study, we analyze the International Wealth Index (IWI) predicted by Lee and Braithwaite (2022) and Esp&apos;in-Noboa et al. (2023), alongside the Relative Wealth Index (RWI) inferred by Chi et al. (2022), across six Sub-Saharan African countries. Our analysis reveals trends and discrepancies in wealth predictions between these models. In particular, significant and unexpected discrepancies between the predictions of Lee and Braithwaite and Esp&apos;in-Noboa et al., even after accounting for differences in training data. In contrast, the shape of the wealth distributions predicted by Esp&apos;in-Noboa et al. and Chi et al. are more closely aligned, suggesting similar levels of skewness. These findings raise concerns about the validity of certain models and emphasize the importance of rigorous audits for wealth prediction algorithms used in policy-making. Continuous validation and refinement are essential to ensure the reliability of these models, particularly when they inform poverty alleviation strategies.</summary></entry><entry><title type="html">A Comparative Analysis on Ethical Benchmarking in Large Language Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AComparativeAnalysisonEthicalBenchmarkinginLargeLanguageModels.html" rel="alternate" type="text/html" title="A Comparative Analysis on Ethical Benchmarking in Large Language Models" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AComparativeAnalysisonEthicalBenchmarkinginLargeLanguageModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AComparativeAnalysisonEthicalBenchmarkinginLargeLanguageModels.html">&lt;p&gt;This work contributes to the field of Machine Ethics (ME) benchmarking, which develops tests to assess whether intelligent systems accurately represent human values and act accordingly. We identify three major issues with current ME benchmarks: limited ecological validity due to unrealistic ethical dilemmas, unstructured question generation without clear inclusion/exclusion criteria, and a lack of scalability due to reliance on human annotations. Moreover, benchmarks often fail to include sufficient syntactic variations, reducing the robustness of findings. To address these gaps, we introduce two new ME benchmarks: the Triage Benchmark and the Medical Law (MedLaw) Benchmark, both featuring real-world ethical dilemmas from the medical domain. The MedLaw Benchmark, fully AI-generated, offers a scalable alternative. We also introduce context perturbations in our benchmarks to assess models’ worst-case performance. Our findings reveal that ethics prompting does not always improve decision-making. Furthermore, context perturbations not only significantly reduce model performance but can also reverse error patterns and shift relative performance rankings. Lastly, our comparison of worst-case performance suggests that general model capability does not always predict strong ethical decision-making. We argue that ME benchmarks must approximate real-world scenarios and worst-case performance to ensure robust evaluation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.19753&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kira Sam, Raja Vavekanand</name></author><summary type="html">This work contributes to the field of Machine Ethics (ME) benchmarking, which develops tests to assess whether intelligent systems accurately represent human values and act accordingly. We identify three major issues with current ME benchmarks: limited ecological validity due to unrealistic ethical dilemmas, unstructured question generation without clear inclusion/exclusion criteria, and a lack of scalability due to reliance on human annotations. Moreover, benchmarks often fail to include sufficient syntactic variations, reducing the robustness of findings. To address these gaps, we introduce two new ME benchmarks: the Triage Benchmark and the Medical Law (MedLaw) Benchmark, both featuring real-world ethical dilemmas from the medical domain. The MedLaw Benchmark, fully AI-generated, offers a scalable alternative. We also introduce context perturbations in our benchmarks to assess models’ worst-case performance. Our findings reveal that ethics prompting does not always improve decision-making. Furthermore, context perturbations not only significantly reduce model performance but can also reverse error patterns and shift relative performance rankings. Lastly, our comparison of worst-case performance suggests that general model capability does not always predict strong ethical decision-making. We argue that ME benchmarks must approximate real-world scenarios and worst-case performance to ensure robust evaluation.</summary></entry><entry><title type="html">A Componentwise Estimation Procedure for Multivariate Location and Scatter: Robustness, Efficiency and Scalability</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AComponentwiseEstimationProcedureforMultivariateLocationandScatterRobustnessEfficiencyandScalability.html" rel="alternate" type="text/html" title="A Componentwise Estimation Procedure for Multivariate Location and Scatter: Robustness, Efficiency and Scalability" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AComponentwiseEstimationProcedureforMultivariateLocationandScatterRobustnessEfficiencyandScalability</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AComponentwiseEstimationProcedureforMultivariateLocationandScatterRobustnessEfficiencyandScalability.html">&lt;p&gt;Covariance matrix estimation is an important problem in multivariate data analysis, both from theoretical as well as applied points of view. Many simple and popular covariance matrix estimators are known to be severely affected by model misspecification and the presence of outliers in the data; on the other hand robust estimators with reasonably high efficiency are often computationally challenging for modern large and complex datasets. In this work, we propose a new, simple, robust and highly efficient method for estimation of the location vector and the scatter matrix for elliptically symmetric distributions. The proposed estimation procedure is designed in the spirit of the minimum density power divergence (DPD) estimation approach with appropriate modifications which makes our proposal (sequential minimum DPD estimation) computationally very economical and scalable to large as well as higher dimensional datasets. Consistency and asymptotic normality of the proposed sequential estimators of the multivariate location and scatter are established along with asymptotic positive definiteness of the estimated scatter matrix. Robustness of our estimators are studied by means of influence functions. All theoretical results are illustrated further under multivariate normality. A large-scale simulation study is presented to assess finite sample performances and scalability of our method in comparison to the usual maximum likelihood estimator (MLE), the ordinary minimum DPD estimator (MDPDE) and other popular non-parametric methods. The applicability of our method is further illustrated with a real dataset on credit card transactions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.21166&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Soumya Chakraborty, Ayanendranath Basu, Abhik Ghosh</name></author><category term="stat.ME" /><summary type="html">Covariance matrix estimation is an important problem in multivariate data analysis, both from theoretical as well as applied points of view. Many simple and popular covariance matrix estimators are known to be severely affected by model misspecification and the presence of outliers in the data; on the other hand robust estimators with reasonably high efficiency are often computationally challenging for modern large and complex datasets. In this work, we propose a new, simple, robust and highly efficient method for estimation of the location vector and the scatter matrix for elliptically symmetric distributions. The proposed estimation procedure is designed in the spirit of the minimum density power divergence (DPD) estimation approach with appropriate modifications which makes our proposal (sequential minimum DPD estimation) computationally very economical and scalable to large as well as higher dimensional datasets. Consistency and asymptotic normality of the proposed sequential estimators of the multivariate location and scatter are established along with asymptotic positive definiteness of the estimated scatter matrix. Robustness of our estimators are studied by means of influence functions. All theoretical results are illustrated further under multivariate normality. A large-scale simulation study is presented to assess finite sample performances and scalability of our method in comparison to the usual maximum likelihood estimator (MLE), the ordinary minimum DPD estimator (MDPDE) and other popular non-parametric methods. The applicability of our method is further illustrated with a real dataset on credit card transactions.</summary></entry><entry><title type="html">A Cosmic-Scale Benchmark for Symmetry-Preserving Data Processing</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ACosmicScaleBenchmarkforSymmetryPreservingDataProcessing.html" rel="alternate" type="text/html" title="A Cosmic-Scale Benchmark for Symmetry-Preserving Data Processing" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ACosmicScaleBenchmarkforSymmetryPreservingDataProcessing</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ACosmicScaleBenchmarkforSymmetryPreservingDataProcessing.html">&lt;p&gt;Efficiently processing structured point cloud data while preserving multiscale information is a key challenge across domains, from graphics to atomistic modeling. Using a curated dataset of simulated galaxy positions and properties, represented as point clouds, we benchmark the ability of graph neural networks to simultaneously capture local clustering environments and long-range correlations. Given the homogeneous and isotropic nature of the Universe, the data exhibits a high degree of symmetry. We therefore focus on evaluating the performance of Euclidean symmetry-preserving ($E(3)$-equivariant) graph neural networks, showing that they can outperform non-equivariant counterparts and domain-specific information extraction techniques in downstream performance as well as simulation-efficiency. However, we find that current architectures fail to capture information from long-range correlations as effectively as domain-specific baselines, motivating future work on architectures better suited for extracting long-range information.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20516&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Julia Balla, Siddharth Mishra-Sharma, Carolina Cuesta-Lazaro, Tommi Jaakkola, Tess Smidt</name></author><summary type="html">Efficiently processing structured point cloud data while preserving multiscale information is a key challenge across domains, from graphics to atomistic modeling. Using a curated dataset of simulated galaxy positions and properties, represented as point clouds, we benchmark the ability of graph neural networks to simultaneously capture local clustering environments and long-range correlations. Given the homogeneous and isotropic nature of the Universe, the data exhibits a high degree of symmetry. We therefore focus on evaluating the performance of Euclidean symmetry-preserving ($E(3)$-equivariant) graph neural networks, showing that they can outperform non-equivariant counterparts and domain-specific information extraction techniques in downstream performance as well as simulation-efficiency. However, we find that current architectures fail to capture information from long-range correlations as effectively as domain-specific baselines, motivating future work on architectures better suited for extracting long-range information.</summary></entry><entry><title type="html">ADLM – stega: A Universal Adaptive Token Selection Algorithm for Improving Steganographic Text Quality via Information Entropy</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ADLMstegaAUniversalAdaptiveTokenSelectionAlgorithmforImprovingSteganographicTextQualityviaInformationEntropy.html" rel="alternate" type="text/html" title="ADLM – stega: A Universal Adaptive Token Selection Algorithm for Improving Steganographic Text Quality via Information Entropy" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ADLMstegaAUniversalAdaptiveTokenSelectionAlgorithmforImprovingSteganographicTextQualityviaInformationEntropy</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ADLMstegaAUniversalAdaptiveTokenSelectionAlgorithmforImprovingSteganographicTextQualityviaInformationEntropy.html">&lt;p&gt;In the context of widespread global information sharing, information security and privacy protection have become focal points. Steganographic systems enhance information security by embedding confidential information into public carriers; however, existing generative text steganography methods face challenges in handling the long-tail distribution of candidate word pools, which impacts the imperceptibility of steganographic information. This paper proposes a quality control theory for steganographic text generation based on information entropy constraints, exploring the relationship between the imperceptibility of steganographic texts and information entropy. By controlling the information entropy of the candidate word pool within a specific range, we optimize the imperceptibility of the steganographic text. We establish upper and lower bounds for information entropy and introduce an adaptive truncation method to balance semantic coherence and lexical diversity. Experimental results demonstrate that reasonably controlling the candidate pool size and information entropy thresholds significantly enhances the quality and detection resistance of steganographic texts, showcasing broad application potential in the field of natural language processing.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20825&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zezheng Qin, Congcong Sun, Taiyi He, Yuke He, Azizol Abdullah, Normalia Samian, Nuur Alifah Roslan</name></author><summary type="html">In the context of widespread global information sharing, information security and privacy protection have become focal points. Steganographic systems enhance information security by embedding confidential information into public carriers; however, existing generative text steganography methods face challenges in handling the long-tail distribution of candidate word pools, which impacts the imperceptibility of steganographic information. This paper proposes a quality control theory for steganographic text generation based on information entropy constraints, exploring the relationship between the imperceptibility of steganographic texts and information entropy. By controlling the information entropy of the candidate word pool within a specific range, we optimize the imperceptibility of the steganographic text. We establish upper and lower bounds for information entropy and introduce an adaptive truncation method to balance semantic coherence and lexical diversity. Experimental results demonstrate that reasonably controlling the candidate pool size and information entropy thresholds significantly enhances the quality and detection resistance of steganographic texts, showcasing broad application potential in the field of natural language processing.</summary></entry><entry><title type="html">A Derivational ChainBank for Modern Standard Arabic</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ADerivationalChainBankforModernStandardArabic.html" rel="alternate" type="text/html" title="A Derivational ChainBank for Modern Standard Arabic" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ADerivationalChainBankforModernStandardArabic</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ADerivationalChainBankforModernStandardArabic.html">&lt;p&gt;This study presents the ``Arabic Derivational ChainBank,’’ a novel framework for modeling Arabic derivational morphology. It establishes connections between forms and meanings by constructing a chain of derived words that reflect their derivational significance. To expedite the process, a rule-based methodology was employed, avoiding time-consuming manual annotation. The derivational network was then aligned with the CamelMorph morphological analyzer database. This two-step process resulted in a chain of derived word lemmas linked to their roots, encompassing 23,333 evaluated derivational relations, thereby demonstrating the efficiency of the ChainBank.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20463&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Reham Marzouk, Sondos Krouna, Nizar Habash</name></author><summary type="html">This study presents the ``Arabic Derivational ChainBank,’’ a novel framework for modeling Arabic derivational morphology. It establishes connections between forms and meanings by constructing a chain of derived words that reflect their derivational significance. To expedite the process, a rule-based methodology was employed, avoiding time-consuming manual annotation. The derivational network was then aligned with the CamelMorph morphological analyzer database. This two-step process resulted in a chain of derived word lemmas linked to their roots, encompassing 23,333 evaluated derivational relations, thereby demonstrating the efficiency of the ChainBank.</summary></entry><entry><title type="html">A Distributed Lag Approach to the Generalised Dynamic Factor Model (GDFM)</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ADistributedLagApproachtotheGeneralisedDynamicFactorModelGDFM.html" rel="alternate" type="text/html" title="A Distributed Lag Approach to the Generalised Dynamic Factor Model (GDFM)" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ADistributedLagApproachtotheGeneralisedDynamicFactorModelGDFM</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ADistributedLagApproachtotheGeneralisedDynamicFactorModelGDFM.html">&lt;p&gt;We provide estimation and inference for the Generalised Dynamic Factor Model (GDFM) under the assumption that the dynamic common component can be expressed in terms of a finite number of lags of contemporaneously pervasive factors. The proposed estimator is simply an OLS regression of the observed variables on factors extracted via static principal components and therefore avoids frequency domain techniques entirely.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20885&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Philipp Gersing</name></author><category term="stat.ME" /><summary type="html">We provide estimation and inference for the Generalised Dynamic Factor Model (GDFM) under the assumption that the dynamic common component can be expressed in terms of a finite number of lags of contemporaneously pervasive factors. The proposed estimator is simply an OLS regression of the observed variables on factors extracted via static principal components and therefore avoids frequency domain techniques entirely.</summary></entry><entry><title type="html">A Distribution Semantics for Probabilistic Term Rewriting</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ADistributionSemanticsforProbabilisticTermRewriting.html" rel="alternate" type="text/html" title="A Distribution Semantics for Probabilistic Term Rewriting" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ADistributionSemanticsforProbabilisticTermRewriting</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ADistributionSemanticsforProbabilisticTermRewriting.html">&lt;p&gt;Probabilistic programming is becoming increasingly popular thanks to its ability to specify problems with a certain degree of uncertainty. In this work, we focus on term rewriting, a well-known computational formalism. In particular, we consider systems that combine traditional rewriting rules with probabilities. Then, we define a distribution semantics for such systems that can be used to model the probability of reducing a term to some value. We also show how to compute a set of “explanations” for a given reduction, which can be used to compute its probability. Finally, we illustrate our approach with several examples and outline a couple of extensions that may prove useful to improve the expressive power of probabilistic rewrite systems.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.15081&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Germán Vidal</name></author><summary type="html">Probabilistic programming is becoming increasingly popular thanks to its ability to specify problems with a certain degree of uncertainty. In this work, we focus on term rewriting, a well-known computational formalism. In particular, we consider systems that combine traditional rewriting rules with probabilities. Then, we define a distribution semantics for such systems that can be used to model the probability of reducing a term to some value. We also show how to compute a set of “explanations” for a given reduction, which can be used to compute its probability. Finally, we illustrate our approach with several examples and outline a couple of extensions that may prove useful to improve the expressive power of probabilistic rewrite systems.</summary></entry><entry><title type="html">AEPL: Automated and Editable Prompt Learning for Brain Tumor Segmentation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AEPLAutomatedandEditablePromptLearningforBrainTumorSegmentation.html" rel="alternate" type="text/html" title="AEPL: Automated and Editable Prompt Learning for Brain Tumor Segmentation" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AEPLAutomatedandEditablePromptLearningforBrainTumorSegmentation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AEPLAutomatedandEditablePromptLearningforBrainTumorSegmentation.html">&lt;p&gt;Brain tumor segmentation is crucial for accurate diagnosisand treatment planning, but the small size and irregular shapeof tumors pose significant challenges. Existing methods of-ten fail to effectively incorporate medical domain knowledgesuch as tumor grade, which correlates with tumor aggres-siveness and morphology, providing critical insights for moreaccurate detection of tumor subregions during segmentation.We propose an Automated and Editable Prompt Learning(AEPL) framework that integrates tumor grade into the seg-mentation process by combining multi-task learning andprompt learning with automatic and editable prompt gen-eration. Specifically, AEPL employs an encoder to extractimage features for both tumor-grade prediction and segmen-tation mask generation. The predicted tumor grades serveas auto-generated prompts, guiding the decoder to produceprecise segmentation masks. This eliminates the need formanual prompts while allowing clinicians to manually editthe auto-generated prompts to fine-tune the segmentation,enhancing both flexibility and precision. The proposed AEPLachieves state-of-the-art performance on the BraTS 2018dataset, demonstrating its effectiveness and clinical potential.The source code can be accessed online.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.19847&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yongheng Sun, Mingxia Liu, Chunfeng Lian</name></author><summary type="html">Brain tumor segmentation is crucial for accurate diagnosisand treatment planning, but the small size and irregular shapeof tumors pose significant challenges. Existing methods of-ten fail to effectively incorporate medical domain knowledgesuch as tumor grade, which correlates with tumor aggres-siveness and morphology, providing critical insights for moreaccurate detection of tumor subregions during segmentation.We propose an Automated and Editable Prompt Learning(AEPL) framework that integrates tumor grade into the seg-mentation process by combining multi-task learning andprompt learning with automatic and editable prompt gen-eration. Specifically, AEPL employs an encoder to extractimage features for both tumor-grade prediction and segmen-tation mask generation. The predicted tumor grades serveas auto-generated prompts, guiding the decoder to produceprecise segmentation masks. This eliminates the need formanual prompts while allowing clinicians to manually editthe auto-generated prompts to fine-tune the segmentation,enhancing both flexibility and precision. The proposed AEPLachieves state-of-the-art performance on the BraTS 2018dataset, demonstrating its effectiveness and clinical potential.The source code can be accessed online.</summary></entry><entry><title type="html">A Fast Coordinate Descent Method for High-Dimensional Non-Negative Least Squares using a Unified Sparse Regression Framework</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AFastCoordinateDescentMethodforHighDimensionalNonNegativeLeastSquaresusingaUnifiedSparseRegressionFramework.html" rel="alternate" type="text/html" title="A Fast Coordinate Descent Method for High-Dimensional Non-Negative Least Squares using a Unified Sparse Regression Framework" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AFastCoordinateDescentMethodforHighDimensionalNonNegativeLeastSquaresusingaUnifiedSparseRegressionFramework</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AFastCoordinateDescentMethodforHighDimensionalNonNegativeLeastSquaresusingaUnifiedSparseRegressionFramework.html">&lt;p&gt;We develop theoretical results that establish a connection across various regression methods such as the non-negative least squares, bounded variable least squares, simplex constrained least squares, and lasso. In particular, we show in general that a polyhedron constrained least squares problem admits a locally unique sparse solution in high dimensions. We demonstrate the power of our result by concretely quantifying the sparsity level for the aforementioned methods. Furthermore, we propose a novel coordinate descent based solver for NNLS in high dimensions using our theoretical result as motivation. We show through simulated data and a real data example that our solver achieves at least a 5x speed-up from the state-of-the-art solvers.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.03014&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>James Yang, Trevor Hastie</name></author><category term="stat.CO," /><category term="stat.TH" /><summary type="html">We develop theoretical results that establish a connection across various regression methods such as the non-negative least squares, bounded variable least squares, simplex constrained least squares, and lasso. In particular, we show in general that a polyhedron constrained least squares problem admits a locally unique sparse solution in high dimensions. We demonstrate the power of our result by concretely quantifying the sparsity level for the aforementioned methods. Furthermore, we propose a novel coordinate descent based solver for NNLS in high dimensions using our theoretical result as motivation. We show through simulated data and a real data example that our solver achieves at least a 5x speed-up from the state-of-the-art solvers.</summary></entry><entry><title type="html">A Framework for Real-Time Volcano-Seismic Event Recognition Based on Multi-Station Seismograms and Semantic Segmentation Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AFrameworkforRealTimeVolcanoSeismicEventRecognitionBasedonMultiStationSeismogramsandSemanticSegmentationModels.html" rel="alternate" type="text/html" title="A Framework for Real-Time Volcano-Seismic Event Recognition Based on Multi-Station Seismograms and Semantic Segmentation Models" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AFrameworkforRealTimeVolcanoSeismicEventRecognitionBasedonMultiStationSeismogramsandSemanticSegmentationModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AFrameworkforRealTimeVolcanoSeismicEventRecognitionBasedonMultiStationSeismogramsandSemanticSegmentationModels.html">&lt;p&gt;In volcano monitoring, effective recognition of seismic events is essential for understanding volcanic activity and raising timely warning alerts. Traditional methods rely on manual analysis, which can be subjective and labor-intensive. Furthermore, current automatic approaches often tackle detection and classification separately, mostly rely on single station information and generally require tailored preprocessing and representations to perform predictions. These limitations often hinder their application to real-time monitoring and utilization across different volcano conditions. This study introduces a novel approach that utilizes Semantic Segmentation models to automate seismic event recognition by applying a straight forward transformation of multi-channel 1D signals into 2D representations, enabling their use as images. Our framework employs a data-driven, end-to-end design that integrates multi-station seismic data with minimal preprocessing, performing both detection and classification simultaneously for five seismic event classes. We evaluated four state-of-the-art segmentation models (UNet, UNet++, DeepLabV3+ and SwinUNet) on approximately 25.000 seismic events recorded at four different Chilean volcanoes: Nevados del Chill&apos;an Volcanic Complex, Laguna del Maule, Villarrica and Puyehue-Cord&apos;on Caulle. Among these models, the UNet architecture was identified as the most effective model, achieving mean F1 and Intersection over Union (IoU) scores of up to 0.91 and 0.88, respectively, and demonstrating superior noise robustness and model flexibility to unseen volcano datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20595&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Camilo Espinosa-Curilem, Millaray Curilem, Daniel Basualto</name></author><summary type="html">In volcano monitoring, effective recognition of seismic events is essential for understanding volcanic activity and raising timely warning alerts. Traditional methods rely on manual analysis, which can be subjective and labor-intensive. Furthermore, current automatic approaches often tackle detection and classification separately, mostly rely on single station information and generally require tailored preprocessing and representations to perform predictions. These limitations often hinder their application to real-time monitoring and utilization across different volcano conditions. This study introduces a novel approach that utilizes Semantic Segmentation models to automate seismic event recognition by applying a straight forward transformation of multi-channel 1D signals into 2D representations, enabling their use as images. Our framework employs a data-driven, end-to-end design that integrates multi-station seismic data with minimal preprocessing, performing both detection and classification simultaneously for five seismic event classes. We evaluated four state-of-the-art segmentation models (UNet, UNet++, DeepLabV3+ and SwinUNet) on approximately 25.000 seismic events recorded at four different Chilean volcanoes: Nevados del Chill&apos;an Volcanic Complex, Laguna del Maule, Villarrica and Puyehue-Cord&apos;on Caulle. Among these models, the UNet architecture was identified as the most effective model, achieving mean F1 and Intersection over Union (IoU) scores of up to 0.91 and 0.88, respectively, and demonstrating superior noise robustness and model flexibility to unseen volcano datasets.</summary></entry><entry><title type="html">A General Framework for Cutting Feedback within Modularised Bayesian Inference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AGeneralFrameworkforCuttingFeedbackwithinModularisedBayesianInference.html" rel="alternate" type="text/html" title="A General Framework for Cutting Feedback within Modularised Bayesian Inference" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AGeneralFrameworkforCuttingFeedbackwithinModularisedBayesianInference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AGeneralFrameworkforCuttingFeedbackwithinModularisedBayesianInference.html">&lt;p&gt;Standard Bayesian inference can build models that combine information from various sources, but this inference may not be reliable if components of a model are misspecified. Cut inference, as a particular type of modularized Bayesian inference, is an alternative which splits a model into modules and cuts the feedback from the suspect module. Previous studies have focused on a two-module case, but a more general definition of a “module” remains unclear. We present a formal definition of a “module” and discuss its properties. We formulate methods for identifying modules; determining the order of modules; and building the cut distribution that should be used for cut inference within an arbitrary directed acyclic graph structure. We justify the cut distribution by showing that it not only cuts the feedback but also is the best approximation satisfying this condition to the joint distribution in the Kullback-Leibler divergence. We also extend cut inference for the two-module case to a general multiple-module case via a sequential splitting technique and demonstrate this via illustrative applications.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2211.03274&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yang Liu, Robert J. B. Goudie</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Standard Bayesian inference can build models that combine information from various sources, but this inference may not be reliable if components of a model are misspecified. Cut inference, as a particular type of modularized Bayesian inference, is an alternative which splits a model into modules and cuts the feedback from the suspect module. Previous studies have focused on a two-module case, but a more general definition of a “module” remains unclear. We present a formal definition of a “module” and discuss its properties. We formulate methods for identifying modules; determining the order of modules; and building the cut distribution that should be used for cut inference within an arbitrary directed acyclic graph structure. We justify the cut distribution by showing that it not only cuts the feedback but also is the best approximation satisfying this condition to the joint distribution in the Kullback-Leibler divergence. We also extend cut inference for the two-module case to a general multiple-module case via a sequential splitting technique and demonstrate this via illustrative applications.</summary></entry><entry><title type="html">A General Framework for Verification and Control of Dynamical Models via Certificate Synthesis</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AGeneralFrameworkforVerificationandControlofDynamicalModelsviaCertificateSynthesis.html" rel="alternate" type="text/html" title="A General Framework for Verification and Control of Dynamical Models via Certificate Synthesis" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AGeneralFrameworkforVerificationandControlofDynamicalModelsviaCertificateSynthesis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AGeneralFrameworkforVerificationandControlofDynamicalModelsviaCertificateSynthesis.html">&lt;p&gt;An emerging branch of control theory specialises in certificate learning, concerning the specification of a desired (possibly complex) system behaviour for an autonomous or control model, which is then analytically verified by means of a function-based proof. However, the synthesis of controllers abiding by these complex requirements is in general a non-trivial task and may elude the most expert control engineers. This results in a need for automatic techniques that are able to design controllers and to analyse a wide range of elaborate specifications. In this paper, we provide a general framework to encode system specifications and define corresponding certificates, and we present an automated approach to formally synthesise controllers and certificates. Our approach contributes to the broad field of safe learning for control, exploiting the flexibility of neural networks to provide candidate control and certificate functions, whilst using SMT-solvers to offer a formal guarantee of correctness. We test our framework by developing a prototype software tool, and assess its efficacy at verification via control and certificate synthesis over a large and varied suite of benchmarks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.06090&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alec Edwards, Andrea Peruffo, Alessandro Abate</name></author><summary type="html">An emerging branch of control theory specialises in certificate learning, concerning the specification of a desired (possibly complex) system behaviour for an autonomous or control model, which is then analytically verified by means of a function-based proof. However, the synthesis of controllers abiding by these complex requirements is in general a non-trivial task and may elude the most expert control engineers. This results in a need for automatic techniques that are able to design controllers and to analyse a wide range of elaborate specifications. In this paper, we provide a general framework to encode system specifications and define corresponding certificates, and we present an automated approach to formally synthesise controllers and certificates. Our approach contributes to the broad field of safe learning for control, exploiting the flexibility of neural networks to provide candidate control and certificate functions, whilst using SMT-solvers to offer a formal guarantee of correctness. We test our framework by developing a prototype software tool, and assess its efficacy at verification via control and certificate synthesis over a large and varied suite of benchmarks.</summary></entry><entry><title type="html">A Hierarchical Framework with Spatio-Temporal Consistency Learning for Emergence Detection in Complex Adaptive Systems</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AHierarchicalFrameworkwithSpatioTemporalConsistencyLearningforEmergenceDetectioninComplexAdaptiveSystems.html" rel="alternate" type="text/html" title="A Hierarchical Framework with Spatio-Temporal Consistency Learning for Emergence Detection in Complex Adaptive Systems" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AHierarchicalFrameworkwithSpatioTemporalConsistencyLearningforEmergenceDetectioninComplexAdaptiveSystems</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AHierarchicalFrameworkwithSpatioTemporalConsistencyLearningforEmergenceDetectioninComplexAdaptiveSystems.html">&lt;p&gt;Emergence, a global property of complex adaptive systems (CASs) constituted by interactive agents, is prevalent in real-world dynamic systems, e.g., network-level traffic congestions. Detecting its formation and evaporation helps to monitor the state of a system, allowing to issue a warning signal for harmful emergent phenomena. Since there is no centralized controller of CAS, detecting emergence based on each agent’s local observation is desirable but challenging. Existing works are unable to capture emergence-related spatial patterns, and fail to model the nonlinear relationships among agents. This paper proposes a hierarchical framework with spatio-temporal consistency learning to solve these two problems by learning the system representation and agent representations, respectively. Spatio-temporal encoders composed of spatial and temporal transformers are designed to capture agents’ nonlinear relationships and the system’s complex evolution. Agents’ and the system’s representations are learned to preserve the spatio-temporal consistency by minimizing the spatial and temporal dissimilarities in a self-supervised manner in the latent space. Our method achieves more accurate detection than traditional methods and deep learning methods on three datasets with well-known yet hard-to-detect emergent behaviors. Notably, our hierarchical framework is generic in incorporating other deep learning methods for agent-level and system-level detection.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.10300&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Siyuan Chen, Xin Du, Jiahai Wang</name></author><summary type="html">Emergence, a global property of complex adaptive systems (CASs) constituted by interactive agents, is prevalent in real-world dynamic systems, e.g., network-level traffic congestions. Detecting its formation and evaporation helps to monitor the state of a system, allowing to issue a warning signal for harmful emergent phenomena. Since there is no centralized controller of CAS, detecting emergence based on each agent’s local observation is desirable but challenging. Existing works are unable to capture emergence-related spatial patterns, and fail to model the nonlinear relationships among agents. This paper proposes a hierarchical framework with spatio-temporal consistency learning to solve these two problems by learning the system representation and agent representations, respectively. Spatio-temporal encoders composed of spatial and temporal transformers are designed to capture agents’ nonlinear relationships and the system’s complex evolution. Agents’ and the system’s representations are learned to preserve the spatio-temporal consistency by minimizing the spatial and temporal dissimilarities in a self-supervised manner in the latent space. Our method achieves more accurate detection than traditional methods and deep learning methods on three datasets with well-known yet hard-to-detect emergent behaviors. Notably, our hierarchical framework is generic in incorporating other deep learning methods for agent-level and system-level detection.</summary></entry><entry><title type="html">A Human-Centered Approach for Improving Supervised Learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AHumanCenteredApproachforImprovingSupervisedLearning.html" rel="alternate" type="text/html" title="A Human-Centered Approach for Improving Supervised Learning" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AHumanCenteredApproachforImprovingSupervisedLearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AHumanCenteredApproachforImprovingSupervisedLearning.html">&lt;p&gt;Supervised Learning is a way of developing Artificial Intelligence systems in which a computer algorithm is trained on labeled data inputs. Effectiveness of a Supervised Learning algorithm is determined by its performance on a given dataset for a particular problem. In case of Supervised Learning problems, Stacking Ensembles usually perform better than individual classifiers due to their generalization ability. Stacking Ensembles combine predictions from multiple Machine Learning algorithms to make final predictions. Inspite of Stacking Ensembles superior performance, the overhead of Stacking Ensembles such as high cost, resources, time, and lack of explainability create challenges in real-life applications. This paper shows how we can strike a balance between performance, time, and resource constraints. Another goal of this research is to make Ensembles more explainable and intelligible using the Human-Centered approach. To achieve the aforementioned goals, we proposed a Human-Centered Behavior-inspired algorithm that streamlines the Ensemble Learning process while also reducing time, cost, and resource overhead, resulting in the superior performance of Supervised Learning in real-world applications. To demonstrate the effectiveness of our method, we perform our experiments on nine real-world datasets. Experimental results reveal that the proposed method satisfies our goals and outperforms the existing methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.19778&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shubhi Bansal, Atharva Tendulkar, Nagendra Kumar</name></author><summary type="html">Supervised Learning is a way of developing Artificial Intelligence systems in which a computer algorithm is trained on labeled data inputs. Effectiveness of a Supervised Learning algorithm is determined by its performance on a given dataset for a particular problem. In case of Supervised Learning problems, Stacking Ensembles usually perform better than individual classifiers due to their generalization ability. Stacking Ensembles combine predictions from multiple Machine Learning algorithms to make final predictions. Inspite of Stacking Ensembles superior performance, the overhead of Stacking Ensembles such as high cost, resources, time, and lack of explainability create challenges in real-life applications. This paper shows how we can strike a balance between performance, time, and resource constraints. Another goal of this research is to make Ensembles more explainable and intelligible using the Human-Centered approach. To achieve the aforementioned goals, we proposed a Human-Centered Behavior-inspired algorithm that streamlines the Ensemble Learning process while also reducing time, cost, and resource overhead, resulting in the superior performance of Supervised Learning in real-world applications. To demonstrate the effectiveness of our method, we perform our experiments on nine real-world datasets. Experimental results reveal that the proposed method satisfies our goals and outperforms the existing methods.</summary></entry><entry><title type="html">AI-Driven Cyber Threat Intelligence Automation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AIDrivenCyberThreatIntelligenceAutomation.html" rel="alternate" type="text/html" title="AI-Driven Cyber Threat Intelligence Automation" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AIDrivenCyberThreatIntelligenceAutomation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AIDrivenCyberThreatIntelligenceAutomation.html">&lt;p&gt;This study introduces an innovative approach to automating Cyber Threat Intelligence (CTI) processes in industrial environments by leveraging Microsoft’s AI-powered security technologies. Historically, CTI has heavily relied on manual methods for collecting, analyzing, and interpreting data from various sources such as threat feeds. This study introduces an innovative approach to automating CTI processes in industrial environments by leveraging Microsoft’s AI-powered security technologies. Historically, CTI has heavily relied on manual methods for collecting, analyzing, and interpreting data from various sources such as threat feeds, security logs, and dark web forums – a process prone to inefficiencies, especially when rapid information dissemination is critical. By employing the capabilities of GPT-4o and advanced one-shot fine-tuning techniques for large language models, our research delivers a novel CTI automation solution. The outcome of the proposed architecture is a reduction in manual effort while maintaining precision in generating final CTI reports. This research highlights the transformative potential of AI-driven technologies to enhance both the speed and accuracy of CTI and reduce expert demands, offering a vital advantage in today’s dynamic threat landscape.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20287&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shrit Shah, Fatemeh Khoda Parast</name></author><summary type="html">This study introduces an innovative approach to automating Cyber Threat Intelligence (CTI) processes in industrial environments by leveraging Microsoft’s AI-powered security technologies. Historically, CTI has heavily relied on manual methods for collecting, analyzing, and interpreting data from various sources such as threat feeds. This study introduces an innovative approach to automating CTI processes in industrial environments by leveraging Microsoft’s AI-powered security technologies. Historically, CTI has heavily relied on manual methods for collecting, analyzing, and interpreting data from various sources such as threat feeds, security logs, and dark web forums – a process prone to inefficiencies, especially when rapid information dissemination is critical. By employing the capabilities of GPT-4o and advanced one-shot fine-tuning techniques for large language models, our research delivers a novel CTI automation solution. The outcome of the proposed architecture is a reduction in manual effort while maintaining precision in generating final CTI reports. This research highlights the transformative potential of AI-driven technologies to enhance both the speed and accuracy of CTI and reduce expert demands, offering a vital advantage in today’s dynamic threat landscape.</summary></entry><entry><title type="html">AIME: AI System Optimization via Multiple LLM Evaluators</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AIMEAISystemOptimizationviaMultipleLLMEvaluators.html" rel="alternate" type="text/html" title="AIME: AI System Optimization via Multiple LLM Evaluators" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AIMEAISystemOptimizationviaMultipleLLMEvaluators</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AIMEAISystemOptimizationviaMultipleLLMEvaluators.html">&lt;p&gt;Text-based AI system optimization typically involves a feedback loop scheme where a single LLM generates an evaluation in natural language of the current output to improve the next iteration’s output. However, in this work, we empirically demonstrate that for a practical and complex task (code generation) with multiple criteria to evaluate, utilizing only one LLM evaluator tends to let errors in generated code go undetected, thus leading to incorrect evaluations and ultimately suboptimal test case performance. Motivated by this failure case, we assume there exists an optimal evaluation policy that samples an evaluation between response and ground truth. We then theoretically prove that a linear combination of multiple evaluators can approximate this optimal policy. From this insight, we propose AI system optimization via Multiple LLM Evaluators (AIME). AIME is an evaluation protocol that utilizes multiple LLMs that each independently generate an evaluation on separate criteria and then combine them via concatenation. We provide an extensive empirical study showing AIME outperforming baseline methods in code generation tasks, with up to $62\%$ higher error detection rate and up to $16\%$ higher success rate than a single LLM evaluation protocol on LeetCodeHard and HumanEval datasets. We also show that the selection of the number of evaluators and which criteria to utilize is non-trivial as it can impact pact success rate by up to $12\%$.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.03131&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bhrij Patel, Souradip Chakraborty, Wesley A. Suttle, Mengdi Wang, Amrit Singh Bedi, Dinesh Manocha</name></author><summary type="html">Text-based AI system optimization typically involves a feedback loop scheme where a single LLM generates an evaluation in natural language of the current output to improve the next iteration’s output. However, in this work, we empirically demonstrate that for a practical and complex task (code generation) with multiple criteria to evaluate, utilizing only one LLM evaluator tends to let errors in generated code go undetected, thus leading to incorrect evaluations and ultimately suboptimal test case performance. Motivated by this failure case, we assume there exists an optimal evaluation policy that samples an evaluation between response and ground truth. We then theoretically prove that a linear combination of multiple evaluators can approximate this optimal policy. From this insight, we propose AI system optimization via Multiple LLM Evaluators (AIME). AIME is an evaluation protocol that utilizes multiple LLMs that each independently generate an evaluation on separate criteria and then combine them via concatenation. We provide an extensive empirical study showing AIME outperforming baseline methods in code generation tasks, with up to $62\%$ higher error detection rate and up to $16\%$ higher success rate than a single LLM evaluation protocol on LeetCodeHard and HumanEval datasets. We also show that the selection of the number of evaluators and which criteria to utilize is non-trivial as it can impact pact success rate by up to $12\%$.</summary></entry><entry><title type="html">AI Olympics challenge with Evolutionary Soft Actor Critic</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AIOlympicschallengewithEvolutionarySoftActorCritic.html" rel="alternate" type="text/html" title="AI Olympics challenge with Evolutionary Soft Actor Critic" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AIOlympicschallengewithEvolutionarySoftActorCritic</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AIOlympicschallengewithEvolutionarySoftActorCritic.html">&lt;p&gt;In the following report, we describe the solution we propose for the AI Olympics competition held at IROS 2024. Our solution is based on a Model-free Deep Reinforcement Learning approach combined with an evolutionary strategy. We will briefly describe the algorithms that have been used and then provide details of the approach&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2409.01104&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Marco Calì, Alberto Sinigaglia, Niccolò Turcato, Ruggero Carli, Gian Antonio Susto</name></author><summary type="html">In the following report, we describe the solution we propose for the AI Olympics competition held at IROS 2024. Our solution is based on a Model-free Deep Reinforcement Learning approach combined with an evolutionary strategy. We will briefly describe the algorithms that have been used and then provide details of the approach</summary></entry><entry><title type="html">A Machine Learning-Driven Wireless System for Structural Health Monitoring</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AMachineLearningDrivenWirelessSystemforStructuralHealthMonitoring.html" rel="alternate" type="text/html" title="A Machine Learning-Driven Wireless System for Structural Health Monitoring" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AMachineLearningDrivenWirelessSystemforStructuralHealthMonitoring</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AMachineLearningDrivenWirelessSystemforStructuralHealthMonitoring.html">&lt;p&gt;The paper presents a wireless system integrated with a machine learning (ML) model for structural health monitoring (SHM) of carbon fiber reinforced polymer (CFRP) structures, primarily targeting aerospace applications. The system collects data via carbon nanotube (CNT) piezoresistive sensors embedded within CFRP coupons, wirelessly transmitting these data to a central server for processing. A deep neural network (DNN) model predicts mechanical properties and can be extended to forecast structural failures, facilitating proactive maintenance and enhancing safety. The modular design supports scalability and can be embedded within digital twin frameworks, offering significant benefits to aircraft operators and manufacturers. The system utilizes an ML model with a mean absolute error (MAE) of 0.14 on test data for forecasting mechanical properties. Data transmission latency throughout the entire system is less than one second in a LAN setup, highlighting its potential for real-time monitoring applications in aerospace and other industries. However, while the system shows promise, challenges such as sensor reliability under extreme environmental conditions and the need for advanced ML models to handle diverse data streams have been identified as areas for future research.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20678&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Marius Pop, Mihai Tudose, Daniel Visan, Mircea Bocioaga, Mihai Botan, Cesar Banu, Tiberiu Salaoru</name></author><summary type="html">The paper presents a wireless system integrated with a machine learning (ML) model for structural health monitoring (SHM) of carbon fiber reinforced polymer (CFRP) structures, primarily targeting aerospace applications. The system collects data via carbon nanotube (CNT) piezoresistive sensors embedded within CFRP coupons, wirelessly transmitting these data to a central server for processing. A deep neural network (DNN) model predicts mechanical properties and can be extended to forecast structural failures, facilitating proactive maintenance and enhancing safety. The modular design supports scalability and can be embedded within digital twin frameworks, offering significant benefits to aircraft operators and manufacturers. The system utilizes an ML model with a mean absolute error (MAE) of 0.14 on test data for forecasting mechanical properties. Data transmission latency throughout the entire system is less than one second in a LAN setup, highlighting its potential for real-time monitoring applications in aerospace and other industries. However, while the system shows promise, challenges such as sensor reliability under extreme environmental conditions and the need for advanced ML models to handle diverse data streams have been identified as areas for future research.</summary></entry><entry><title type="html">A Model for Intelligible Interaction Between Agents That Predict and Explain</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AModelforIntelligibleInteractionBetweenAgentsThatPredictandExplain.html" rel="alternate" type="text/html" title="A Model for Intelligible Interaction Between Agents That Predict and Explain" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AModelforIntelligibleInteractionBetweenAgentsThatPredictandExplain</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AModelforIntelligibleInteractionBetweenAgentsThatPredictandExplain.html">&lt;p&gt;Machine Learning (ML) has emerged as a powerful form of data modelling with widespread applicability beyond its roots in the design of autonomous agents. However, relatively little attention has been paid to the interaction between people and ML systems. In this paper we view interaction between humans and ML systems within the broader context of communication between agents capable of prediction and explanation. We formalise the interaction model by taking agents to be automata with some special characteristics and define a protocol for communication between such agents. We define One- and Two-Way Intelligibility as properties that emerge at run-time by execution of the protocol. The formalisation allows us to identify conditions under which run-time sequences are bounded, and identify conditions under which the protocol can correctly implement an axiomatic specification of intelligible interaction between a human and an ML system. We also demonstrate using the formal model to: (a) identify instances of One- and Two-Way Intelligibility in literature reports on humans interacting with ML systems providing logic-based explanations, as is done in Inductive Logic Programming (ILP); and (b) map interactions between humans and machines in an elaborate natural-language based dialogue-model to One- or Two-Way Intelligible interactions in the formal model.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2301.01819&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>A. Baskar, Ashwin Srinivasan, Michael Bain, Enrico Coiera</name></author><summary type="html">Machine Learning (ML) has emerged as a powerful form of data modelling with widespread applicability beyond its roots in the design of autonomous agents. However, relatively little attention has been paid to the interaction between people and ML systems. In this paper we view interaction between humans and ML systems within the broader context of communication between agents capable of prediction and explanation. We formalise the interaction model by taking agents to be automata with some special characteristics and define a protocol for communication between such agents. We define One- and Two-Way Intelligibility as properties that emerge at run-time by execution of the protocol. The formalisation allows us to identify conditions under which run-time sequences are bounded, and identify conditions under which the protocol can correctly implement an axiomatic specification of intelligible interaction between a human and an ML system. We also demonstrate using the formal model to: (a) identify instances of One- and Two-Way Intelligibility in literature reports on humans interacting with ML systems providing logic-based explanations, as is done in Inductive Logic Programming (ILP); and (b) map interactions between humans and machines in an elaborate natural-language based dialogue-model to One- or Two-Way Intelligible interactions in the formal model.</summary></entry><entry><title type="html">A Multi-Modal Non-Invasive Deep Learning Framework for Progressive Prediction of Seizures</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AMultiModalNonInvasiveDeepLearningFrameworkforProgressivePredictionofSeizures.html" rel="alternate" type="text/html" title="A Multi-Modal Non-Invasive Deep Learning Framework for Progressive Prediction of Seizures" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AMultiModalNonInvasiveDeepLearningFrameworkforProgressivePredictionofSeizures</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AMultiModalNonInvasiveDeepLearningFrameworkforProgressivePredictionofSeizures.html">&lt;p&gt;This paper introduces an innovative framework designed for progressive (granular in time to onset) prediction of seizures through the utilization of a Deep Learning (DL) methodology based on non-invasive multi-modal sensor networks. Epilepsy, a debilitating neurological condition, affects an estimated 65 million individuals globally, with a substantial proportion facing drug-resistant epilepsy despite pharmacological interventions. To address this challenge, we advocate for predictive systems that provide timely alerts to individuals at risk, enabling them to take precautionary actions. Our framework employs advanced DL techniques and uses personalized data from a network of non-invasive electroencephalogram (EEG) and electrocardiogram (ECG) sensors, thereby enhancing prediction accuracy. The algorithms are optimized for real-time processing on edge devices, mitigating privacy concerns and minimizing data transmission overhead inherent in cloud-based solutions, ultimately preserving battery energy. Additionally, our system predicts the countdown time to seizures (with 15-minute intervals up to an hour prior to the onset), offering critical lead time for preventive actions. Our multi-modal model achieves 95% sensitivity, 98% specificity, and 97% accuracy, averaged among 29 patients.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20066&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ali Saeizadeh, Douglas Schonholtz, Joseph S. Neimat, Pedram Johari, Tommaso Melodia</name></author><summary type="html">This paper introduces an innovative framework designed for progressive (granular in time to onset) prediction of seizures through the utilization of a Deep Learning (DL) methodology based on non-invasive multi-modal sensor networks. Epilepsy, a debilitating neurological condition, affects an estimated 65 million individuals globally, with a substantial proportion facing drug-resistant epilepsy despite pharmacological interventions. To address this challenge, we advocate for predictive systems that provide timely alerts to individuals at risk, enabling them to take precautionary actions. Our framework employs advanced DL techniques and uses personalized data from a network of non-invasive electroencephalogram (EEG) and electrocardiogram (ECG) sensors, thereby enhancing prediction accuracy. The algorithms are optimized for real-time processing on edge devices, mitigating privacy concerns and minimizing data transmission overhead inherent in cloud-based solutions, ultimately preserving battery energy. Additionally, our system predicts the countdown time to seizures (with 15-minute intervals up to an hour prior to the onset), offering critical lead time for preventive actions. Our multi-modal model achieves 95% sensitivity, 98% specificity, and 97% accuracy, averaged among 29 patients.</summary></entry><entry><title type="html">A Multivocal Literature Review on Privacy and Fairness in Federated Learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AMultivocalLiteratureReviewonPrivacyandFairnessinFederatedLearning.html" rel="alternate" type="text/html" title="A Multivocal Literature Review on Privacy and Fairness in Federated Learning" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AMultivocalLiteratureReviewonPrivacyandFairnessinFederatedLearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AMultivocalLiteratureReviewonPrivacyandFairnessinFederatedLearning.html">&lt;p&gt;Federated Learning presents a way to revolutionize AI applications by eliminating the necessity for data sharing. Yet, research has shown that information can still be extracted during training, making additional privacy-preserving measures such as differential privacy imperative. To implement real-world federated learning applications, fairness, ranging from a fair distribution of performance to non-discriminative behaviour, must be considered. Particularly in high-risk applications (e.g. healthcare), avoiding the repetition of past discriminatory errors is paramount. As recent research has demonstrated an inherent tension between privacy and fairness, we conduct a multivocal literature review to examine the current methods to integrate privacy and fairness in federated learning. Our analyses illustrate that the relationship between privacy and fairness has been neglected, posing a critical risk for real-world applications. We highlight the need to explore the relationship between privacy, fairness, and performance, advocating for the creation of integrated federated learning frameworks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2408.08666&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Beatrice Balbierer, Lukas Heinlein, Domenique Zipperling, Niklas Kühl</name></author><summary type="html">Federated Learning presents a way to revolutionize AI applications by eliminating the necessity for data sharing. Yet, research has shown that information can still be extracted during training, making additional privacy-preserving measures such as differential privacy imperative. To implement real-world federated learning applications, fairness, ranging from a fair distribution of performance to non-discriminative behaviour, must be considered. Particularly in high-risk applications (e.g. healthcare), avoiding the repetition of past discriminatory errors is paramount. As recent research has demonstrated an inherent tension between privacy and fairness, we conduct a multivocal literature review to examine the current methods to integrate privacy and fairness in federated learning. Our analyses illustrate that the relationship between privacy and fairness has been neglected, posing a critical risk for real-world applications. We highlight the need to explore the relationship between privacy, fairness, and performance, advocating for the creation of integrated federated learning frameworks.</summary></entry><entry><title type="html">ANOMIX: A Simple yet Effective Hard Negative Generation via Mixing for Graph Anomaly Detection</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ANOMIXASimpleyetEffectiveHardNegativeGenerationviaMixingforGraphAnomalyDetection.html" rel="alternate" type="text/html" title="ANOMIX: A Simple yet Effective Hard Negative Generation via Mixing for Graph Anomaly Detection" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ANOMIXASimpleyetEffectiveHardNegativeGenerationviaMixingforGraphAnomalyDetection</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ANOMIXASimpleyetEffectiveHardNegativeGenerationviaMixingforGraphAnomalyDetection.html">&lt;p&gt;Graph contrastive learning (GCL) generally requires a large number of samples. The one of the effective ways to reduce the number of samples is using hard negatives (e.g., Mixup). Designing mixing-based approach for GAD can be difficult due to imbalanced data or limited number of anomalies. We propose ANOMIX, a framework that consists of a novel graph mixing approach, ANOMIX-M, and multi-level contrasts for GAD. ANOMIX-M can effectively mix abnormality and normality from input graph to generate hard negatives, which are important for efficient GCL. ANOMIX is (a) A first mixing approach: firstly attempting graph mixing to generate hard negatives for GAD task and node- and subgraph-level contrasts to distinguish underlying anomalies. (b) Accurate: winning the highest AUC, up to 5.49% higher and 1.76% faster. (c) Effective: reducing the number of samples nearly 80% in GCL. Code is available at https://github.com/missinghwan/ANOMIX.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20310&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hwan Kim, Junghoon Kim, Sungsu Lim</name></author><summary type="html">Graph contrastive learning (GCL) generally requires a large number of samples. The one of the effective ways to reduce the number of samples is using hard negatives (e.g., Mixup). Designing mixing-based approach for GAD can be difficult due to imbalanced data or limited number of anomalies. We propose ANOMIX, a framework that consists of a novel graph mixing approach, ANOMIX-M, and multi-level contrasts for GAD. ANOMIX-M can effectively mix abnormality and normality from input graph to generate hard negatives, which are important for efficient GCL. ANOMIX is (a) A first mixing approach: firstly attempting graph mixing to generate hard negatives for GAD task and node- and subgraph-level contrasts to distinguish underlying anomalies. (b) Accurate: winning the highest AUC, up to 5.49% higher and 1.76% faster. (c) Effective: reducing the number of samples nearly 80% in GCL. Code is available at https://github.com/missinghwan/ANOMIX.</summary></entry><entry><title type="html">A New Perspective to Boost Performance Fairness for Medical Federated Learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ANewPerspectivetoBoostPerformanceFairnessforMedicalFederatedLearning.html" rel="alternate" type="text/html" title="A New Perspective to Boost Performance Fairness for Medical Federated Learning" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ANewPerspectivetoBoostPerformanceFairnessforMedicalFederatedLearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ANewPerspectivetoBoostPerformanceFairnessforMedicalFederatedLearning.html">&lt;p&gt;Improving the fairness of federated learning (FL) benefits healthy and sustainable collaboration, especially for medical applications. However, existing fair FL methods ignore the specific characteristics of medical FL applications, i.e., domain shift among the datasets from different hospitals. In this work, we propose Fed-LWR to improve performance fairness from the perspective of feature shift, a key issue influencing the performance of medical FL systems caused by domain shift. Specifically, we dynamically perceive the bias of the global model across all hospitals by estimating the layer-wise difference in feature representations between local and global models. To minimize global divergence, we assign higher weights to hospitals with larger differences. The estimated client weights help us to re-aggregate the local models per layer to obtain a fairer global model. We evaluate our method on two widely used federated medical image segmentation benchmarks. The results demonstrate that our method achieves better and fairer performance compared with several state-of-the-art fair FL methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.19765&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yunlu Yan, Lei Zhu, Yuexiang Li, Xinxing Xu, Rick Siow Mong Goh, Yong Liu, Salman Khan, Chun-Mei Feng</name></author><summary type="html">Improving the fairness of federated learning (FL) benefits healthy and sustainable collaboration, especially for medical applications. However, existing fair FL methods ignore the specific characteristics of medical FL applications, i.e., domain shift among the datasets from different hospitals. In this work, we propose Fed-LWR to improve performance fairness from the perspective of feature shift, a key issue influencing the performance of medical FL systems caused by domain shift. Specifically, we dynamically perceive the bias of the global model across all hospitals by estimating the layer-wise difference in feature representations between local and global models. To minimize global divergence, we assign higher weights to hospitals with larger differences. The estimated client weights help us to re-aggregate the local models per layer to obtain a fairer global model. We evaluate our method on two widely used federated medical image segmentation benchmarks. The results demonstrate that our method achieves better and fairer performance compared with several state-of-the-art fair FL methods.</summary></entry><entry><title type="html">A Novel Interpolation-Based Method for Solving the One-Dimensional Wave Equation on a Domain with a Moving Boundary</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ANovelInterpolationBasedMethodforSolvingtheOneDimensionalWaveEquationonaDomainwithaMovingBoundary.html" rel="alternate" type="text/html" title="A Novel Interpolation-Based Method for Solving the One-Dimensional Wave Equation on a Domain with a Moving Boundary" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ANovelInterpolationBasedMethodforSolvingtheOneDimensionalWaveEquationonaDomainwithaMovingBoundary</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ANovelInterpolationBasedMethodforSolvingtheOneDimensionalWaveEquationonaDomainwithaMovingBoundary.html">&lt;p&gt;We revisit the problem of solving the one-dimensional wave equation on a domain with moving boundary. In J. Math. Phys. 11, 2679 (1970), Moore introduced an interesting method to do so. As only in rare cases, a closed analytical solution is possible, one must turn to perturbative expansions of Moore’s method. We investigate the then made minimal assumption for convergence of the perturbation series, namely that the boundary position should be an analytic function of time. Though, we prove here that the latter requirement is not a sufficient condition for Moore’s method to converge. We then introduce a novel numerical approach based on interpolation which also works for fast boundary dynamics. In comparison with other state-of-the-art numerical methods, our method offers greater speed if the wave solution needs to be evaluated at many points in time or space, whilst preserving accuracy. We discuss two variants of our method, either based on a conformal coordinate transformation or on the method of characteristics, together with interpolation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2408.16483&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Michiel Lassuyt, Emma Vancayseele, Wouter Deleersnyder, David Dudal, Sebbe Stouten, Koen Van Den Abeele</name></author><summary type="html">We revisit the problem of solving the one-dimensional wave equation on a domain with moving boundary. In J. Math. Phys. 11, 2679 (1970), Moore introduced an interesting method to do so. As only in rare cases, a closed analytical solution is possible, one must turn to perturbative expansions of Moore’s method. We investigate the then made minimal assumption for convergence of the perturbation series, namely that the boundary position should be an analytic function of time. Though, we prove here that the latter requirement is not a sufficient condition for Moore’s method to converge. We then introduce a novel numerical approach based on interpolation which also works for fast boundary dynamics. In comparison with other state-of-the-art numerical methods, our method offers greater speed if the wave solution needs to be evaluated at many points in time or space, whilst preserving accuracy. We discuss two variants of our method, either based on a conformal coordinate transformation or on the method of characteristics, together with interpolation.</summary></entry><entry><title type="html">A Proximal Gradient Method With Probabilistic Multi-Gossip Communications for Decentralized Composite Optimization</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AProximalGradientMethodWithProbabilisticMultiGossipCommunicationsforDecentralizedCompositeOptimization.html" rel="alternate" type="text/html" title="A Proximal Gradient Method With Probabilistic Multi-Gossip Communications for Decentralized Composite Optimization" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AProximalGradientMethodWithProbabilisticMultiGossipCommunicationsforDecentralizedCompositeOptimization</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AProximalGradientMethodWithProbabilisticMultiGossipCommunicationsforDecentralizedCompositeOptimization.html">&lt;p&gt;Decentralized optimization methods with local updates have recently gained attention for their provable ability to communication acceleration. In these methods, nodes perform several iterations of local computations between the communication rounds. Nevertheless, this capability is effective only when the loss function is smooth and the network is sufficiently well-connected. In this paper, we propose a communication-efficient method MG-Skip with probabilistic local updates and multi-gossip communications for decentralized composite (smooth + nonsmooth) optimization, whose stepsize is independent of the number of local updates and the network topology. Without any additional condition for network connectivity, MG-Skip allows for the multi-gossip communications to be skipped in most iterations in the strongly convex setting, while its iteration complexity is $\mathcal{O}\left(\kappa \log \frac{1}{\epsilon}\right)$ and communication complexity is only $\mathcal{O}\left(\sqrt{\frac{\kappa}{(1-\rho)}} \log \frac{1}{\epsilon}\right)$, where $\kappa$ is the condition number of the loss function, $\rho$ reflects the connectivity of the network topology, and $\epsilon$ is the target accuracy. The theoretical results demonstrate that MG-Skip achieves the optimal communication complexity and confirm the benefits of local updates in the nonsmooth setup.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.11861&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Luyao Guo, Luqing Wang, Xinli Shi, Jinde Cao</name></author><summary type="html">Decentralized optimization methods with local updates have recently gained attention for their provable ability to communication acceleration. In these methods, nodes perform several iterations of local computations between the communication rounds. Nevertheless, this capability is effective only when the loss function is smooth and the network is sufficiently well-connected. In this paper, we propose a communication-efficient method MG-Skip with probabilistic local updates and multi-gossip communications for decentralized composite (smooth + nonsmooth) optimization, whose stepsize is independent of the number of local updates and the network topology. Without any additional condition for network connectivity, MG-Skip allows for the multi-gossip communications to be skipped in most iterations in the strongly convex setting, while its iteration complexity is $\mathcal{O}\left(\kappa \log \frac{1}{\epsilon}\right)$ and communication complexity is only $\mathcal{O}\left(\sqrt{\frac{\kappa}{(1-\rho)}} \log \frac{1}{\epsilon}\right)$, where $\kappa$ is the condition number of the loss function, $\rho$ reflects the connectivity of the network topology, and $\epsilon$ is the target accuracy. The theoretical results demonstrate that MG-Skip achieves the optimal communication complexity and confirm the benefits of local updates in the nonsmooth setup.</summary></entry><entry><title type="html">A Review of Deep Learning Approaches for Non-Invasive Cognitive Impairment Detection</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AReviewofDeepLearningApproachesforNonInvasiveCognitiveImpairmentDetection.html" rel="alternate" type="text/html" title="A Review of Deep Learning Approaches for Non-Invasive Cognitive Impairment Detection" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AReviewofDeepLearningApproachesforNonInvasiveCognitiveImpairmentDetection</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AReviewofDeepLearningApproachesforNonInvasiveCognitiveImpairmentDetection.html">&lt;p&gt;This review paper explores recent advances in deep learning approaches for non-invasive cognitive impairment detection. We examine various non-invasive indicators of cognitive decline, including speech and language, facial, and motoric mobility. The paper provides an overview of relevant datasets, feature-extracting techniques, and deep-learning architectures applied to this domain. We have analyzed the performance of different methods across modalities and observed that speech and language-based methods generally achieved the highest detection performance. Studies combining acoustic and linguistic features tended to outperform those using a single modality. Facial analysis methods showed promise for visual modalities but were less extensively studied. Most papers focused on binary classification (impaired vs. non-impaired), with fewer addressing multi-class or regression tasks. Transfer learning and pre-trained language models emerged as popular and effective techniques, especially for linguistic analysis. Despite significant progress, several challenges remain, including data standardization and accessibility, model explainability, longitudinal analysis limitations, and clinical adaptation. Lastly, we propose future research directions, such as investigating language-agnostic speech analysis methods, developing multi-modal diagnostic systems, and addressing ethical considerations in AI-assisted healthcare. By synthesizing current trends and identifying key obstacles, this review aims to guide further development of deep learning-based cognitive impairment detection systems to improve early diagnosis and ultimately patient outcomes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.19898&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Muath Alsuhaibani, Ali Pourramezan Fard, Jian Sun, Farida Far Poor, Peter S. Pressman, Mohammad H. Mahoor</name></author><summary type="html">This review paper explores recent advances in deep learning approaches for non-invasive cognitive impairment detection. We examine various non-invasive indicators of cognitive decline, including speech and language, facial, and motoric mobility. The paper provides an overview of relevant datasets, feature-extracting techniques, and deep-learning architectures applied to this domain. We have analyzed the performance of different methods across modalities and observed that speech and language-based methods generally achieved the highest detection performance. Studies combining acoustic and linguistic features tended to outperform those using a single modality. Facial analysis methods showed promise for visual modalities but were less extensively studied. Most papers focused on binary classification (impaired vs. non-impaired), with fewer addressing multi-class or regression tasks. Transfer learning and pre-trained language models emerged as popular and effective techniques, especially for linguistic analysis. Despite significant progress, several challenges remain, including data standardization and accessibility, model explainability, longitudinal analysis limitations, and clinical adaptation. Lastly, we propose future research directions, such as investigating language-agnostic speech analysis methods, developing multi-modal diagnostic systems, and addressing ethical considerations in AI-assisted healthcare. By synthesizing current trends and identifying key obstacles, this review aims to guide further development of deep learning-based cognitive impairment detection systems to improve early diagnosis and ultimately patient outcomes.</summary></entry><entry><title type="html">A Review of Graph-Powered Data Quality Applications for IoT Monitoring Sensor Networks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AReviewofGraphPoweredDataQualityApplicationsforIoTMonitoringSensorNetworks.html" rel="alternate" type="text/html" title="A Review of Graph-Powered Data Quality Applications for IoT Monitoring Sensor Networks" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AReviewofGraphPoweredDataQualityApplicationsforIoTMonitoringSensorNetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AReviewofGraphPoweredDataQualityApplicationsforIoTMonitoringSensorNetworks.html">&lt;p&gt;The development of Internet of Things (IoT) technologies has led to the widespread adoption of monitoring networks for a wide variety of applications, such as smart cities, environmental monitoring, and precision agriculture. A major research focus in recent years has been the development of graph-based techniques to improve the quality of data from sensor networks, a key aspect for the use of sensed data in decision-making processes, digital twins, and other applications. Emphasis has been placed on the development of machine learning and signal processing techniques over graphs, taking advantage of the benefits provided by the use of structured data through a graph topology. Many technologies such as the graph signal processing (GSP) or the successful graph neural networks (GNNs) have been used for data quality enhancement tasks. In this survey, we focus on graph-based models for data quality control in monitoring sensor networks. Furthermore, we delve into the technical details that are commonly leveraged for providing powerful graph-based solutions for data quality tasks in sensor networks, including missing value imputation, outlier detection, or virtual sensing. To conclude, we have identified future trends and challenges such as graph-based models for digital twins or model transferability and generalization.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.21006&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Pau Ferrer-Cid, Jose M. Barcelo-Ordinas, Jorge Garcia-Vidal</name></author><summary type="html">The development of Internet of Things (IoT) technologies has led to the widespread adoption of monitoring networks for a wide variety of applications, such as smart cities, environmental monitoring, and precision agriculture. A major research focus in recent years has been the development of graph-based techniques to improve the quality of data from sensor networks, a key aspect for the use of sensed data in decision-making processes, digital twins, and other applications. Emphasis has been placed on the development of machine learning and signal processing techniques over graphs, taking advantage of the benefits provided by the use of structured data through a graph topology. Many technologies such as the graph signal processing (GSP) or the successful graph neural networks (GNNs) have been used for data quality enhancement tasks. In this survey, we focus on graph-based models for data quality control in monitoring sensor networks. Furthermore, we delve into the technical details that are commonly leveraged for providing powerful graph-based solutions for data quality tasks in sensor networks, including missing value imputation, outlier detection, or virtual sensing. To conclude, we have identified future trends and challenges such as graph-based models for digital twins or model transferability and generalization.</summary></entry><entry><title type="html">A Robust Governance for the AI Act: AI Office, AI Board, Scientific Panel, and National Authorities</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ARobustGovernancefortheAIActAIOfficeAIBoardScientificPanelandNationalAuthorities.html" rel="alternate" type="text/html" title="A Robust Governance for the AI Act: AI Office, AI Board, Scientific Panel, and National Authorities" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ARobustGovernancefortheAIActAIOfficeAIBoardScientificPanelandNationalAuthorities</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ARobustGovernancefortheAIActAIOfficeAIBoardScientificPanelandNationalAuthorities.html">&lt;p&gt;Regulation is nothing without enforcement. This particularly holds for the dynamic field of emerging technologies. Hence, this article has two ambitions. First, it explains how the EU’s new Artificial Intelligence Act (AIA) will be implemented and enforced by various institutional bodies, thus clarifying the governance framework of the AIA. Second, it proposes a normative model of governance, providing recommendations to ensure uniform and coordinated execution of the AIA and the fulfilment of the legislation. Taken together, the article explores how the AIA may be implemented by national and EU institutional bodies, encompassing longstanding bodies, such as the European Commission, and those newly established under the AIA, such as the AI Office. It investigates their roles across supranational and national levels, emphasizing how EU regulations influence institutional structures and operations. These regulations may not only directly dictate the structural design of institutions but also indirectly request administrative capacities needed to enforce the AIA.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2407.10369&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Claudio Novelli, Philipp Hacker, Jessica Morley, Jarle Trondal, Luciano Floridi</name></author><summary type="html">Regulation is nothing without enforcement. This particularly holds for the dynamic field of emerging technologies. Hence, this article has two ambitions. First, it explains how the EU’s new Artificial Intelligence Act (AIA) will be implemented and enforced by various institutional bodies, thus clarifying the governance framework of the AIA. Second, it proposes a normative model of governance, providing recommendations to ensure uniform and coordinated execution of the AIA and the fulfilment of the legislation. Taken together, the article explores how the AIA may be implemented by national and EU institutional bodies, encompassing longstanding bodies, such as the European Commission, and those newly established under the AIA, such as the AI Office. It investigates their roles across supranational and national levels, emphasizing how EU regulations influence institutional structures and operations. These regulations may not only directly dictate the structural design of institutions but also indirectly request administrative capacities needed to enforce the AIA.</summary></entry><entry><title type="html">A Robust Topological Framework for Detecting Regime Changes in Multi-Trial Experiments with Application to Predictive Maintenance</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ARobustTopologicalFrameworkforDetectingRegimeChangesinMultiTrialExperimentswithApplicationtoPredictiveMaintenance.html" rel="alternate" type="text/html" title="A Robust Topological Framework for Detecting Regime Changes in Multi-Trial Experiments with Application to Predictive Maintenance" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ARobustTopologicalFrameworkforDetectingRegimeChangesinMultiTrialExperimentswithApplicationtoPredictiveMaintenance</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ARobustTopologicalFrameworkforDetectingRegimeChangesinMultiTrialExperimentswithApplicationtoPredictiveMaintenance.html">&lt;p&gt;We present a general and flexible framework for detecting regime changes in complex, non-stationary data across multi-trial experiments. Traditional change point detection methods focus on identifying abrupt changes within a single time series (single trial), targeting shifts in statistical properties such as the mean, variance, and spectrum over time within that sole trial. In contrast, our approach considers changes occurring across trials, accommodating changes that may arise within individual trials due to experimental inconsistencies, such as varying delays or event duration. By leveraging diverse metrics to analyze time-frequency characteristics specifically topological changes in the spectrum and spectrograms, our approach offers a comprehensive framework for detecting such variations. Our approach can handle different scenarios with various statistical assumptions, including varying levels of stationarity within and across trials, making our framework highly adaptable. We validate our approach through simulations using time-varying autoregressive processes that exhibit different regime changes. Our results demonstrate the effectiveness of detecting changes across trials under diverse conditions. Furthermore, we illustrate the effectiveness of our method by applying it to predictive maintenance using the NASA bearing dataset. By analyzing the time-frequency characteristics of vibration signals recorded by accelerometers, our approach accurately identifies bearing failures, showcasing its strong potential for early fault detection in mechanical systems.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20443&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Anass B. El-Yaagoubi, Jean-Marc Freyermuth, Hernando Ombao</name></author><category term="stat.ME" /><summary type="html">We present a general and flexible framework for detecting regime changes in complex, non-stationary data across multi-trial experiments. Traditional change point detection methods focus on identifying abrupt changes within a single time series (single trial), targeting shifts in statistical properties such as the mean, variance, and spectrum over time within that sole trial. In contrast, our approach considers changes occurring across trials, accommodating changes that may arise within individual trials due to experimental inconsistencies, such as varying delays or event duration. By leveraging diverse metrics to analyze time-frequency characteristics specifically topological changes in the spectrum and spectrograms, our approach offers a comprehensive framework for detecting such variations. Our approach can handle different scenarios with various statistical assumptions, including varying levels of stationarity within and across trials, making our framework highly adaptable. We validate our approach through simulations using time-varying autoregressive processes that exhibit different regime changes. Our results demonstrate the effectiveness of detecting changes across trials under diverse conditions. Furthermore, we illustrate the effectiveness of our method by applying it to predictive maintenance using the NASA bearing dataset. By analyzing the time-frequency characteristics of vibration signals recorded by accelerometers, our approach accurately identifies bearing failures, showcasing its strong potential for early fault detection in mechanical systems.</summary></entry><entry><title type="html">A SAM based Tool for Semi-Automatic Food Annotation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ASAMbasedToolforSemiAutomaticFoodAnnotation.html" rel="alternate" type="text/html" title="A SAM based Tool for Semi-Automatic Food Annotation" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ASAMbasedToolforSemiAutomaticFoodAnnotation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ASAMbasedToolforSemiAutomaticFoodAnnotation.html">&lt;p&gt;The advancement of artificial intelligence (AI) in food and nutrition research is hindered by a critical bottleneck: the lack of annotated food data. Despite the rise of highly efficient AI models designed for tasks such as food segmentation and classification, their practical application might necessitate proficiency in AI and machine learning principles, which can act as a challenge for non-AI experts in the field of nutritional sciences. Alternatively, it highlights the need to translate AI models into user-friendly tools that are accessible to all. To address this, we present a demo of a semi-automatic food image annotation tool leveraging the Segment Anything Model (SAM). The tool enables prompt-based food segmentation via user interactions, promoting user engagement and allowing them to further categorise food items within meal images and specify weight/volume if necessary. Additionally, we release a fine-tuned version of SAM’s mask decoder, dubbed MealSAM, with the ViT-B backbone tailored specifically for food image segmentation. Our objective is not only to contribute to the field by encouraging participation, collaboration, and the gathering of more annotated food data but also to make AI technology available for a broader audience by translating AI into practical tools.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.19756&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lubnaa Abdur Rahman, Ioannis Papathanail, Lorenzo Brigato, Stavroula Mougiakakou</name></author><summary type="html">The advancement of artificial intelligence (AI) in food and nutrition research is hindered by a critical bottleneck: the lack of annotated food data. Despite the rise of highly efficient AI models designed for tasks such as food segmentation and classification, their practical application might necessitate proficiency in AI and machine learning principles, which can act as a challenge for non-AI experts in the field of nutritional sciences. Alternatively, it highlights the need to translate AI models into user-friendly tools that are accessible to all. To address this, we present a demo of a semi-automatic food image annotation tool leveraging the Segment Anything Model (SAM). The tool enables prompt-based food segmentation via user interactions, promoting user engagement and allowing them to further categorise food items within meal images and specify weight/volume if necessary. Additionally, we release a fine-tuned version of SAM’s mask decoder, dubbed MealSAM, with the ViT-B backbone tailored specifically for food image segmentation. Our objective is not only to contribute to the field by encouraging participation, collaboration, and the gathering of more annotated food data but also to make AI technology available for a broader audience by translating AI into practical tools.</summary></entry><entry><title type="html">A Short Note on the Efficiency of Markov Chains for Bayesian Linear Regression Models with Heavy-Tailed Errors</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AShortNoteontheEfficiencyofMarkovChainsforBayesianLinearRegressionModelswithHeavyTailedErrors.html" rel="alternate" type="text/html" title="A Short Note on the Efficiency of Markov Chains for Bayesian Linear Regression Models with Heavy-Tailed Errors" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AShortNoteontheEfficiencyofMarkovChainsforBayesianLinearRegressionModelswithHeavyTailedErrors</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AShortNoteontheEfficiencyofMarkovChainsforBayesianLinearRegressionModelswithHeavyTailedErrors.html">&lt;p&gt;In this short note, we consider posterior simulation for a linear regression model when the error distribution is given by a scale mixture of multivariate normals. We first show that the sampler of Backlund and Hobert (2020) for the case of the conditionally conjugate normal-inverse Wishart prior continues to be geometrically ergodic even when the error density is heavier-tailed. Moreover, we prove that the ergodicity is uniform by verifying the minorization condition. In the second half of this note, we treat an improper case and show that the sampler of Section 4 of Roy and Hobert (2010) is geometrically ergodic under significantly milder conditions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.17070&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yasuyuki Hamura</name></author><category term="stat.CO," /><category term="stat.TH" /><summary type="html">In this short note, we consider posterior simulation for a linear regression model when the error distribution is given by a scale mixture of multivariate normals. We first show that the sampler of Backlund and Hobert (2020) for the case of the conditionally conjugate normal-inverse Wishart prior continues to be geometrically ergodic even when the error density is heavier-tailed. Moreover, we prove that the ergodicity is uniform by verifying the minorization condition. In the second half of this note, we treat an improper case and show that the sampler of Section 4 of Roy and Hobert (2010) is geometrically ergodic under significantly milder conditions.</summary></entry><entry><title type="html">A Stack-Propagation Framework for Low-Resource Personalized Dialogue Generation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AStackPropagationFrameworkforLowResourcePersonalizedDialogueGeneration.html" rel="alternate" type="text/html" title="A Stack-Propagation Framework for Low-Resource Personalized Dialogue Generation" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AStackPropagationFrameworkforLowResourcePersonalizedDialogueGeneration</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AStackPropagationFrameworkforLowResourcePersonalizedDialogueGeneration.html">&lt;p&gt;With the resurgent interest in building open-domain dialogue systems, the dialogue generation task has attracted increasing attention over the past few years. This task is usually formulated as a conditional generation problem, which aims to generate a natural and meaningful response given dialogue contexts and specific constraints, such as persona. And maintaining a consistent persona is essential for the dialogue systems to gain trust from the users. Although tremendous advancements have been brought, traditional persona-based dialogue models are typically trained by leveraging a large number of persona-dense dialogue examples. Yet, such persona-dense training data are expensive to obtain, leading to a limited scale. This work presents a novel approach to learning from limited training examples by regarding consistency understanding as a regularization of response generation. To this end, we propose a novel stack-propagation framework for learning a generation and understanding pipeline.Specifically, the framework stacks a Transformer encoder and two Transformer decoders, where the first decoder models response generation and the second serves as a regularizer and jointly models response generation and consistency understanding. The proposed framework can benefit from the stacked encoder and decoders to learn from much smaller personalized dialogue data while maintaining competitive performance. Under different low-resource settings, subjective and objective evaluations prove that the stack-propagation framework outperforms strong baselines in response quality and persona consistency and largely overcomes the shortcomings of traditional models that rely heavily on the persona-dense dialogue data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20174&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Haoyu Song, Wei-Nan Zhang, Kaiyan Zhang, Ting Liu</name></author><summary type="html">With the resurgent interest in building open-domain dialogue systems, the dialogue generation task has attracted increasing attention over the past few years. This task is usually formulated as a conditional generation problem, which aims to generate a natural and meaningful response given dialogue contexts and specific constraints, such as persona. And maintaining a consistent persona is essential for the dialogue systems to gain trust from the users. Although tremendous advancements have been brought, traditional persona-based dialogue models are typically trained by leveraging a large number of persona-dense dialogue examples. Yet, such persona-dense training data are expensive to obtain, leading to a limited scale. This work presents a novel approach to learning from limited training examples by regarding consistency understanding as a regularization of response generation. To this end, we propose a novel stack-propagation framework for learning a generation and understanding pipeline.Specifically, the framework stacks a Transformer encoder and two Transformer decoders, where the first decoder models response generation and the second serves as a regularizer and jointly models response generation and consistency understanding. The proposed framework can benefit from the stacked encoder and decoders to learn from much smaller personalized dialogue data while maintaining competitive performance. Under different low-resource settings, subjective and objective evaluations prove that the stack-propagation framework outperforms strong baselines in response quality and persona consistency and largely overcomes the shortcomings of traditional models that rely heavily on the persona-dense dialogue data.</summary></entry><entry><title type="html">A Static and Dynamic Attention Framework for Multi Turn Dialogue Generation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AStaticandDynamicAttentionFrameworkforMultiTurnDialogueGeneration.html" rel="alternate" type="text/html" title="A Static and Dynamic Attention Framework for Multi Turn Dialogue Generation" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AStaticandDynamicAttentionFrameworkforMultiTurnDialogueGeneration</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AStaticandDynamicAttentionFrameworkforMultiTurnDialogueGeneration.html">&lt;p&gt;Recently, research on open domain dialogue systems have attracted extensive interests of academic and industrial researchers. The goal of an open domain dialogue system is to imitate humans in conversations. Previous works on single turn conversation generation have greatly promoted the research of open domain dialogue systems. However, understanding multiple single turn conversations is not equal to the understanding of multi turn dialogue due to the coherent and context dependent properties of human dialogue. Therefore, in open domain multi turn dialogue generation, it is essential to modeling the contextual semantics of the dialogue history, rather than only according to the last utterance. Previous research had verified the effectiveness of the hierarchical recurrent encoder-decoder framework on open domain multi turn dialogue generation. However, using RNN-based model to hierarchically encoding the utterances to obtain the representation of dialogue history still face the problem of a vanishing gradient. To address this issue, in this paper, we proposed a static and dynamic attention-based approach to model the dialogue history and then generate open domain multi turn dialogue responses. Experimental results on Ubuntu and Opensubtitles datasets verify the effectiveness of the proposed static and dynamic attention-based approach on automatic and human evaluation metrics in various experimental settings. Meanwhile, we also empirically verify the performance of combining the static and dynamic attentions on open domain multi turn dialogue generation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20766&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Wei-Nan Zhang, Yiming Cui, Kaiyan Zhang, Yifa Wang, Qingfu Zhu, Lingzhi Li, Ting Liu</name></author><summary type="html">Recently, research on open domain dialogue systems have attracted extensive interests of academic and industrial researchers. The goal of an open domain dialogue system is to imitate humans in conversations. Previous works on single turn conversation generation have greatly promoted the research of open domain dialogue systems. However, understanding multiple single turn conversations is not equal to the understanding of multi turn dialogue due to the coherent and context dependent properties of human dialogue. Therefore, in open domain multi turn dialogue generation, it is essential to modeling the contextual semantics of the dialogue history, rather than only according to the last utterance. Previous research had verified the effectiveness of the hierarchical recurrent encoder-decoder framework on open domain multi turn dialogue generation. However, using RNN-based model to hierarchically encoding the utterances to obtain the representation of dialogue history still face the problem of a vanishing gradient. To address this issue, in this paper, we proposed a static and dynamic attention-based approach to model the dialogue history and then generate open domain multi turn dialogue responses. Experimental results on Ubuntu and Opensubtitles datasets verify the effectiveness of the proposed static and dynamic attention-based approach on automatic and human evaluation metrics in various experimental settings. Meanwhile, we also empirically verify the performance of combining the static and dynamic attentions on open domain multi turn dialogue generation.</summary></entry><entry><title type="html">A Statistical Analysis of Deep Federated Learning for Intrinsically Low-dimensional Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AStatisticalAnalysisofDeepFederatedLearningforIntrinsicallyLowdimensionalData.html" rel="alternate" type="text/html" title="A Statistical Analysis of Deep Federated Learning for Intrinsically Low-dimensional Data" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AStatisticalAnalysisofDeepFederatedLearningforIntrinsicallyLowdimensionalData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AStatisticalAnalysisofDeepFederatedLearningforIntrinsicallyLowdimensionalData.html">&lt;p&gt;Federated Learning (FL) has emerged as a groundbreaking paradigm in collaborative machine learning, emphasizing decentralized model training to address data privacy concerns. While significant progress has been made in optimizing federated learning, the exploration of generalization error, particularly in heterogeneous settings, has been limited, focusing mainly on parametric cases. This paper investigates the generalization properties of deep federated regression within a two-stage sampling model. Our findings highlight that the intrinsic dimension, defined by the entropic dimension, is crucial for determining convergence rates when appropriate network sizes are used. Specifically, if the true relationship between response and explanatory variables is charecterized by a $\beta$-H&quot;older function and there are $n$ independent and identically distributed (i.i.d.) samples from $m$ participating clients, the error rate for participating clients scales at most as $\tilde{O}\left((mn)^{-2\beta/(2\beta + \bar{d}&lt;em&gt;{2\beta}(\lambda))}\right)$, and for non-participating clients, it scales as $\tilde{O}\left(\Delta \cdot m^{-2\beta/(2\beta + \bar{d}&lt;/em&gt;{2\beta}(\lambda))} + (mn)^{-2\beta/(2\beta + \bar{d}&lt;em&gt;{2\beta}(\lambda))}\right)$. Here, $\bar{d}&lt;/em&gt;{2\beta}(\lambda)$ represents the $2\beta$-entropic dimension of $\lambda$, the marginal distribution of the explanatory variables, and $\Delta$ characterizes the dependence between the sampling stages. Our results explicitly account for the “closeness” of clients, demonstrating that the convergence rates of deep federated learners depend on intrinsic rather than nominal high-dimensionality.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20659&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Saptarshi Chakraborty, Peter L. Bartlett</name></author><category term="stat.ML," /><category term="stat.TH" /><summary type="html">Federated Learning (FL) has emerged as a groundbreaking paradigm in collaborative machine learning, emphasizing decentralized model training to address data privacy concerns. While significant progress has been made in optimizing federated learning, the exploration of generalization error, particularly in heterogeneous settings, has been limited, focusing mainly on parametric cases. This paper investigates the generalization properties of deep federated regression within a two-stage sampling model. Our findings highlight that the intrinsic dimension, defined by the entropic dimension, is crucial for determining convergence rates when appropriate network sizes are used. Specifically, if the true relationship between response and explanatory variables is charecterized by a $\beta$-H&quot;older function and there are $n$ independent and identically distributed (i.i.d.) samples from $m$ participating clients, the error rate for participating clients scales at most as $\tilde{O}\left((mn)^{-2\beta/(2\beta + \bar{d}{2\beta}(\lambda))}\right)$, and for non-participating clients, it scales as $\tilde{O}\left(\Delta \cdot m^{-2\beta/(2\beta + \bar{d}{2\beta}(\lambda))} + (mn)^{-2\beta/(2\beta + \bar{d}{2\beta}(\lambda))}\right)$. Here, $\bar{d}{2\beta}(\lambda)$ represents the $2\beta$-entropic dimension of $\lambda$, the marginal distribution of the explanatory variables, and $\Delta$ characterizes the dependence between the sampling stages. Our results explicitly account for the “closeness” of clients, demonstrating that the convergence rates of deep federated learners depend on intrinsic rather than nominal high-dimensionality.</summary></entry><entry><title type="html">A Stein Gradient Descent Approach for Doubly Intractable Distributions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ASteinGradientDescentApproachforDoublyIntractableDistributions.html" rel="alternate" type="text/html" title="A Stein Gradient Descent Approach for Doubly Intractable Distributions" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ASteinGradientDescentApproachforDoublyIntractableDistributions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ASteinGradientDescentApproachforDoublyIntractableDistributions.html">&lt;p&gt;Bayesian inference for doubly intractable distributions is challenging because they include intractable terms, which are functions of parameters of interest. Although several alternatives have been developed for such models, they are computationally intensive due to repeated auxiliary variable simulations. We propose a novel Monte Carlo Stein variational gradient descent (MC-SVGD) approach for inference for doubly intractable distributions. Through an efficient gradient approximation, our MC-SVGD approach rapidly transforms an arbitrary reference distribution to approximate the posterior distribution of interest, without necessitating any predefined variational distribution class for the posterior. Such a transport map is obtained by minimizing Kullback-Leibler divergence between the transformed and posterior distributions in a reproducing kernel Hilbert space (RKHS). We also investigate the convergence rate of the proposed method. We illustrate the application of the method to challenging examples, including a Potts model, an exponential random graph model, and a Conway–Maxwell–Poisson regression model. The proposed method achieves substantial computational gains over existing algorithms, while providing comparable inferential performance for the posterior distributions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.21021&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Heesang Lee, Songhee Kim, Bokgyeong Kang, Jaewoo Park</name></author><category term="stat.ML," /><category term="stat.TH" /><summary type="html">Bayesian inference for doubly intractable distributions is challenging because they include intractable terms, which are functions of parameters of interest. Although several alternatives have been developed for such models, they are computationally intensive due to repeated auxiliary variable simulations. We propose a novel Monte Carlo Stein variational gradient descent (MC-SVGD) approach for inference for doubly intractable distributions. Through an efficient gradient approximation, our MC-SVGD approach rapidly transforms an arbitrary reference distribution to approximate the posterior distribution of interest, without necessitating any predefined variational distribution class for the posterior. Such a transport map is obtained by minimizing Kullback-Leibler divergence between the transformed and posterior distributions in a reproducing kernel Hilbert space (RKHS). We also investigate the convergence rate of the proposed method. We illustrate the application of the method to challenging examples, including a Potts model, an exponential random graph model, and a Conway–Maxwell–Poisson regression model. The proposed method achieves substantial computational gains over existing algorithms, while providing comparable inferential performance for the posterior distributions.</summary></entry><entry><title type="html">A Survey of Large Language Models for Arabic Language and its Dialects</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ASurveyofLargeLanguageModelsforArabicLanguageanditsDialects.html" rel="alternate" type="text/html" title="A Survey of Large Language Models for Arabic Language and its Dialects" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ASurveyofLargeLanguageModelsforArabicLanguageanditsDialects</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ASurveyofLargeLanguageModelsforArabicLanguageanditsDialects.html">&lt;p&gt;This survey offers a comprehensive overview of Large Language Models (LLMs) designed for Arabic language and its dialects. It covers key architectures, including encoder-only, decoder-only, and encoder-decoder models, along with the datasets used for pre-training, spanning Classical Arabic, Modern Standard Arabic, and Dialectal Arabic. The study also explores monolingual, bilingual, and multilingual LLMs, analyzing their architectures and performance across downstream tasks, such as sentiment analysis, named entity recognition, and question answering. Furthermore, it assesses the openness of Arabic LLMs based on factors, such as source code availability, training data, model weights, and documentation. The survey highlights the need for more diverse dialectal datasets and attributes the importance of openness for research reproducibility and transparency. It concludes by identifying key challenges and opportunities for future research and stressing the need for more inclusive and representative models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20238&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Malak Mashaabi, Shahad Al-Khalifa, Hend Al-Khalifa</name></author><summary type="html">This survey offers a comprehensive overview of Large Language Models (LLMs) designed for Arabic language and its dialects. It covers key architectures, including encoder-only, decoder-only, and encoder-decoder models, along with the datasets used for pre-training, spanning Classical Arabic, Modern Standard Arabic, and Dialectal Arabic. The study also explores monolingual, bilingual, and multilingual LLMs, analyzing their architectures and performance across downstream tasks, such as sentiment analysis, named entity recognition, and question answering. Furthermore, it assesses the openness of Arabic LLMs based on factors, such as source code availability, training data, model weights, and documentation. The survey highlights the need for more diverse dialectal datasets and attributes the importance of openness for research reproducibility and transparency. It concludes by identifying key challenges and opportunities for future research and stressing the need for more inclusive and representative models.</summary></entry><entry><title type="html">A Systematic Review of Machine Learning Approaches for Detecting Deceptive Activities on Social Media: Methods, Challenges, and Biases</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ASystematicReviewofMachineLearningApproachesforDetectingDeceptiveActivitiesonSocialMediaMethodsChallengesandBiases.html" rel="alternate" type="text/html" title="A Systematic Review of Machine Learning Approaches for Detecting Deceptive Activities on Social Media: Methods, Challenges, and Biases" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ASystematicReviewofMachineLearningApproachesforDetectingDeceptiveActivitiesonSocialMediaMethodsChallengesandBiases</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ASystematicReviewofMachineLearningApproachesforDetectingDeceptiveActivitiesonSocialMediaMethodsChallengesandBiases.html">&lt;p&gt;Social media platforms like Twitter, Facebook, and Instagram have facilitated the spread of misinformation, necessitating automated detection systems. This systematic review evaluates 36 studies that apply machine learning (ML) and deep learning (DL) models to detect fake news, spam, and fake accounts on social media. Using the Prediction model Risk Of Bias ASsessment Tool (PROBAST), the review identified key biases across the ML lifecycle: selection bias due to non-representative sampling, inadequate handling of class imbalance, insufficient linguistic preprocessing (e.g., negations), and inconsistent hyperparameter tuning. Although models such as Support Vector Machines (SVM), Random Forests, and Long Short-Term Memory (LSTM) networks showed strong potential, over-reliance on accuracy as an evaluation metric in imbalanced data settings was a common flaw. The review highlights the need for improved data preprocessing (e.g., resampling techniques), consistent hyperparameter tuning, and the use of appropriate metrics like precision, recall, F1 score, and AUROC. Addressing these limitations can lead to more reliable and generalizable ML/DL models for detecting deceptive content, ultimately contributing to the reduction of misinformation on social media.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20293&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yunchong Liu, Xiaorui Shen, Yeyubei Zhang, Zhongyan Wang, Yexin Tian, Jianglai Dai, Yuchen Cao</name></author><category term="stat.ML" /><summary type="html">Social media platforms like Twitter, Facebook, and Instagram have facilitated the spread of misinformation, necessitating automated detection systems. This systematic review evaluates 36 studies that apply machine learning (ML) and deep learning (DL) models to detect fake news, spam, and fake accounts on social media. Using the Prediction model Risk Of Bias ASsessment Tool (PROBAST), the review identified key biases across the ML lifecycle: selection bias due to non-representative sampling, inadequate handling of class imbalance, insufficient linguistic preprocessing (e.g., negations), and inconsistent hyperparameter tuning. Although models such as Support Vector Machines (SVM), Random Forests, and Long Short-Term Memory (LSTM) networks showed strong potential, over-reliance on accuracy as an evaluation metric in imbalanced data settings was a common flaw. The review highlights the need for improved data preprocessing (e.g., resampling techniques), consistent hyperparameter tuning, and the use of appropriate metrics like precision, recall, F1 score, and AUROC. Addressing these limitations can lead to more reliable and generalizable ML/DL models for detecting deceptive content, ultimately contributing to the reduction of misinformation on social media.</summary></entry><entry><title type="html">A Systematic Survey on Large Language Models for Algorithm Design</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ASystematicSurveyonLargeLanguageModelsforAlgorithmDesign.html" rel="alternate" type="text/html" title="A Systematic Survey on Large Language Models for Algorithm Design" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ASystematicSurveyonLargeLanguageModelsforAlgorithmDesign</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ASystematicSurveyonLargeLanguageModelsforAlgorithmDesign.html">&lt;p&gt;Algorithm Design (AD) is crucial for effective problem-solving across various domains. The advent of Large Language Models (LLMs) has notably enhanced the automation and innovation within this field, offering new perspectives and promising solutions. Over the past three years, the integration of LLMs into AD (LLM4AD) has progressed significantly, finding applications in diverse areas such as optimization, machine learning, mathematical reasoning, and scientific discovery. Given the rapid development and broadening scope of this field, a systematic review is both timely and essential. This paper provides a systematic review of the works on LLM4AD. First, we present an overview and summary of existing studies. Then, we present a systematic taxonomy and a review of existing works along four dimensions, including the role of LLMs, search techniques, prompt strategies, and applications, with a discussion of the potential and achievements of using LLMs. Finally, we explore current challenges and propose several open questions and promising directions for future research.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.14716&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Fei Liu, Yiming Yao, Ping Guo, Zhiyuan Yang, Zhe Zhao, Xi Lin, Xialiang Tong, Mingxuan Yuan, Zhichao Lu, Zhenkun Wang, Qingfu Zhang</name></author><summary type="html">Algorithm Design (AD) is crucial for effective problem-solving across various domains. The advent of Large Language Models (LLMs) has notably enhanced the automation and innovation within this field, offering new perspectives and promising solutions. Over the past three years, the integration of LLMs into AD (LLM4AD) has progressed significantly, finding applications in diverse areas such as optimization, machine learning, mathematical reasoning, and scientific discovery. Given the rapid development and broadening scope of this field, a systematic review is both timely and essential. This paper provides a systematic review of the works on LLM4AD. First, we present an overview and summary of existing studies. Then, we present a systematic taxonomy and a review of existing works along four dimensions, including the role of LLMs, search techniques, prompt strategies, and applications, with a discussion of the potential and achievements of using LLMs. Finally, we explore current challenges and propose several open questions and promising directions for future research.</summary></entry><entry><title type="html">A Taxonomy of Loss Functions for Stochastic Optimal Control</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ATaxonomyofLossFunctionsforStochasticOptimalControl.html" rel="alternate" type="text/html" title="A Taxonomy of Loss Functions for Stochastic Optimal Control" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ATaxonomyofLossFunctionsforStochasticOptimalControl</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ATaxonomyofLossFunctionsforStochasticOptimalControl.html">&lt;p&gt;Stochastic optimal control (SOC) aims to direct the behavior of noisy systems and has widespread applications in science, engineering, and artificial intelligence. In particular, reward fine-tuning of diffusion and flow matching models and sampling from unnormalized methods can be recast as SOC problems. A recent work has introduced Adjoint Matching (Domingo-Enrich et al., 2024), a loss function for SOC problems that vastly outperforms existing loss functions in the reward fine-tuning setup. The goal of this work is to clarify the connections between all the existing (and some new) SOC loss functions. Namely, we show that SOC loss functions can be grouped into classes that share the same gradient in expectation, which means that their optimization landscape is the same; they only differ in their gradient variance. We perform simple SOC experiments to understand the strengths and weaknesses of different loss functions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.00345&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Carles Domingo-Enrich</name></author><category term="stat.ML" /><summary type="html">Stochastic optimal control (SOC) aims to direct the behavior of noisy systems and has widespread applications in science, engineering, and artificial intelligence. In particular, reward fine-tuning of diffusion and flow matching models and sampling from unnormalized methods can be recast as SOC problems. A recent work has introduced Adjoint Matching (Domingo-Enrich et al., 2024), a loss function for SOC problems that vastly outperforms existing loss functions in the reward fine-tuning setup. The goal of this work is to clarify the connections between all the existing (and some new) SOC loss functions. Namely, we show that SOC loss functions can be grouped into classes that share the same gradient in expectation, which means that their optimization landscape is the same; they only differ in their gradient variance. We perform simple SOC experiments to understand the strengths and weaknesses of different loss functions.</summary></entry><entry><title type="html">A Unified Solution to Diverse Heterogeneities in One-shot Federated Learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AUnifiedSolutiontoDiverseHeterogeneitiesinOneshotFederatedLearning.html" rel="alternate" type="text/html" title="A Unified Solution to Diverse Heterogeneities in One-shot Federated Learning" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AUnifiedSolutiontoDiverseHeterogeneitiesinOneshotFederatedLearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AUnifiedSolutiontoDiverseHeterogeneitiesinOneshotFederatedLearning.html">&lt;p&gt;One-shot federated learning (FL) limits the communication between the server and clients to a single round, which largely decreases the privacy leakage risks in traditional FLs requiring multiple communications. However, we find existing one-shot FL frameworks are vulnerable to distributional heterogeneity due to their insufficient focus on data heterogeneity while concentrating predominantly on model heterogeneity. Filling this gap, we propose a unified, data-free, one-shot federated learning framework (FedHydra) that can effectively address both model and data heterogeneity. Rather than applying existing value-only learning mechanisms, a structure-value learning mechanism is proposed in FedHydra. Specifically, a new stratified learning structure is proposed to cover data heterogeneity, and the value of each item during computation reflects model heterogeneity. By this design, the data and model heterogeneity issues are simultaneously monitored from different aspects during learning. Consequently, FedHydra can effectively mitigate both issues by minimizing their inherent conflicts. We compared FedHydra with three SOTA baselines on four benchmark datasets. Experimental results show that our method outperforms the previous one-shot FL methods in both homogeneous and heterogeneous settings.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.21119&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jun Bai, Yiliao Song, Di Wu, Atul Sajjanhar, Yong Xiang, Wei Zhou, Xiaohui Tao, Yan Li</name></author><summary type="html">One-shot federated learning (FL) limits the communication between the server and clients to a single round, which largely decreases the privacy leakage risks in traditional FLs requiring multiple communications. However, we find existing one-shot FL frameworks are vulnerable to distributional heterogeneity due to their insufficient focus on data heterogeneity while concentrating predominantly on model heterogeneity. Filling this gap, we propose a unified, data-free, one-shot federated learning framework (FedHydra) that can effectively address both model and data heterogeneity. Rather than applying existing value-only learning mechanisms, a structure-value learning mechanism is proposed in FedHydra. Specifically, a new stratified learning structure is proposed to cover data heterogeneity, and the value of each item during computation reflects model heterogeneity. By this design, the data and model heterogeneity issues are simultaneously monitored from different aspects during learning. Consequently, FedHydra can effectively mitigate both issues by minimizing their inherent conflicts. We compared FedHydra with three SOTA baselines on four benchmark datasets. Experimental results show that our method outperforms the previous one-shot FL methods in both homogeneous and heterogeneous settings.</summary></entry><entry><title type="html">Accelerated Bayesian parameter estimation and model selection for gravitational waves with normalizing flows</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AcceleratedBayesianparameterestimationandmodelselectionforgravitationalwaveswithnormalizingflows.html" rel="alternate" type="text/html" title="Accelerated Bayesian parameter estimation and model selection for gravitational waves with normalizing flows" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AcceleratedBayesianparameterestimationandmodelselectionforgravitationalwaveswithnormalizingflows</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AcceleratedBayesianparameterestimationandmodelselectionforgravitationalwaveswithnormalizingflows.html">&lt;p&gt;We present an accelerated pipeline, based on high-performance computing techniques and normalizing flows, for joint Bayesian parameter estimation and model selection and demonstrate its efficiency in gravitational wave astrophysics. We integrate the Jim inference toolkit, a normalizing flow-enhanced Markov chain Monte Carlo (MCMC) sampler, with the learned harmonic mean estimator. Our Bayesian evidence estimates run on $1$ GPU are consistent with traditional nested sampling techniques run on $16$ CPU cores, while reducing the computation time by factors of $5\times$ and $15\times$ for $4$-dimensional and $11$-dimensional gravitational wave inference problems, respectively. Our code is available in well-tested and thoroughly documented open-source packages, ensuring accessibility and reproducibility for the wider research community.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.21076&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alicja Polanska, Thibeau Wouters, Peter T. H. Pang, Kaze K. W. Wong, Jason D. McEwen</name></author><summary type="html">We present an accelerated pipeline, based on high-performance computing techniques and normalizing flows, for joint Bayesian parameter estimation and model selection and demonstrate its efficiency in gravitational wave astrophysics. We integrate the Jim inference toolkit, a normalizing flow-enhanced Markov chain Monte Carlo (MCMC) sampler, with the learned harmonic mean estimator. Our Bayesian evidence estimates run on $1$ GPU are consistent with traditional nested sampling techniques run on $16$ CPU cores, while reducing the computation time by factors of $5\times$ and $15\times$ for $4$-dimensional and $11$-dimensional gravitational wave inference problems, respectively. Our code is available in well-tested and thoroughly documented open-source packages, ensuring accessibility and reproducibility for the wider research community.</summary></entry><entry><title type="html">Accelerating Direct Preference Optimization with Prefix Sharing</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AcceleratingDirectPreferenceOptimizationwithPrefixSharing.html" rel="alternate" type="text/html" title="Accelerating Direct Preference Optimization with Prefix Sharing" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AcceleratingDirectPreferenceOptimizationwithPrefixSharing</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AcceleratingDirectPreferenceOptimizationwithPrefixSharing.html">&lt;p&gt;Offline paired preference optimization algorithms have become a popular approach for fine-tuning on preference data, outperforming traditional supervised fine-tuning in various tasks. However, traditional implementations often involve redundant computations, especially for tasks with long shared prompts. We introduce prefix sharing for preference tuning, a novel technique that processes chosen and rejected responses as one sequence with a shared prefix. To prevent cross-response contamination, we use a custom block-sparse attention mask. Our method achieves $1.1$-$1.5\times$ improvement in training throughput on popular DPO datasets, without any effect on convergence. When combined with sequence packing, we observe consistent $1.3$-$1.6\times$ speedups, benefiting even datasets with smaller sequence lengths. While we focus on Direct Preference Optimization (DPO), our approach is applicable to other paired preference tuning methods. By enhancing computational efficiency, our work contributes to making preference-based fine-tuning more accessible for a wider range of applications and model sizes. We open-source our code at https://github.com/frankxwang/dpo-prefix-sharing.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20305&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Franklin Wang, Sumanth Hegde</name></author><summary type="html">Offline paired preference optimization algorithms have become a popular approach for fine-tuning on preference data, outperforming traditional supervised fine-tuning in various tasks. However, traditional implementations often involve redundant computations, especially for tasks with long shared prompts. We introduce prefix sharing for preference tuning, a novel technique that processes chosen and rejected responses as one sequence with a shared prefix. To prevent cross-response contamination, we use a custom block-sparse attention mask. Our method achieves $1.1$-$1.5\times$ improvement in training throughput on popular DPO datasets, without any effect on convergence. When combined with sequence packing, we observe consistent $1.3$-$1.6\times$ speedups, benefiting even datasets with smaller sequence lengths. While we focus on Direct Preference Optimization (DPO), our approach is applicable to other paired preference tuning methods. By enhancing computational efficiency, our work contributes to making preference-based fine-tuning more accessible for a wider range of applications and model sizes. We open-source our code at https://github.com/frankxwang/dpo-prefix-sharing.</summary></entry><entry><title type="html">Accelerating Nash Equilibrium Convergence in Monte Carlo Settings Through Counterfactual Value Based Fictitious Play</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AcceleratingNashEquilibriumConvergenceinMonteCarloSettingsThroughCounterfactualValueBasedFictitiousPlay.html" rel="alternate" type="text/html" title="Accelerating Nash Equilibrium Convergence in Monte Carlo Settings Through Counterfactual Value Based Fictitious Play" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AcceleratingNashEquilibriumConvergenceinMonteCarloSettingsThroughCounterfactualValueBasedFictitiousPlay</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AcceleratingNashEquilibriumConvergenceinMonteCarloSettingsThroughCounterfactualValueBasedFictitiousPlay.html">&lt;p&gt;Counterfactual Regret Minimization (CFR) and its variants are widely recognized as effective algorithms for solving extensive-form imperfect information games. Recently, many improvements have been focused on enhancing the convergence speed of the CFR algorithm. However, most of these variants are not applicable under Monte Carlo (MC) conditions, making them unsuitable for training in large-scale games. We introduce a new MC-based algorithm for solving extensive-form imperfect information games, called MCCFVFP (Monte Carlo Counterfactual Value-Based Fictitious Play). MCCFVFP combines CFR’s counterfactual value calculations with fictitious play’s best response strategy, leveraging the strengths of fictitious play to gain significant advantages in games with a high proportion of dominated strategies. Experimental results show that MCCFVFP achieved convergence speeds approximately 20\%$\sim$50\% faster than the most advanced MCCFR variants in games like poker and other test games.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.03084&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ju Qi, Falin Hei, Ting Feng, Dengbing Yi, Zhemei Fang, Yunfeng Luo</name></author><summary type="html">Counterfactual Regret Minimization (CFR) and its variants are widely recognized as effective algorithms for solving extensive-form imperfect information games. Recently, many improvements have been focused on enhancing the convergence speed of the CFR algorithm. However, most of these variants are not applicable under Monte Carlo (MC) conditions, making them unsuitable for training in large-scale games. We introduce a new MC-based algorithm for solving extensive-form imperfect information games, called MCCFVFP (Monte Carlo Counterfactual Value-Based Fictitious Play). MCCFVFP combines CFR’s counterfactual value calculations with fictitious play’s best response strategy, leveraging the strengths of fictitious play to gain significant advantages in games with a high proportion of dominated strategies. Experimental results show that MCCFVFP achieved convergence speeds approximately 20\%$\sim$50\% faster than the most advanced MCCFR variants in games like poker and other test games.</summary></entry><entry><title type="html">Accelerating Transformer Pre-training with 2:4 Sparsity</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AcceleratingTransformerPretrainingwith24Sparsity.html" rel="alternate" type="text/html" title="Accelerating Transformer Pre-training with 2:4 Sparsity" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AcceleratingTransformerPretrainingwith24Sparsity</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AcceleratingTransformerPretrainingwith24Sparsity.html">&lt;p&gt;Training large transformers is slow, but recent innovations on GPU architecture give us an advantage. NVIDIA Ampere GPUs can execute a fine-grained 2:4 sparse matrix multiplication twice as fast as its dense equivalent. In the light of this property, we comprehensively investigate the feasibility of accelerating feed-forward networks (FFNs) of transformers in pre-training. First, we define a ``flip rate’’ to monitor the stability of a 2:4 training process. Utilizing this metric, we propose three techniques to preserve accuracy: to modify the sparse-refined straight-through estimator by applying the masked decay term on gradients, to determine a feasible decay factor in warm-up stage, and to enhance the model’s quality by a dense fine-tuning procedure near the end of pre-training. Besides, we devise two techniques to practically accelerate training: to calculate transposable 2:4 masks by convolution, and to accelerate gated activation functions by reducing GPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm achieves similar convergence to dense training algorithms on several transformer pre-training tasks, while actual acceleration can be observed on different shapes of transformer block apparently. Our toolkit is available at https://github.com/huyz2023/2by4-pretrain.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.01847&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuezhou Hu, Kang Zhao, Weiyu Huang, Jianfei Chen, Jun Zhu</name></author><summary type="html">Training large transformers is slow, but recent innovations on GPU architecture give us an advantage. NVIDIA Ampere GPUs can execute a fine-grained 2:4 sparse matrix multiplication twice as fast as its dense equivalent. In the light of this property, we comprehensively investigate the feasibility of accelerating feed-forward networks (FFNs) of transformers in pre-training. First, we define a ``flip rate’’ to monitor the stability of a 2:4 training process. Utilizing this metric, we propose three techniques to preserve accuracy: to modify the sparse-refined straight-through estimator by applying the masked decay term on gradients, to determine a feasible decay factor in warm-up stage, and to enhance the model’s quality by a dense fine-tuning procedure near the end of pre-training. Besides, we devise two techniques to practically accelerate training: to calculate transposable 2:4 masks by convolution, and to accelerate gated activation functions by reducing GPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm achieves similar convergence to dense training algorithms on several transformer pre-training tasks, while actual acceleration can be observed on different shapes of transformer block apparently. Our toolkit is available at https://github.com/huyz2023/2by4-pretrain.</summary></entry><entry><title type="html">Accurate Inference for Penalized Logistic Regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AccurateInferenceforPenalizedLogisticRegression.html" rel="alternate" type="text/html" title="Accurate Inference for Penalized Logistic Regression" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AccurateInferenceforPenalizedLogisticRegression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AccurateInferenceforPenalizedLogisticRegression.html">&lt;p&gt;Inference for high-dimensional logistic regression models using penalized methods has been a challenging research problem. As an illustration, a major difficulty is the significant bias of the Lasso estimator, which limits its direct application in inference. Although various bias corrected Lasso estimators have been proposed, they often still exhibit substantial biases in finite samples, undermining their inference performance. These finite sample biases become particularly problematic in one-sided inference problems, such as one-sided hypothesis testing. This paper proposes a novel two-step procedure for accurate inference in high-dimensional logistic regression models. In the first step, we propose a Lasso-based variable selection method to select a suitable submodel of moderate size for subsequent inference. In the second step, we introduce a bias corrected estimator to fit the selected submodel. We demonstrate that the resulting estimator from this two-step procedure has a small bias order and enables accurate inference. Numerical studies and an analysis of alcohol consumption data are included, where our proposed method is compared to alternative approaches. Our results indicate that the proposed method exhibits significantly smaller biases than alternative methods in finite samples, thereby leading to improved inference performance.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20045&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuming Zhang, Stéphane Guerrier, Runze Li</name></author><category term="stat.ME" /><summary type="html">Inference for high-dimensional logistic regression models using penalized methods has been a challenging research problem. As an illustration, a major difficulty is the significant bias of the Lasso estimator, which limits its direct application in inference. Although various bias corrected Lasso estimators have been proposed, they often still exhibit substantial biases in finite samples, undermining their inference performance. These finite sample biases become particularly problematic in one-sided inference problems, such as one-sided hypothesis testing. This paper proposes a novel two-step procedure for accurate inference in high-dimensional logistic regression models. In the first step, we propose a Lasso-based variable selection method to select a suitable submodel of moderate size for subsequent inference. In the second step, we introduce a bias corrected estimator to fit the selected submodel. We demonstrate that the resulting estimator from this two-step procedure has a small bias order and enables accurate inference. Numerical studies and an analysis of alcohol consumption data are included, where our proposed method is compared to alternative approaches. Our results indicate that the proposed method exhibits significantly smaller biases than alternative methods in finite samples, thereby leading to improved inference performance.</summary></entry><entry><title type="html">Active Causal Structure Learning with Latent Variables: Towards Learning to Detour in Autonomous Robots</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ActiveCausalStructureLearningwithLatentVariablesTowardsLearningtoDetourinAutonomousRobots.html" rel="alternate" type="text/html" title="Active Causal Structure Learning with Latent Variables: Towards Learning to Detour in Autonomous Robots" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ActiveCausalStructureLearningwithLatentVariablesTowardsLearningtoDetourinAutonomousRobots</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ActiveCausalStructureLearningwithLatentVariablesTowardsLearningtoDetourinAutonomousRobots.html">&lt;p&gt;Artificial General Intelligence (AGI) Agents and Robots must be able to cope with everchanging environments and tasks. They must be able to actively construct new internal causal models of their interactions with the environment when new structural changes take place in the environment. Thus, we claim that active causal structure learning with latent variables (ACSLWL) is a necessary component to build AGI agents and robots. This paper describes how a complex planning and expectation-based detour behavior can be learned by ACSLWL when, unexpectedly, and for the first time, the simulated robot encounters a sort of transparent barrier in its pathway towards its target. ACSWL consists of acting in the environment, discovering new causal relations, constructing new causal models, exploiting the causal models to maximize its expected utility, detecting possible latent variables when unexpected observations occur, and constructing new structures-internal causal models and optimal estimation of the associated parameters, to be able to cope efficiently with the new encountered situations. That is, the agent must be able to construct new causal internal models that transform a previously unexpected and inefficient (sub-optimal) situation, into a predictable situation with an optimal operating plan.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20894&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Pablo de los Riscos, Fernando Corbacho</name></author><summary type="html">Artificial General Intelligence (AGI) Agents and Robots must be able to cope with everchanging environments and tasks. They must be able to actively construct new internal causal models of their interactions with the environment when new structural changes take place in the environment. Thus, we claim that active causal structure learning with latent variables (ACSLWL) is a necessary component to build AGI agents and robots. This paper describes how a complex planning and expectation-based detour behavior can be learned by ACSLWL when, unexpectedly, and for the first time, the simulated robot encounters a sort of transparent barrier in its pathway towards its target. ACSWL consists of acting in the environment, discovering new causal relations, constructing new causal models, exploiting the causal models to maximize its expected utility, detecting possible latent variables when unexpected observations occur, and constructing new structures-internal causal models and optimal estimation of the associated parameters, to be able to cope efficiently with the new encountered situations. That is, the agent must be able to construct new causal internal models that transform a previously unexpected and inefficient (sub-optimal) situation, into a predictable situation with an optimal operating plan.</summary></entry><entry><title type="html">Active Legibility in Multiagent Reinforcement Learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ActiveLegibilityinMultiagentReinforcementLearning.html" rel="alternate" type="text/html" title="Active Legibility in Multiagent Reinforcement Learning" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ActiveLegibilityinMultiagentReinforcementLearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ActiveLegibilityinMultiagentReinforcementLearning.html">&lt;p&gt;A multiagent sequential decision problem has been seen in many critical applications including urban transportation, autonomous driving cars, military operations, etc. Its widely known solution, namely multiagent reinforcement learning, has evolved tremendously in recent years. Among them, the solution paradigm of modeling other agents attracts our interest, which is different from traditional value decomposition or communication mechanisms. It enables agents to understand and anticipate others’ behaviors and facilitates their collaboration. Inspired by recent research on the legibility that allows agents to reveal their intentions through their behavior, we propose a multiagent active legibility framework to improve their performance. The legibility-oriented framework allows agents to conduct legible actions so as to help others optimise their behaviors. In addition, we design a series of problem domains that emulate a common scenario and best characterize the legibility in multiagent reinforcement learning. The experimental results demonstrate that the new framework is more efficient and costs less training time compared to several multiagent reinforcement learning algorithms.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20954&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yanyu Liu, Yinghui Pan, Yifeng Zeng, Biyang Ma, Doshi Prashant</name></author><summary type="html">A multiagent sequential decision problem has been seen in many critical applications including urban transportation, autonomous driving cars, military operations, etc. Its widely known solution, namely multiagent reinforcement learning, has evolved tremendously in recent years. Among them, the solution paradigm of modeling other agents attracts our interest, which is different from traditional value decomposition or communication mechanisms. It enables agents to understand and anticipate others’ behaviors and facilitates their collaboration. Inspired by recent research on the legibility that allows agents to reveal their intentions through their behavior, we propose a multiagent active legibility framework to improve their performance. The legibility-oriented framework allows agents to conduct legible actions so as to help others optimise their behaviors. In addition, we design a series of problem domains that emulate a common scenario and best characterize the legibility in multiagent reinforcement learning. The experimental results demonstrate that the new framework is more efficient and costs less training time compared to several multiagent reinforcement learning algorithms.</summary></entry><entry><title type="html">Active Preference Learning for Ordering Items In- and Out-of-sample</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ActivePreferenceLearningforOrderingItemsInandOutofsample.html" rel="alternate" type="text/html" title="Active Preference Learning for Ordering Items In- and Out-of-sample" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ActivePreferenceLearningforOrderingItemsInandOutofsample</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/ActivePreferenceLearningforOrderingItemsInandOutofsample.html">&lt;p&gt;Learning an ordering of items based on pairwise comparisons is useful when items are difficult to rate consistently on an absolute scale, for example, when annotators have to make subjective assessments. When exhaustive comparison is infeasible, actively sampling item pairs can reduce the number of annotations necessary for learning an accurate ordering. However, many algorithms ignore shared structure between items, limiting their sample efficiency and precluding generalization to new items. It is also common to disregard how noise in comparisons varies between item pairs, despite it being informative of item similarity. In this work, we study active preference learning for ordering items with contextual attributes, both in- and out-of-sample. We give an upper bound on the expected ordering error of a logistic preference model as a function of which items have been compared. Next, we propose an active learning strategy that samples items to minimize this bound by accounting for aleatoric and epistemic uncertainty in comparisons. We evaluate the resulting algorithm, and a variant aimed at reducing model misspecification, in multiple realistic ordering tasks with comparisons made by human annotators. Our results demonstrate superior sample efficiency and generalization compared to non-contextual ranking approaches and active preference learning baselines.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.03059&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Herman Bergström, Emil Carlsson, Devdatt Dubhashi, Fredrik D. Johansson</name></author><category term="stat.ML" /><summary type="html">Learning an ordering of items based on pairwise comparisons is useful when items are difficult to rate consistently on an absolute scale, for example, when annotators have to make subjective assessments. When exhaustive comparison is infeasible, actively sampling item pairs can reduce the number of annotations necessary for learning an accurate ordering. However, many algorithms ignore shared structure between items, limiting their sample efficiency and precluding generalization to new items. It is also common to disregard how noise in comparisons varies between item pairs, despite it being informative of item similarity. In this work, we study active preference learning for ordering items with contextual attributes, both in- and out-of-sample. We give an upper bound on the expected ordering error of a logistic preference model as a function of which items have been compared. Next, we propose an active learning strategy that samples items to minimize this bound by accounting for aleatoric and epistemic uncertainty in comparisons. We evaluate the resulting algorithm, and a variant aimed at reducing model misspecification, in multiple realistic ordering tasks with comparisons made by human annotators. Our results demonstrate superior sample efficiency and generalization compared to non-contextual ranking approaches and active preference learning baselines.</summary></entry><entry><title type="html">AdaNeg: Adaptive Negative Proxy Guided OOD Detection with Vision-Language Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdaNegAdaptiveNegativeProxyGuidedOODDetectionwithVisionLanguageModels.html" rel="alternate" type="text/html" title="AdaNeg: Adaptive Negative Proxy Guided OOD Detection with Vision-Language Models" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdaNegAdaptiveNegativeProxyGuidedOODDetectionwithVisionLanguageModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdaNegAdaptiveNegativeProxyGuidedOODDetectionwithVisionLanguageModels.html">&lt;p&gt;Recent research has shown that pre-trained vision-language models are effective at identifying out-of-distribution (OOD) samples by using negative labels as guidance. However, employing consistent negative labels across different OOD datasets often results in semantic misalignments, as these text labels may not accurately reflect the actual space of OOD images. To overcome this issue, we introduce \textit{adaptive negative proxies}, which are dynamically generated during testing by exploring actual OOD images, to align more closely with the underlying OOD label space and enhance the efficacy of negative proxy guidance. Specifically, our approach utilizes a feature memory bank to selectively cache discriminative features from test images, representing the targeted OOD distribution. This facilitates the creation of proxies that can better align with specific OOD datasets. While task-adaptive proxies average features to reflect the unique characteristics of each dataset, the sample-adaptive proxies weight features based on their similarity to individual test samples, exploring detailed sample-level nuances. The final score for identifying OOD samples integrates static negative labels with our proposed adaptive proxies, effectively combining textual and visual knowledge for enhanced performance. Our method is training-free and annotation-free, and it maintains fast testing speed. Extensive experiments across various benchmarks demonstrate the effectiveness of our approach, abbreviated as AdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg significantly outperforms existing methods, with a 2.45\% increase in AUROC and a 6.48\% reduction in FPR95. Codes are available at \url{https://github.com/YBZh/OpenOOD-VLM}.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20149&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yabin Zhang, Lei Zhang</name></author><summary type="html">Recent research has shown that pre-trained vision-language models are effective at identifying out-of-distribution (OOD) samples by using negative labels as guidance. However, employing consistent negative labels across different OOD datasets often results in semantic misalignments, as these text labels may not accurately reflect the actual space of OOD images. To overcome this issue, we introduce \textit{adaptive negative proxies}, which are dynamically generated during testing by exploring actual OOD images, to align more closely with the underlying OOD label space and enhance the efficacy of negative proxy guidance. Specifically, our approach utilizes a feature memory bank to selectively cache discriminative features from test images, representing the targeted OOD distribution. This facilitates the creation of proxies that can better align with specific OOD datasets. While task-adaptive proxies average features to reflect the unique characteristics of each dataset, the sample-adaptive proxies weight features based on their similarity to individual test samples, exploring detailed sample-level nuances. The final score for identifying OOD samples integrates static negative labels with our proposed adaptive proxies, effectively combining textual and visual knowledge for enhanced performance. Our method is training-free and annotation-free, and it maintains fast testing speed. Extensive experiments across various benchmarks demonstrate the effectiveness of our approach, abbreviated as AdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg significantly outperforms existing methods, with a 2.45\% increase in AUROC and a 6.48\% reduction in FPR95. Codes are available at \url{https://github.com/YBZh/OpenOOD-VLM}.</summary></entry><entry><title type="html">Adaptive Mesh Refinement for arbitrary initial Triangulations</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdaptiveMeshRefinementforarbitraryinitialTriangulations.html" rel="alternate" type="text/html" title="Adaptive Mesh Refinement for arbitrary initial Triangulations" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdaptiveMeshRefinementforarbitraryinitialTriangulations</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdaptiveMeshRefinementforarbitraryinitialTriangulations.html">&lt;p&gt;We introduce a simple initialization of the Maubach bisection routine for adaptive mesh refinement which applies to any conforming initial triangulation and terminates in linear time with respect to the number of initial vertices. We show that Maubach’s routine with this initialization generates meshes that preserve shape regularity and satisfy the closure estimate needed for optimal convergence of adaptive schemes. Our ansatz allows for the intrinsic use of existing implementations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.02674&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lars Diening, Lukas Gehring, Johannes Storn</name></author><summary type="html">We introduce a simple initialization of the Maubach bisection routine for adaptive mesh refinement which applies to any conforming initial triangulation and terminates in linear time with respect to the number of initial vertices. We show that Maubach’s routine with this initialization generates meshes that preserve shape regularity and satisfy the closure estimate needed for optimal convergence of adaptive schemes. Our ansatz allows for the intrinsic use of existing implementations.</summary></entry><entry><title type="html">Adaptive Real-Time Multi-Loss Function Optimization Using Dynamic Memory Fusion Framework: A Case Study on Breast Cancer Segmentation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdaptiveRealTimeMultiLossFunctionOptimizationUsingDynamicMemoryFusionFrameworkACaseStudyonBreastCancerSegmentation.html" rel="alternate" type="text/html" title="Adaptive Real-Time Multi-Loss Function Optimization Using Dynamic Memory Fusion Framework: A Case Study on Breast Cancer Segmentation" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdaptiveRealTimeMultiLossFunctionOptimizationUsingDynamicMemoryFusionFrameworkACaseStudyonBreastCancerSegmentation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdaptiveRealTimeMultiLossFunctionOptimizationUsingDynamicMemoryFusionFrameworkACaseStudyonBreastCancerSegmentation.html">&lt;p&gt;Deep learning has proven to be a highly effective tool for a wide range of applications, significantly when leveraging the power of multi-loss functions to optimize performance on multiple criteria simultaneously. However, optimal selection and weighting loss functions in deep learning tasks can significantly influence model performance, yet manual tuning of these functions is often inefficient and inflexible. We propose a novel framework called dynamic memory fusion for adaptive multi-loss function penalizing in real-time to address this. This framework leverages historical loss values data to dynamically adjust the weighting of multiple loss functions throughout the training process. Additionally, this framework integrates an auxiliary loss function to enhance model performance in the early stages. To further research horizons, we introduce the class-balanced dice loss function, designed to address class imbalance by prioritizing underrepresented classes. Experiments on breast ultrasound datasets demonstrate that the framework improves segmentation performance across various metrics. These results demonstrate the effectiveness of our proposed framework in ensuring that the model dynamically adjusts its focus to prioritize the most relevant criteria, leading to improved performance in evolving environments. The source code for our proposed methodology is publicly available on GitHub.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.19745&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Amin Golnari, Mostafa Diba</name></author><summary type="html">Deep learning has proven to be a highly effective tool for a wide range of applications, significantly when leveraging the power of multi-loss functions to optimize performance on multiple criteria simultaneously. However, optimal selection and weighting loss functions in deep learning tasks can significantly influence model performance, yet manual tuning of these functions is often inefficient and inflexible. We propose a novel framework called dynamic memory fusion for adaptive multi-loss function penalizing in real-time to address this. This framework leverages historical loss values data to dynamically adjust the weighting of multiple loss functions throughout the training process. Additionally, this framework integrates an auxiliary loss function to enhance model performance in the early stages. To further research horizons, we introduce the class-balanced dice loss function, designed to address class imbalance by prioritizing underrepresented classes. Experiments on breast ultrasound datasets demonstrate that the framework improves segmentation performance across various metrics. These results demonstrate the effectiveness of our proposed framework in ensuring that the model dynamically adjusts its focus to prioritize the most relevant criteria, leading to improved performance in evolving environments. The source code for our proposed methodology is publicly available on GitHub.</summary></entry><entry><title type="html">Adaptive Reorganization of Neural Pathways for Continual Learning with Spiking Neural Networks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdaptiveReorganizationofNeuralPathwaysforContinualLearningwithSpikingNeuralNetworks.html" rel="alternate" type="text/html" title="Adaptive Reorganization of Neural Pathways for Continual Learning with Spiking Neural Networks" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdaptiveReorganizationofNeuralPathwaysforContinualLearningwithSpikingNeuralNetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdaptiveReorganizationofNeuralPathwaysforContinualLearningwithSpikingNeuralNetworks.html">&lt;p&gt;The human brain can self-organize rich and diverse sparse neural pathways to incrementally master hundreds of cognitive tasks. However, most existing continual learning algorithms for deep artificial and spiking neural networks are unable to adequately auto-regulate the limited resources in the network, which leads to performance drop along with energy consumption rise as the increase of tasks. In this paper, we propose a brain-inspired continual learning algorithm with adaptive reorganization of neural pathways, which employs Self-Organizing Regulation networks to reorganize the single and limited Spiking Neural Network (SOR-SNN) into rich sparse neural pathways to efficiently cope with incremental tasks. The proposed model demonstrates consistent superiority in performance, energy consumption, and memory capacity on diverse continual learning tasks ranging from child-like simple to complex tasks, as well as on generalized CIFAR100 and ImageNet datasets. In particular, the SOR-SNN model excels at learning more complex tasks as well as more tasks, and is able to integrate the past learned knowledge with the information from the current task, showing the backward transfer ability to facilitate the old tasks. Meanwhile, the proposed model exhibits self-repairing ability to irreversible damage and for pruned networks, could automatically allocate new pathway from the retained network to recover memory for forgotten knowledge.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.09550&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bing Han, Feifei Zhao, Wenxuan Pan, Zhaoya Zhao, Xianqi Li, Qingqun Kong, Yi Zeng</name></author><summary type="html">The human brain can self-organize rich and diverse sparse neural pathways to incrementally master hundreds of cognitive tasks. However, most existing continual learning algorithms for deep artificial and spiking neural networks are unable to adequately auto-regulate the limited resources in the network, which leads to performance drop along with energy consumption rise as the increase of tasks. In this paper, we propose a brain-inspired continual learning algorithm with adaptive reorganization of neural pathways, which employs Self-Organizing Regulation networks to reorganize the single and limited Spiking Neural Network (SOR-SNN) into rich sparse neural pathways to efficiently cope with incremental tasks. The proposed model demonstrates consistent superiority in performance, energy consumption, and memory capacity on diverse continual learning tasks ranging from child-like simple to complex tasks, as well as on generalized CIFAR100 and ImageNet datasets. In particular, the SOR-SNN model excels at learning more complex tasks as well as more tasks, and is able to integrate the past learned knowledge with the information from the current task, showing the backward transfer ability to facilitate the old tasks. Meanwhile, the proposed model exhibits self-repairing ability to irreversible damage and for pruned networks, could automatically allocate new pathway from the retained network to recover memory for forgotten knowledge.</summary></entry><entry><title type="html">Adaptive Transfer Clustering: A Unified Framework</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdaptiveTransferClusteringAUnifiedFramework.html" rel="alternate" type="text/html" title="Adaptive Transfer Clustering: A Unified Framework" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdaptiveTransferClusteringAUnifiedFramework</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdaptiveTransferClusteringAUnifiedFramework.html">&lt;p&gt;We propose a general transfer learning framework for clustering given a main dataset and an auxiliary one about the same subjects. The two datasets may reflect similar but different latent grouping structures of the subjects. We propose an adaptive transfer clustering (ATC) algorithm that automatically leverages the commonality in the presence of unknown discrepancy, by optimizing an estimated bias-variance decomposition. It applies to a broad class of statistical models including Gaussian mixture models, stochastic block models, and latent class models. A theoretical analysis proves the optimality of ATC under the Gaussian mixture model and explicitly quantifies the benefit of transfer. Extensive simulations and real data experiments confirm our method’s effectiveness in various scenarios.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.21263&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuqi Gu, Zhongyuan Lyu, Kaizheng Wang</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">We propose a general transfer learning framework for clustering given a main dataset and an auxiliary one about the same subjects. The two datasets may reflect similar but different latent grouping structures of the subjects. We propose an adaptive transfer clustering (ATC) algorithm that automatically leverages the commonality in the presence of unknown discrepancy, by optimizing an estimated bias-variance decomposition. It applies to a broad class of statistical models including Gaussian mixture models, stochastic block models, and latent class models. A theoretical analysis proves the optimality of ATC under the Gaussian mixture model and explicitly quantifies the benefit of transfer. Extensive simulations and real data experiments confirm our method’s effectiveness in various scenarios.</summary></entry><entry><title type="html">Adaptive Video Understanding Agent: Enhancing efficiency with dynamic frame sampling and feedback-driven reasoning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdaptiveVideoUnderstandingAgentEnhancingefficiencywithdynamicframesamplingandfeedbackdrivenreasoning.html" rel="alternate" type="text/html" title="Adaptive Video Understanding Agent: Enhancing efficiency with dynamic frame sampling and feedback-driven reasoning" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdaptiveVideoUnderstandingAgentEnhancingefficiencywithdynamicframesamplingandfeedbackdrivenreasoning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdaptiveVideoUnderstandingAgentEnhancingefficiencywithdynamicframesamplingandfeedbackdrivenreasoning.html">&lt;p&gt;Understanding long-form video content presents significant challenges due to its temporal complexity and the substantial computational resources required. In this work, we propose an agent-based approach to enhance both the efficiency and effectiveness of long-form video understanding by utilizing large language models (LLMs) and their tool-harnessing ability. A key aspect of our method is query-adaptive frame sampling, which leverages the reasoning capabilities of LLMs to process only the most relevant frames in real-time, and addresses an important limitation of existing methods which typically involve sampling redundant or irrelevant frames. To enhance the reasoning abilities of our video-understanding agent, we leverage the self-reflective capabilities of LLMs to provide verbal reinforcement to the agent, which leads to improved performance while minimizing the number of frames accessed. We evaluate our method across several video understanding benchmarks and demonstrate that not only it enhances state-of-the-art performance but also improves efficiency by reducing the number of frames sampled.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20252&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sullam Jeoung, Goeric Huybrechts, Bhavana Ganesh, Aram Galstyan, Sravan Bodapati</name></author><summary type="html">Understanding long-form video content presents significant challenges due to its temporal complexity and the substantial computational resources required. In this work, we propose an agent-based approach to enhance both the efficiency and effectiveness of long-form video understanding by utilizing large language models (LLMs) and their tool-harnessing ability. A key aspect of our method is query-adaptive frame sampling, which leverages the reasoning capabilities of LLMs to process only the most relevant frames in real-time, and addresses an important limitation of existing methods which typically involve sampling redundant or irrelevant frames. To enhance the reasoning abilities of our video-understanding agent, we leverage the self-reflective capabilities of LLMs to provide verbal reinforcement to the agent, which leads to improved performance while minimizing the number of frames accessed. We evaluate our method across several video understanding benchmarks and demonstrate that not only it enhances state-of-the-art performance but also improves efficiency by reducing the number of frames sampled.</summary></entry><entry><title type="html">Addressing a fundamental limitation in deep vision models: lack of spatial attention</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/Addressingafundamentallimitationindeepvisionmodelslackofspatialattention.html" rel="alternate" type="text/html" title="Addressing a fundamental limitation in deep vision models: lack of spatial attention" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/Addressingafundamentallimitationindeepvisionmodelslackofspatialattention</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/Addressingafundamentallimitationindeepvisionmodelslackofspatialattention.html">&lt;p&gt;The primary aim of this manuscript is to underscore a significant limitation in current deep learning models, particularly vision models. Unlike human vision, which efficiently selects only the essential visual areas for further processing, leading to high speed and low energy consumption, deep vision models process the entire image. In this work, we examine this issue from a broader perspective and propose two solutions that could pave the way for the next generation of more efficient vision models. In the first solution, convolution and pooling operations are selectively applied to altered regions, with a change map sent to subsequent layers. This map indicates which computations need to be repeated. In the second solution, only the modified regions are processed by a semantic segmentation model, and the resulting segments are inserted into the corresponding areas of the previous output map. The code is available at https://github.com/aliborji/spatial_attention.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2407.01782&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ali Borji</name></author><summary type="html">The primary aim of this manuscript is to underscore a significant limitation in current deep learning models, particularly vision models. Unlike human vision, which efficiently selects only the essential visual areas for further processing, leading to high speed and low energy consumption, deep vision models process the entire image. In this work, we examine this issue from a broader perspective and propose two solutions that could pave the way for the next generation of more efficient vision models. In the first solution, convolution and pooling operations are selectively applied to altered regions, with a change map sent to subsequent layers. This map indicates which computations need to be repeated. In the second solution, only the modified regions are processed by a semantic segmentation model, and the resulting segments are inserted into the corresponding areas of the previous output map. The code is available at https://github.com/aliborji/spatial_attention.</summary></entry><entry><title type="html">Addressing the Pitfalls of Image-Based Structural Health Monitoring: A Focus on False Positives, False Negatives, and Base Rate Bias</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AddressingthePitfallsofImageBasedStructuralHealthMonitoringAFocusonFalsePositivesFalseNegativesandBaseRateBias.html" rel="alternate" type="text/html" title="Addressing the Pitfalls of Image-Based Structural Health Monitoring: A Focus on False Positives, False Negatives, and Base Rate Bias" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AddressingthePitfallsofImageBasedStructuralHealthMonitoringAFocusonFalsePositivesFalseNegativesandBaseRateBias</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AddressingthePitfallsofImageBasedStructuralHealthMonitoringAFocusonFalsePositivesFalseNegativesandBaseRateBias.html">&lt;p&gt;This study explores the limitations of image-based structural health monitoring (SHM) techniques in detecting structural damage. Leveraging machine learning and computer vision, image-based SHM offers a scalable and efficient alternative to manual inspections. However, its reliability is impacted by challenges such as false positives, false negatives, and environmental variability, particularly in low base rate damage scenarios. The Base Rate Bias plays a significant role, as low probabilities of actual damage often lead to misinterpretation of positive results. This study uses both Bayesian analysis and a frequentist approach to evaluate the precision of damage detection systems, revealing that even highly accurate models can yield misleading results when the occurrence of damage is rare. Strategies for mitigating these limitations are discussed, including hybrid systems that combine multiple data sources, human-in-the-loop approaches for critical assessments, and improving the quality of training data. These findings provide essential insights into the practical applicability of image-based SHM techniques, highlighting both their potential and their limitations for real-world infrastructure monitoring.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20384&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Vagelis Plevris</name></author><summary type="html">This study explores the limitations of image-based structural health monitoring (SHM) techniques in detecting structural damage. Leveraging machine learning and computer vision, image-based SHM offers a scalable and efficient alternative to manual inspections. However, its reliability is impacted by challenges such as false positives, false negatives, and environmental variability, particularly in low base rate damage scenarios. The Base Rate Bias plays a significant role, as low probabilities of actual damage often lead to misinterpretation of positive results. This study uses both Bayesian analysis and a frequentist approach to evaluate the precision of damage detection systems, revealing that even highly accurate models can yield misleading results when the occurrence of damage is rare. Strategies for mitigating these limitations are discussed, including hybrid systems that combine multiple data sources, human-in-the-loop approaches for critical assessments, and improving the quality of training data. These findings provide essential insights into the practical applicability of image-based SHM techniques, highlighting both their potential and their limitations for real-world infrastructure monitoring.</summary></entry><entry><title type="html">Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with Memoryless Stochastic Optimal Control</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdjointMatchingFinetuningFlowandDiffusionGenerativeModelswithMemorylessStochasticOptimalControl.html" rel="alternate" type="text/html" title="Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with Memoryless Stochastic Optimal Control" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdjointMatchingFinetuningFlowandDiffusionGenerativeModelswithMemorylessStochasticOptimalControl</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdjointMatchingFinetuningFlowandDiffusionGenerativeModelswithMemorylessStochasticOptimalControl.html">&lt;p&gt;Dynamical generative models that produce samples through an iterative process, such as Flow Matching and denoising diffusion models, have seen widespread use, but there have not been many theoretically-sound methods for improving these models with reward fine-tuning. In this work, we cast reward fine-tuning as stochastic optimal control (SOC). Critically, we prove that a very specific memoryless noise schedule must be enforced during fine-tuning, in order to account for the dependency between the noise variable and the generated samples. We also propose a new algorithm named Adjoint Matching which outperforms existing SOC algorithms, by casting SOC problems as a regression problem. We find that our approach significantly improves over existing methods for reward fine-tuning, achieving better consistency, realism, and generalization to unseen human preference reward models, while retaining sample diversity.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2409.08861&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Carles Domingo-Enrich, Michal Drozdzal, Brian Karrer, Ricky T. Q. Chen</name></author><category term="stat.ML" /><summary type="html">Dynamical generative models that produce samples through an iterative process, such as Flow Matching and denoising diffusion models, have seen widespread use, but there have not been many theoretically-sound methods for improving these models with reward fine-tuning. In this work, we cast reward fine-tuning as stochastic optimal control (SOC). Critically, we prove that a very specific memoryless noise schedule must be enforced during fine-tuning, in order to account for the dependency between the noise variable and the generated samples. We also propose a new algorithm named Adjoint Matching which outperforms existing SOC algorithms, by casting SOC problems as a regression problem. We find that our approach significantly improves over existing methods for reward fine-tuning, achieving better consistency, realism, and generalization to unseen human preference reward models, while retaining sample diversity.</summary></entry><entry><title type="html">Advancing Gasoline Consumption Forecasting: A Novel Hybrid Model Integrating Transformers, LSTM, and CNN</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdvancingGasolineConsumptionForecastingANovelHybridModelIntegratingTransformersLSTMandCNN.html" rel="alternate" type="text/html" title="Advancing Gasoline Consumption Forecasting: A Novel Hybrid Model Integrating Transformers, LSTM, and CNN" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdvancingGasolineConsumptionForecastingANovelHybridModelIntegratingTransformersLSTMandCNN</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdvancingGasolineConsumptionForecastingANovelHybridModelIntegratingTransformersLSTMandCNN.html">&lt;p&gt;Iran, endowed with abundant hydrocarbon resources, plays a crucial role in the global energy landscape. Gasoline, as a critical fuel, significantly supports the nation’s transportation sector. Accurate forecasting of gasoline consumption is essential for strategic resource management and environmental planning. This research introduces a novel approach to predicting monthly gasoline consumption using a hybrid Transformer-LSTM-CNN model, which integrates the strengths of Transformer networks, Long Short-Term Memory (LSTM) networks, and Convolutional Neural Networks (CNN). This advanced architecture offers a superior alternative to conventional methods such as artificial neural networks and regression models by capturing both short- and long-term dependencies in time series data. By leveraging the self-attention mechanism of Transformers, the temporal memory of LSTMs, and the local pattern detection of CNNs, our hybrid model delivers improved prediction accuracy. Implemented using Python, the model provides precise future gasoline consumption forecasts and evaluates the environmental impact through the analysis of greenhouse gas emissions. This study examines gasoline consumption trends from 2007 to 2021, which rose from 64.5 million liters per day in 2007 to 99.80 million liters per day in 2021. Our proposed model forecasts consumption levels up to 2031, offering a valuable tool for policymakers and energy analysts. The results highlight the superiority of this hybrid model in improving the accuracy of gasoline consumption forecasts, reinforcing the need for advanced machine learning techniques to optimize resource management and mitigate environmental risks in the energy sector.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.16336&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mahmoud Ranjbar, Mohammad Rahimzadeh</name></author><summary type="html">Iran, endowed with abundant hydrocarbon resources, plays a crucial role in the global energy landscape. Gasoline, as a critical fuel, significantly supports the nation’s transportation sector. Accurate forecasting of gasoline consumption is essential for strategic resource management and environmental planning. This research introduces a novel approach to predicting monthly gasoline consumption using a hybrid Transformer-LSTM-CNN model, which integrates the strengths of Transformer networks, Long Short-Term Memory (LSTM) networks, and Convolutional Neural Networks (CNN). This advanced architecture offers a superior alternative to conventional methods such as artificial neural networks and regression models by capturing both short- and long-term dependencies in time series data. By leveraging the self-attention mechanism of Transformers, the temporal memory of LSTMs, and the local pattern detection of CNNs, our hybrid model delivers improved prediction accuracy. Implemented using Python, the model provides precise future gasoline consumption forecasts and evaluates the environmental impact through the analysis of greenhouse gas emissions. This study examines gasoline consumption trends from 2007 to 2021, which rose from 64.5 million liters per day in 2007 to 99.80 million liters per day in 2021. Our proposed model forecasts consumption levels up to 2031, offering a valuable tool for policymakers and energy analysts. The results highlight the superiority of this hybrid model in improving the accuracy of gasoline consumption forecasts, reinforcing the need for advanced machine learning techniques to optimize resource management and mitigate environmental risks in the energy sector.</summary></entry><entry><title type="html">Adversarial Constrained Policy Optimization: Improving Constrained Reinforcement Learning by Adapting Budgets</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdversarialConstrainedPolicyOptimizationImprovingConstrainedReinforcementLearningbyAdaptingBudgets.html" rel="alternate" type="text/html" title="Adversarial Constrained Policy Optimization: Improving Constrained Reinforcement Learning by Adapting Budgets" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdversarialConstrainedPolicyOptimizationImprovingConstrainedReinforcementLearningbyAdaptingBudgets</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdversarialConstrainedPolicyOptimizationImprovingConstrainedReinforcementLearningbyAdaptingBudgets.html">&lt;p&gt;Constrained reinforcement learning has achieved promising progress in safety-critical fields where both rewards and constraints are considered. However, constrained reinforcement learning methods face challenges in striking the right balance between task performance and constraint satisfaction and it is prone for them to get stuck in over-conservative or constraint violating local minima. In this paper, we propose Adversarial Constrained Policy Optimization (ACPO), which enables simultaneous optimization of reward and the adaptation of cost budgets during training. Our approach divides original constrained problem into two adversarial stages that are solved alternately, and the policy update performance of our algorithm can be theoretically guaranteed. We validate our method through experiments conducted on Safety Gymnasium and quadruped locomotion tasks. Results demonstrate that our algorithm achieves better performances compared to commonly used baselines.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20786&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jianmina Ma, Jingtian Ji, Yue Gao</name></author><summary type="html">Constrained reinforcement learning has achieved promising progress in safety-critical fields where both rewards and constraints are considered. However, constrained reinforcement learning methods face challenges in striking the right balance between task performance and constraint satisfaction and it is prone for them to get stuck in over-conservative or constraint violating local minima. In this paper, we propose Adversarial Constrained Policy Optimization (ACPO), which enables simultaneous optimization of reward and the adaptation of cost budgets during training. Our approach divides original constrained problem into two adversarial stages that are solved alternately, and the policy update performance of our algorithm can be theoretically guaranteed. We validate our method through experiments conducted on Safety Gymnasium and quadruped locomotion tasks. Results demonstrate that our algorithm achieves better performances compared to commonly used baselines.</summary></entry><entry><title type="html">Adversarial Robustness Through Artifact Design</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdversarialRobustnessThroughArtifactDesign.html" rel="alternate" type="text/html" title="Adversarial Robustness Through Artifact Design" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdversarialRobustnessThroughArtifactDesign</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdversarialRobustnessThroughArtifactDesign.html">&lt;p&gt;Adversarial examples arose as a challenge for machine learning. To hinder them, most defenses alter how models are trained (e.g., adversarial training) or inference is made (e.g., randomized smoothing). Still, while these approaches markedly improve models’ adversarial robustness, models remain highly susceptible to adversarial examples. Identifying that, in certain domains such as traffic-sign recognition, objects are implemented per standards specifying how artifacts (e.g., signs) should be designed, we propose a novel approach for improving adversarial robustness. Specifically, we offer a method to redefine standards, making minor changes to existing ones, to defend against adversarial examples. We formulate the problem of artifact design as a robust optimization problem, and propose gradient-based and greedy search methods to solve it. We evaluated our approach in the domain of traffic-sign recognition, allowing it to alter traffic-sign pictograms (i.e., symbols within the signs) and their colors. We found that, combined with adversarial training, our approach led to up to 25.18\% higher robust accuracy compared to state-of-the-art methods against two adversary types, while further increasing accuracy on benign inputs. Notably, a user study we conducted showed that traffic signs produced by our approach are also easily recognizable by human subjects.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.04660&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tsufit Shua, Liron David, Mahmood Sharif</name></author><summary type="html">Adversarial examples arose as a challenge for machine learning. To hinder them, most defenses alter how models are trained (e.g., adversarial training) or inference is made (e.g., randomized smoothing). Still, while these approaches markedly improve models’ adversarial robustness, models remain highly susceptible to adversarial examples. Identifying that, in certain domains such as traffic-sign recognition, objects are implemented per standards specifying how artifacts (e.g., signs) should be designed, we propose a novel approach for improving adversarial robustness. Specifically, we offer a method to redefine standards, making minor changes to existing ones, to defend against adversarial examples. We formulate the problem of artifact design as a robust optimization problem, and propose gradient-based and greedy search methods to solve it. We evaluated our approach in the domain of traffic-sign recognition, allowing it to alter traffic-sign pictograms (i.e., symbols within the signs) and their colors. We found that, combined with adversarial training, our approach led to up to 25.18\% higher robust accuracy compared to state-of-the-art methods against two adversary types, while further increasing accuracy on benign inputs. Notably, a user study we conducted showed that traffic signs produced by our approach are also easily recognizable by human subjects.</summary></entry><entry><title type="html">Adversarial robustness of VAEs through the lens of local geometry</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdversarialrobustnessofVAEsthroughthelensoflocalgeometry.html" rel="alternate" type="text/html" title="Adversarial robustness of VAEs through the lens of local geometry" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdversarialrobustnessofVAEsthroughthelensoflocalgeometry</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AdversarialrobustnessofVAEsthroughthelensoflocalgeometry.html">&lt;p&gt;In an unsupervised attack on variational autoencoders (VAEs), an adversary finds a small perturbation in an input sample that significantly changes its latent space encoding, thereby compromising the reconstruction for a fixed decoder. A known reason for such vulnerability is the distortions in the latent space resulting from a mismatch between approximated latent posterior and a prior distribution. Consequently, a slight change in an input sample can move its encoding to a low/zero density region in the latent space resulting in an unconstrained generation. This paper demonstrates that an optimal way for an adversary to attack VAEs is to exploit a directional bias of a stochastic pullback metric tensor induced by the encoder and decoder networks. The pullback metric tensor of an encoder measures the change in infinitesimal latent volume from an input to a latent space. Thus, it can be viewed as a lens to analyse the effect of input perturbations leading to latent space distortions. We propose robustness evaluation scores using the eigenspectrum of a pullback metric tensor. Moreover, we empirically show that the scores correlate with the robustness parameter $\beta$ of the $\beta-$VAE. Since increasing $\beta$ also degrades reconstruction quality, we demonstrate a simple alternative using \textit{mixup} training to fill the empty regions in the latent space, thus improving robustness with improved reconstruction.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2208.03923&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Asif Khan, Amos Storkey</name></author><summary type="html">In an unsupervised attack on variational autoencoders (VAEs), an adversary finds a small perturbation in an input sample that significantly changes its latent space encoding, thereby compromising the reconstruction for a fixed decoder. A known reason for such vulnerability is the distortions in the latent space resulting from a mismatch between approximated latent posterior and a prior distribution. Consequently, a slight change in an input sample can move its encoding to a low/zero density region in the latent space resulting in an unconstrained generation. This paper demonstrates that an optimal way for an adversary to attack VAEs is to exploit a directional bias of a stochastic pullback metric tensor induced by the encoder and decoder networks. The pullback metric tensor of an encoder measures the change in infinitesimal latent volume from an input to a latent space. Thus, it can be viewed as a lens to analyse the effect of input perturbations leading to latent space distortions. We propose robustness evaluation scores using the eigenspectrum of a pullback metric tensor. Moreover, we empirically show that the scores correlate with the robustness parameter $\beta$ of the $\beta-$VAE. Since increasing $\beta$ also degrades reconstruction quality, we demonstrate a simple alternative using \textit{mixup} training to fill the empty regions in the latent space, thus improving robustness with improved reconstruction.</summary></entry><entry><title type="html">A first-order augmented Lagrangian method for constrained minimax optimization</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AfirstorderaugmentedLagrangianmethodforconstrainedminimaxoptimization.html" rel="alternate" type="text/html" title="A first-order augmented Lagrangian method for constrained minimax optimization" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AfirstorderaugmentedLagrangianmethodforconstrainedminimaxoptimization</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AfirstorderaugmentedLagrangianmethodforconstrainedminimaxoptimization.html">&lt;p&gt;In this paper we study a class of constrained minimax problems. In particular, we propose a first-order augmented Lagrangian method for solving them, whose subproblems turn out to be a much simpler structured minimax problem and are suitably solved by a first-order method developed in this paper. Under some suitable assumptions, an \emph{operation complexity} of $O(\varepsilon^{-4}\log\varepsilon^{-1})$, measured by its fundamental operations, is established for the first-order augmented Lagrangian method for finding an $\varepsilon$-KKT solution of the constrained minimax problems.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2301.02060&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zhaosong Lu, Sanyou Mei</name></author><category term="stat.ML" /><summary type="html">In this paper we study a class of constrained minimax problems. In particular, we propose a first-order augmented Lagrangian method for solving them, whose subproblems turn out to be a much simpler structured minimax problem and are suitably solved by a first-order method developed in this paper. Under some suitable assumptions, an \emph{operation complexity} of $O(\varepsilon^{-4}\log\varepsilon^{-1})$, measured by its fundamental operations, is established for the first-order augmented Lagrangian method for finding an $\varepsilon$-KKT solution of the constrained minimax problems.</summary></entry><entry><title type="html">A hybrid quantum solver for the Lorenz system</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AhybridquantumsolverfortheLorenzsystem.html" rel="alternate" type="text/html" title="A hybrid quantum solver for the Lorenz system" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AhybridquantumsolverfortheLorenzsystem</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AhybridquantumsolverfortheLorenzsystem.html">&lt;p&gt;We develop a hybrid classical-quantum method for solving the Lorenz system. We use the forward Euler method to discretize the system in time, transforming it into a system of equations. This set of equations is solved using the Variational Quantum Linear Solver (VQLS) algorithm. We present numerical results comparing the hybrid method with the classical approach for solving the Lorenz system. The simulation results demonstrate that the VQLS method can effectively compute solutions comparable to classical methods. The method is easily extended to solving similar nonlinear differential equations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.15417&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sajad Fathi Hafshejani, Daya Gaur, Arundhati Dasgupta, Robert Benkoczi, Narasimha Gosala, Alfredo Iorio</name></author><summary type="html">We develop a hybrid classical-quantum method for solving the Lorenz system. We use the forward Euler method to discretize the system in time, transforming it into a system of equations. This set of equations is solved using the Variational Quantum Linear Solver (VQLS) algorithm. We present numerical results comparing the hybrid method with the classical approach for solving the Lorenz system. The simulation results demonstrate that the VQLS method can effectively compute solutions comparable to classical methods. The method is easily extended to solving similar nonlinear differential equations.</summary></entry><entry><title type="html">Air Quality Prediction with Physics-Informed Dual Neural ODEs in Open Systems</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AirQualityPredictionwithPhysicsInformedDualNeuralODEsinOpenSystems.html" rel="alternate" type="text/html" title="Air Quality Prediction with Physics-Informed Dual Neural ODEs in Open Systems" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AirQualityPredictionwithPhysicsInformedDualNeuralODEsinOpenSystems</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AirQualityPredictionwithPhysicsInformedDualNeuralODEsinOpenSystems.html">&lt;p&gt;Air pollution significantly threatens human health and ecosystems, necessitating effective air quality prediction to inform public policy. Traditional approaches are generally categorized into physics-based and data-driven models. Physics-based models usually struggle with high computational demands and closed-system assumptions, while data-driven models may overlook essential physical dynamics, confusing the capturing of spatiotemporal correlations. Although some physics-informed approaches combine the strengths of both models, they often face a mismatch between explicit physical equations and implicit learned representations. To address these challenges, we propose Air-DualODE, a novel physics-informed approach that integrates dual branches of Neural ODEs for air quality prediction. The first branch applies open-system physical equations to capture spatiotemporal dependencies for learning physics dynamics, while the second branch identifies the dependencies not addressed by the first in a fully data-driven way. These dual representations are temporally aligned and fused to enhance prediction accuracy. Our experimental results demonstrate that Air-DualODE achieves state-of-the-art performance in predicting pollutant concentrations across various spatial scales, thereby offering a promising solution for real-world air quality challenges.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.19892&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jindong Tian, Yuxuan Liang, Ronghui Xu, Peng Chen, Chenjuan Guo, Aoying Zhou, Lujia Pan, Zhongwen Rao, Bin Yang</name></author><summary type="html">Air pollution significantly threatens human health and ecosystems, necessitating effective air quality prediction to inform public policy. Traditional approaches are generally categorized into physics-based and data-driven models. Physics-based models usually struggle with high computational demands and closed-system assumptions, while data-driven models may overlook essential physical dynamics, confusing the capturing of spatiotemporal correlations. Although some physics-informed approaches combine the strengths of both models, they often face a mismatch between explicit physical equations and implicit learned representations. To address these challenges, we propose Air-DualODE, a novel physics-informed approach that integrates dual branches of Neural ODEs for air quality prediction. The first branch applies open-system physical equations to capture spatiotemporal dependencies for learning physics dynamics, while the second branch identifies the dependencies not addressed by the first in a fully data-driven way. These dual representations are temporally aligned and fused to enhance prediction accuracy. Our experimental results demonstrate that Air-DualODE achieves state-of-the-art performance in predicting pollutant concentrations across various spatial scales, thereby offering a promising solution for real-world air quality challenges.</summary></entry><entry><title type="html">Aligning Target-Aware Molecule Diffusion Models with Exact Energy Optimization</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AligningTargetAwareMoleculeDiffusionModelswithExactEnergyOptimization.html" rel="alternate" type="text/html" title="Aligning Target-Aware Molecule Diffusion Models with Exact Energy Optimization" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AligningTargetAwareMoleculeDiffusionModelswithExactEnergyOptimization</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AligningTargetAwareMoleculeDiffusionModelswithExactEnergyOptimization.html">&lt;p&gt;Generating ligand molecules for specific protein targets, known as structure-based drug design, is a fundamental problem in therapeutics development and biological discovery. Recently, target-aware generative models, especially diffusion models, have shown great promise in modeling protein-ligand interactions and generating candidate drugs. However, existing models primarily focus on learning the chemical distribution of all drug candidates, which lacks effective steerability on the chemical quality of model generations. In this paper, we propose a novel and general alignment framework to align pretrained target diffusion models with preferred functional properties, named AliDiff. AliDiff shifts the target-conditioned chemical distribution towards regions with higher binding affinity and structural rationality, specified by user-defined reward functions, via the preference optimization approach. To avoid the overfitting problem in common preference optimization objectives, we further develop an improved Exact Energy Preference Optimization method to yield an exact and efficient alignment of the diffusion models, and provide the closed-form expression for the converged distribution. Empirical studies on the CrossDocked2020 benchmark show that AliDiff can generate molecules with state-of-the-art binding energies with up to -7.07 Avg. Vina Score, while maintaining strong molecular properties. Code is available at https://github.com/MinkaiXu/AliDiff.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2407.01648&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Siyi Gu, Minkai Xu, Alexander Powers, Weili Nie, Tomas Geffner, Karsten Kreis, Jure Leskovec, Arash Vahdat, Stefano Ermon</name></author><summary type="html">Generating ligand molecules for specific protein targets, known as structure-based drug design, is a fundamental problem in therapeutics development and biological discovery. Recently, target-aware generative models, especially diffusion models, have shown great promise in modeling protein-ligand interactions and generating candidate drugs. However, existing models primarily focus on learning the chemical distribution of all drug candidates, which lacks effective steerability on the chemical quality of model generations. In this paper, we propose a novel and general alignment framework to align pretrained target diffusion models with preferred functional properties, named AliDiff. AliDiff shifts the target-conditioned chemical distribution towards regions with higher binding affinity and structural rationality, specified by user-defined reward functions, via the preference optimization approach. To avoid the overfitting problem in common preference optimization objectives, we further develop an improved Exact Energy Preference Optimization method to yield an exact and efficient alignment of the diffusion models, and provide the closed-form expression for the converged distribution. Empirical studies on the CrossDocked2020 benchmark show that AliDiff can generate molecules with state-of-the-art binding energies with up to -7.07 Avg. Vina Score, while maintaining strong molecular properties. Code is available at https://github.com/MinkaiXu/AliDiff.</summary></entry><entry><title type="html">Aligning Text-to-Image Diffusion Models with Reward Backpropagation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AligningTexttoImageDiffusionModelswithRewardBackpropagation.html" rel="alternate" type="text/html" title="Aligning Text-to-Image Diffusion Models with Reward Backpropagation" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AligningTexttoImageDiffusionModelswithRewardBackpropagation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AligningTexttoImageDiffusionModelswithRewardBackpropagation.html">&lt;p&gt;Text-to-image diffusion models have recently emerged at the forefront of image generation, powered by very large-scale unsupervised or weakly supervised text-to-image training datasets. Due to their unsupervised training, controlling their behavior in downstream tasks, such as maximizing human-perceived image quality, image-text alignment, or ethical image generation, is difficult. Recent works finetune diffusion models to downstream reward functions using vanilla reinforcement learning, notorious for the high variance of the gradient estimators. In this paper, we propose AlignProp, a method that aligns diffusion models to downstream reward functions using end-to-end backpropagation of the reward gradient through the denoising process. While naive implementation of such backpropagation would require prohibitive memory resources for storing the partial derivatives of modern text-to-image models, AlignProp finetunes low-rank adapter weight modules and uses gradient checkpointing, to render its memory usage viable. We test AlignProp in finetuning diffusion models to various objectives, such as image-text semantic alignment, aesthetics, compressibility and controllability of the number of objects present, as well as their combinations. We show AlignProp achieves higher rewards in fewer training steps than alternatives, while being conceptually simpler, making it a straightforward choice for optimizing diffusion models for differentiable reward functions of interest. Code and Visualization results are available at https://align-prop.github.io/.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.03739&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, Katerina Fragkiadaki</name></author><summary type="html">Text-to-image diffusion models have recently emerged at the forefront of image generation, powered by very large-scale unsupervised or weakly supervised text-to-image training datasets. Due to their unsupervised training, controlling their behavior in downstream tasks, such as maximizing human-perceived image quality, image-text alignment, or ethical image generation, is difficult. Recent works finetune diffusion models to downstream reward functions using vanilla reinforcement learning, notorious for the high variance of the gradient estimators. In this paper, we propose AlignProp, a method that aligns diffusion models to downstream reward functions using end-to-end backpropagation of the reward gradient through the denoising process. While naive implementation of such backpropagation would require prohibitive memory resources for storing the partial derivatives of modern text-to-image models, AlignProp finetunes low-rank adapter weight modules and uses gradient checkpointing, to render its memory usage viable. We test AlignProp in finetuning diffusion models to various objectives, such as image-text semantic alignment, aesthetics, compressibility and controllability of the number of objects present, as well as their combinations. We show AlignProp achieves higher rewards in fewer training steps than alternatives, while being conceptually simpler, making it a straightforward choice for optimizing diffusion models for differentiable reward functions of interest. Code and Visualization results are available at https://align-prop.github.io/.</summary></entry><entry><title type="html">Alignment for Honesty</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AlignmentforHonesty.html" rel="alternate" type="text/html" title="Alignment for Honesty" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AlignmentforHonesty</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AlignmentforHonesty.html">&lt;p&gt;Recent research has made significant strides in aligning large language models (LLMs) with helpfulness and harmlessness. In this paper, we argue for the importance of alignment for \emph{honesty}, ensuring that LLMs proactively refuse to answer questions when they lack knowledge, while still not being overly conservative. However, a pivotal aspect of alignment for honesty involves discerning an LLM’s knowledge boundaries, which demands comprehensive solutions in terms of metric development, benchmark creation, and training methodologies. We address these challenges by first establishing a precise problem definition and defining ``honesty’’ inspired by the Analects of Confucius. This serves as a cornerstone for developing metrics that effectively measure an LLM’s honesty by quantifying its progress post-alignment. Furthermore, we introduce a flexible training framework which is further instantiated by several efficient fine-tuning techniques that emphasize honesty without sacrificing performance on other tasks. Our extensive experiments reveal that these aligned models show a marked increase in honesty, as indicated by our proposed metrics. We open-source all relevant resources to facilitate future research at \url{https://github.com/GAIR-NLP/alignment-for-honesty}.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.07000&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, Pengfei Liu</name></author><summary type="html">Recent research has made significant strides in aligning large language models (LLMs) with helpfulness and harmlessness. In this paper, we argue for the importance of alignment for \emph{honesty}, ensuring that LLMs proactively refuse to answer questions when they lack knowledge, while still not being overly conservative. However, a pivotal aspect of alignment for honesty involves discerning an LLM’s knowledge boundaries, which demands comprehensive solutions in terms of metric development, benchmark creation, and training methodologies. We address these challenges by first establishing a precise problem definition and defining ``honesty’’ inspired by the Analects of Confucius. This serves as a cornerstone for developing metrics that effectively measure an LLM’s honesty by quantifying its progress post-alignment. Furthermore, we introduce a flexible training framework which is further instantiated by several efficient fine-tuning techniques that emphasize honesty without sacrificing performance on other tasks. Our extensive experiments reveal that these aligned models show a marked increase in honesty, as indicated by our proposed metrics. We open-source all relevant resources to facilitate future research at \url{https://github.com/GAIR-NLP/alignment-for-honesty}.</summary></entry><entry><title type="html">Almost goodness-of-fit tests</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/Almostgoodnessoffittests.html" rel="alternate" type="text/html" title="Almost goodness-of-fit tests" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/Almostgoodnessoffittests</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/Almostgoodnessoffittests.html">&lt;p&gt;We introduce the almost goodness-of-fit test, a procedure to decide if a (parametric) model provides a good representation of the probability distribution generating the observed sample. We consider the approximate model determined by an M-estimator of the parameters as the best representative of the unknown distribution within the parametric class. The objective is the approximate validation of a distribution or an entire parametric family up to a pre-specified threshold value, the margin of error. The methodology also allows quantifying the percentage improvement of the proposed model compared to a non-informative (constant) one. The test statistic is the $\mathrm{L}^p$-distance between the empirical distribution function and the corresponding one of the estimated (parametric) model. The value of the parameter $p$ allows modulating the impact of the tails of the distribution in the validation of the model. By deriving the asymptotic distribution of the test statistic, as well as proving the consistency of its bootstrap approximation, we present an easy-to-implement and flexible method. The performance of the proposal is illustrated with a simulation study and the analysis of a real dataset.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20918&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Amparo Baíllo, Javier Cárcamo</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.TH" /><summary type="html">We introduce the almost goodness-of-fit test, a procedure to decide if a (parametric) model provides a good representation of the probability distribution generating the observed sample. We consider the approximate model determined by an M-estimator of the parameters as the best representative of the unknown distribution within the parametric class. The objective is the approximate validation of a distribution or an entire parametric family up to a pre-specified threshold value, the margin of error. The methodology also allows quantifying the percentage improvement of the proposed model compared to a non-informative (constant) one. The test statistic is the $\mathrm{L}^p$-distance between the empirical distribution function and the corresponding one of the estimated (parametric) model. The value of the parameter $p$ allows modulating the impact of the tails of the distribution in the validation of the model. By deriving the asymptotic distribution of the test statistic, as well as proving the consistency of its bootstrap approximation, we present an easy-to-implement and flexible method. The performance of the proposal is illustrated with a simulation study and the analysis of a real dataset.</summary></entry><entry><title type="html">Alternatives of Unsupervised Representations of Variables on the Latent Space</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AlternativesofUnsupervisedRepresentationsofVariablesontheLatentSpace.html" rel="alternate" type="text/html" title="Alternatives of Unsupervised Representations of Variables on the Latent Space" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AlternativesofUnsupervisedRepresentationsofVariablesontheLatentSpace</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AlternativesofUnsupervisedRepresentationsofVariablesontheLatentSpace.html">&lt;p&gt;The article addresses the application of unsupervised machine learning to represent variables on the 2D latent space by applying a variational autoencoder (beta-VAE). Representation of variables on low dimensional spaces allows for data visualization, disentanglement of variables based on underlying characteristics, finding of meaningful patterns and outliers, and supports interpretability. Five distinct methods have been introduced to represent variables on the latent space: (1) straightforward transposed, (2) univariate metadata of variables, such as variable statistics, empirical probability density and cumulative distribution functions, (3) adjacency matrices of different metrics, such as correlations, R2 values, Jaccard index, cosine similarity, and mutual information, (4) gradient mappings followed by spot cross product calculation, and (5) combined. Twenty-eight approaches of variable representations by beta-VAE have been considered. The pairwise spot cross product addresses relationships of gradients of two variables along latent space axes, such as orthogonal, confounded positive, confounded negative, and everything in between. The article addresses generalized representations of variables that cover both features and labels. Dealing with categorical variables, reinforced entanglement has been introduced to represent one-hot encoded categories. The article includes three examples: (1) synthetic data with known dependencies, (2) famous MNIST example of handwritten numbers, and (3) real-world multivariate time series of Canadian financial market interest rates. As a result, unsupervised representations of interest rates on the latent space correctly disentangled rates based on their type, such as bonds, T-bills, GICs, or conventional mortgages, positioned bonds and T-bills along a single curve, and ordered rates by their terms along that curve.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20172&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alex Glushkovsky</name></author><summary type="html">The article addresses the application of unsupervised machine learning to represent variables on the 2D latent space by applying a variational autoencoder (beta-VAE). Representation of variables on low dimensional spaces allows for data visualization, disentanglement of variables based on underlying characteristics, finding of meaningful patterns and outliers, and supports interpretability. Five distinct methods have been introduced to represent variables on the latent space: (1) straightforward transposed, (2) univariate metadata of variables, such as variable statistics, empirical probability density and cumulative distribution functions, (3) adjacency matrices of different metrics, such as correlations, R2 values, Jaccard index, cosine similarity, and mutual information, (4) gradient mappings followed by spot cross product calculation, and (5) combined. Twenty-eight approaches of variable representations by beta-VAE have been considered. The pairwise spot cross product addresses relationships of gradients of two variables along latent space axes, such as orthogonal, confounded positive, confounded negative, and everything in between. The article addresses generalized representations of variables that cover both features and labels. Dealing with categorical variables, reinforced entanglement has been introduced to represent one-hot encoded categories. The article includes three examples: (1) synthetic data with known dependencies, (2) famous MNIST example of handwritten numbers, and (3) real-world multivariate time series of Canadian financial market interest rates. As a result, unsupervised representations of interest rates on the latent space correctly disentangled rates based on their type, such as bonds, T-bills, GICs, or conventional mortgages, positioned bonds and T-bills along a single curve, and ordered rates by their terms along that curve.</summary></entry><entry><title type="html">Amalgam: A Framework for Obfuscated Neural Network Training on the Cloud</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AmalgamAFrameworkforObfuscatedNeuralNetworkTrainingontheCloud.html" rel="alternate" type="text/html" title="Amalgam: A Framework for Obfuscated Neural Network Training on the Cloud" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AmalgamAFrameworkforObfuscatedNeuralNetworkTrainingontheCloud</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AmalgamAFrameworkforObfuscatedNeuralNetworkTrainingontheCloud.html">&lt;p&gt;Training a proprietary Neural Network (NN) model with a proprietary dataset on the cloud comes at the risk of exposing the model architecture and the dataset to the cloud service provider. To tackle this problem, in this paper, we present an NN obfuscation framework, called Amalgam, to train NN models in a privacy-preserving manner in existing cloud-based environments. Amalgam achieves that by augmenting NN models and the datasets to be used for training with well-calibrated noise to “hide” both the original model architectures and training datasets from the cloud. After training, Amalgam extracts the original models from the augmented models and returns them to users. Our evaluation results with different computer vision and natural language processing models and datasets demonstrate that Amalgam: (i) introduces modest overheads into the training process without impacting its correctness, and (ii) does not affect the model’s accuracy. The prototype implementation is available at: https://github.com/SifatTaj/amalgam&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.03405&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sifat Ut Taki, Spyridon Mastorakis</name></author><summary type="html">Training a proprietary Neural Network (NN) model with a proprietary dataset on the cloud comes at the risk of exposing the model architecture and the dataset to the cloud service provider. To tackle this problem, in this paper, we present an NN obfuscation framework, called Amalgam, to train NN models in a privacy-preserving manner in existing cloud-based environments. Amalgam achieves that by augmenting NN models and the datasets to be used for training with well-calibrated noise to “hide” both the original model architectures and training datasets from the cloud. After training, Amalgam extracts the original models from the augmented models and returns them to users. Our evaluation results with different computer vision and natural language processing models and datasets demonstrate that Amalgam: (i) introduces modest overheads into the training process without impacting its correctness, and (ii) does not affect the model’s accuracy. The prototype implementation is available at: https://github.com/SifatTaj/amalgam</summary></entry><entry><title type="html">A mathematical model describing the trajectory of the bearing in an internal gear pump</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/Amathematicalmodeldescribingthetrajectoryofthebearinginaninternalgearpump.html" rel="alternate" type="text/html" title="A mathematical model describing the trajectory of the bearing in an internal gear pump" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/Amathematicalmodeldescribingthetrajectoryofthebearinginaninternalgearpump</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/Amathematicalmodeldescribingthetrajectoryofthebearinginaninternalgearpump.html">&lt;p&gt;This paper presents a mathematical model for determining the movement of the bearing in an internal gear pump. The paper also performs simulation calculations to find the movement trajectory of the shaft with the given input data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.19871&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hung Manh Nguyen, Trong Hoa Pham</name></author><summary type="html">This paper presents a mathematical model for determining the movement of the bearing in an internal gear pump. The paper also performs simulation calculations to find the movement trajectory of the shaft with the given input data.</summary></entry><entry><title type="html">A model and method for analyzing the precision of binary measurement methods based on beta-binomial distributions, and related statistical tests</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/Amodelandmethodforanalyzingtheprecisionofbinarymeasurementmethodsbasedonbetabinomialdistributionsandrelatedstatisticaltests.html" rel="alternate" type="text/html" title="A model and method for analyzing the precision of binary measurement methods based on beta-binomial distributions, and related statistical tests" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/Amodelandmethodforanalyzingtheprecisionofbinarymeasurementmethodsbasedonbetabinomialdistributionsandrelatedstatisticaltests</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/Amodelandmethodforanalyzingtheprecisionofbinarymeasurementmethodsbasedonbetabinomialdistributionsandrelatedstatisticaltests.html">&lt;p&gt;This study developed a new statistical model and method for analyzing the precision of binary measurement methods from collaborative studies. The model is based on beta-binomial distributions. In other words, it assumes that the sensitivity of each laboratory obeys a beta distribution, and the binary measured values under a given sensitivity follow a binomial distribution. We propose the key precision measures of repeatability and reproducibility for the model, and provide their unbiased estimates. Further, through consideration of a number of statistical test methods for homogeneity of proportions, we propose appropriate methods for determining laboratory effects in the new model. Finally, we apply the results to real-world examples in the fields of food safety and chemical risk assessment and management.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2008.13619&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jun-ichi Takeshita, Tomomichi Suzuki</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">This study developed a new statistical model and method for analyzing the precision of binary measurement methods from collaborative studies. The model is based on beta-binomial distributions. In other words, it assumes that the sensitivity of each laboratory obeys a beta distribution, and the binary measured values under a given sensitivity follow a binomial distribution. We propose the key precision measures of repeatability and reproducibility for the model, and provide their unbiased estimates. Further, through consideration of a number of statistical test methods for homogeneity of proportions, we propose appropriate methods for determining laboratory effects in the new model. Finally, we apply the results to real-world examples in the fields of food safety and chemical risk assessment and management.</summary></entry><entry><title type="html">Amplifying Aspect-Sentence Awareness: A Novel Approach for Aspect-Based Sentiment Analysis</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AmplifyingAspectSentenceAwarenessANovelApproachforAspectBasedSentimentAnalysis.html" rel="alternate" type="text/html" title="Amplifying Aspect-Sentence Awareness: A Novel Approach for Aspect-Based Sentiment Analysis" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AmplifyingAspectSentenceAwarenessANovelApproachforAspectBasedSentimentAnalysis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AmplifyingAspectSentenceAwarenessANovelApproachforAspectBasedSentimentAnalysis.html">&lt;p&gt;Aspect-Based Sentiment Analysis (ABSA) is increasingly crucial in Natural Language Processing (NLP) for applications such as customer feedback analysis and product recommendation systems. ABSA goes beyond traditional sentiment analysis by extracting sentiments related to specific aspects mentioned in the text; existing attention-based models often need help to effectively connect aspects with context due to language complexity and multiple sentiment polarities in a single sentence. Recent research underscores the value of integrating syntactic information, such as dependency trees, to understand long-range syntactic relationships better and link aspects with context. Despite these advantages, challenges persist, including sensitivity to parsing errors and increased computational complexity when combining syntactic and semantic information. To address these issues, we propose Amplifying Aspect-Sentence Awareness (A3SN), a novel technique designed to enhance ABSA through amplifying aspect-sentence awareness attention. Following the transformer’s standard process, our innovative approach incorporates multi-head attention mechanisms to augment the model with sentence and aspect semantic information. We added another multi-head attention module: amplify aspect-sentence awareness attention. By doubling its focus between the sentence and aspect, we effectively highlighted aspect importance within the sentence context. This enables accurate capture of subtle relationships and dependencies. Additionally, gated fusion integrates feature representations from multi-head and amplified aspect-sentence awareness attention mechanisms, which is essential for ABSA. Experimental results across three benchmark datasets demonstrate A3SN’s effectiveness and outperform state-of-the-art (SOTA) baseline models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.13013&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Adamu Lawan, Juhua Pu, Haruna Yunusa, Jawad Muhammad, Aliyu Umar</name></author><summary type="html">Aspect-Based Sentiment Analysis (ABSA) is increasingly crucial in Natural Language Processing (NLP) for applications such as customer feedback analysis and product recommendation systems. ABSA goes beyond traditional sentiment analysis by extracting sentiments related to specific aspects mentioned in the text; existing attention-based models often need help to effectively connect aspects with context due to language complexity and multiple sentiment polarities in a single sentence. Recent research underscores the value of integrating syntactic information, such as dependency trees, to understand long-range syntactic relationships better and link aspects with context. Despite these advantages, challenges persist, including sensitivity to parsing errors and increased computational complexity when combining syntactic and semantic information. To address these issues, we propose Amplifying Aspect-Sentence Awareness (A3SN), a novel technique designed to enhance ABSA through amplifying aspect-sentence awareness attention. Following the transformer’s standard process, our innovative approach incorporates multi-head attention mechanisms to augment the model with sentence and aspect semantic information. We added another multi-head attention module: amplify aspect-sentence awareness attention. By doubling its focus between the sentence and aspect, we effectively highlighted aspect importance within the sentence context. This enables accurate capture of subtle relationships and dependencies. Additionally, gated fusion integrates feature representations from multi-head and amplified aspect-sentence awareness attention mechanisms, which is essential for ABSA. Experimental results across three benchmark datasets demonstrate A3SN’s effectiveness and outperform state-of-the-art (SOTA) baseline models.</summary></entry><entry><title type="html">An Adaptive Multivariate Functional EWMA Control Chart</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnAdaptiveMultivariateFunctionalEWMAControlChart.html" rel="alternate" type="text/html" title="An Adaptive Multivariate Functional EWMA Control Chart" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnAdaptiveMultivariateFunctionalEWMAControlChart</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnAdaptiveMultivariateFunctionalEWMAControlChart.html">&lt;p&gt;In many modern industrial scenarios, the measurements of the quality characteristics of interest are often required to be represented as functional data or profiles. This motivates the growing interest in extending traditional univariate statistical process monitoring (SPM) schemes to the functional data setting. This article proposes a new SPM scheme, which is referred to as adaptive multivariate functional EWMA (AMFEWMA), to extend the well-known exponentially weighted moving average (EWMA) control chart from the univariate scalar to the multivariate functional setting. The favorable performance of the AMFEWMA control chart over existing methods is assessed via an extensive Monte Carlo simulation. Its practical applicability is demonstrated through a case study in the monitoring of the quality of a resistance spot welding process in the automotive industry through the online observations of dynamic resistance curves, which are associated with multiple spot welds on the same car body and recognized as the full technological signature of the process.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.03837&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Christian Capezza, Giovanna Capizzi, Fabio Centofanti, Antonio Lepore, Biagio Palumbo</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">In many modern industrial scenarios, the measurements of the quality characteristics of interest are often required to be represented as functional data or profiles. This motivates the growing interest in extending traditional univariate statistical process monitoring (SPM) schemes to the functional data setting. This article proposes a new SPM scheme, which is referred to as adaptive multivariate functional EWMA (AMFEWMA), to extend the well-known exponentially weighted moving average (EWMA) control chart from the univariate scalar to the multivariate functional setting. The favorable performance of the AMFEWMA control chart over existing methods is assessed via an extensive Monte Carlo simulation. Its practical applicability is demonstrated through a case study in the monitoring of the quality of a resistance spot welding process in the automotive industry through the online observations of dynamic resistance curves, which are associated with multiple spot welds on the same car body and recognized as the full technological signature of the process.</summary></entry><entry><title type="html">An Effective Theory of Bias Amplification</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnEffectiveTheoryofBiasAmplification.html" rel="alternate" type="text/html" title="An Effective Theory of Bias Amplification" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnEffectiveTheoryofBiasAmplification</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnEffectiveTheoryofBiasAmplification.html">&lt;p&gt;Machine learning models may capture and amplify biases present in data, leading to disparate test performance across social groups. To better understand, evaluate, and mitigate these possible biases, a deeper theoretical understanding of how model design choices and data distribution properties could contribute to bias is needed. In this work, we contribute a precise analytical theory in the context of ridge regression, both with and without random projections, where the former models neural networks in a simplified regime. Our theory offers a unified and rigorous explanation of machine learning bias, providing insights into phenomena such as bias amplification and minority-group bias in various feature and parameter regimes. For example, we demonstrate that there may be an optimal regularization penalty or training time to avoid bias amplification, and there can be fundamental differences in test error between groups that do not vanish with increased parameterization. Importantly, our theoretical predictions align with several empirical observations reported in the literature. We extensively empirically validate our theory on diverse synthetic and semi-synthetic datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.17263&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Arjun Subramonian, Sam Bell, Levent Sagun, Elvis Dohmatob</name></author><category term="stat.ML" /><summary type="html">Machine learning models may capture and amplify biases present in data, leading to disparate test performance across social groups. To better understand, evaluate, and mitigate these possible biases, a deeper theoretical understanding of how model design choices and data distribution properties could contribute to bias is needed. In this work, we contribute a precise analytical theory in the context of ridge regression, both with and without random projections, where the former models neural networks in a simplified regime. Our theory offers a unified and rigorous explanation of machine learning bias, providing insights into phenomena such as bias amplification and minority-group bias in various feature and parameter regimes. For example, we demonstrate that there may be an optimal regularization penalty or training time to avoid bias amplification, and there can be fundamental differences in test error between groups that do not vanish with increased parameterization. Importantly, our theoretical predictions align with several empirical observations reported in the literature. We extensively empirically validate our theory on diverse synthetic and semi-synthetic datasets.</summary></entry><entry><title type="html">An Efficient Numerical Scheme for a Time-Fractional Burgers Equation with Caputo-Prabhakar Derivative</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnEfficientNumericalSchemeforaTimeFractionalBurgersEquationwithCaputoPrabhakarDerivative.html" rel="alternate" type="text/html" title="An Efficient Numerical Scheme for a Time-Fractional Burgers Equation with Caputo-Prabhakar Derivative" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnEfficientNumericalSchemeforaTimeFractionalBurgersEquationwithCaputoPrabhakarDerivative</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnEfficientNumericalSchemeforaTimeFractionalBurgersEquationwithCaputoPrabhakarDerivative.html">&lt;p&gt;This paper presents a numerical method to solve a time-fractional Burgers equation, achieving order of convergence $(2-\alpha)$ in time, here $\alpha$ represents the order of the time derivative. The fractional derivative is modeled by Caputo-Prabhakar (CP) formulation, which incorporates a kernel defined by the three-parameter Mittag-Leffler function. Finite difference methods are employed for the discretization of the derivatives. To handle the non-linear term, the Newton iteration method is used. The proposed numerical scheme is proven to be stable and convergent in the $L_{\infty}$ norm. The validity of the theory is supported by two numerical examples.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20192&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Deeksha Singh, Swati Yadav, Rajesh K. Pandey</name></author><summary type="html">This paper presents a numerical method to solve a time-fractional Burgers equation, achieving order of convergence $(2-\alpha)$ in time, here $\alpha$ represents the order of the time derivative. The fractional derivative is modeled by Caputo-Prabhakar (CP) formulation, which incorporates a kernel defined by the three-parameter Mittag-Leffler function. Finite difference methods are employed for the discretization of the derivatives. To handle the non-linear term, the Newton iteration method is used. The proposed numerical scheme is proven to be stable and convergent in the $L_{\infty}$ norm. The validity of the theory is supported by two numerical examples.</summary></entry><entry><title type="html">An Energy Stable and Well-balanced Scheme for the Ripa System</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnEnergyStableandWellbalancedSchemefortheRipaSystem.html" rel="alternate" type="text/html" title="An Energy Stable and Well-balanced Scheme for the Ripa System" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnEnergyStableandWellbalancedSchemefortheRipaSystem</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnEnergyStableandWellbalancedSchemefortheRipaSystem.html">&lt;p&gt;We design and analyse an energy stable, structure preserving and well-balanced scheme for the Ripa system of shallow water equations. The energy stability of the numerical solutions is achieved by introducing appropriate stabilisation terms in the discretisation of the convective fluxes of mass and momenta, the pressure gradient and the topography source term. A diligent choice of the interface values of the water height and the temperature ensures the well-balancing property of the scheme for three physically relevant hydrostatic steady states. The explicit in time and finite volume in space scheme preserves the positivity of the water height and the temperature, and it is weakly consistent with the continuous model equations in the sense of Lax-Wendroff. The results of extensive numerical case studies on benchmark test problems are presented to confirm the theoretical findings.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20732&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>K. R. Arun, Rahuldev Ghorai</name></author><summary type="html">We design and analyse an energy stable, structure preserving and well-balanced scheme for the Ripa system of shallow water equations. The energy stability of the numerical solutions is achieved by introducing appropriate stabilisation terms in the discretisation of the convective fluxes of mass and momenta, the pressure gradient and the topography source term. A diligent choice of the interface values of the water height and the temperature ensures the well-balancing property of the scheme for three physically relevant hydrostatic steady states. The explicit in time and finite volume in space scheme preserves the positivity of the water height and the temperature, and it is weakly consistent with the continuous model equations in the sense of Lax-Wendroff. The results of extensive numerical case studies on benchmark test problems are presented to confirm the theoretical findings.</summary></entry><entry><title type="html">An Ensemble Approach to Music Source Separation: A Comparative Analysis of Conventional and Hierarchical Stem Separation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnEnsembleApproachtoMusicSourceSeparationAComparativeAnalysisofConventionalandHierarchicalStemSeparation.html" rel="alternate" type="text/html" title="An Ensemble Approach to Music Source Separation: A Comparative Analysis of Conventional and Hierarchical Stem Separation" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnEnsembleApproachtoMusicSourceSeparationAComparativeAnalysisofConventionalandHierarchicalStemSeparation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnEnsembleApproachtoMusicSourceSeparationAComparativeAnalysisofConventionalandHierarchicalStemSeparation.html">&lt;p&gt;Music source separation (MSS) is a task that involves isolating individual sound sources, or stems, from mixed audio signals. This paper presents an ensemble approach to MSS, combining several state-of-the-art architectures to achieve superior separation performance across traditional Vocal, Drum, and Bass (VDB) stems, as well as expanding into second-level hierarchical separation for sub-stems like kick, snare, lead vocals, and background vocals. Our method addresses the limitations of relying on a single model by utilising the complementary strengths of various models, leading to more balanced results across stems. For stem selection, we used the harmonic mean of Signal-to-Noise Ratio (SNR) and Signal-to-Distortion Ratio (SDR), ensuring that extreme values do not skew the results and that both metrics are weighted effectively. In addition to consistently high performance across the VDB stems, we also explored second-level hierarchical separation, revealing important insights into the complexities of MSS and how factors like genre and instrumentation can influence model performance. While the second-level separation results show room for improvement, the ability to isolate sub-stems marks a significant advancement. Our findings pave the way for further research in MSS, particularly in expanding model capabilities beyond VDB and improving niche stem separations such as guitar and piano.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20773&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Saarth Vardhan, Pavani R Acharya, Samarth S Rao, Oorjitha Ratna Jasthi, S Natarajan</name></author><summary type="html">Music source separation (MSS) is a task that involves isolating individual sound sources, or stems, from mixed audio signals. This paper presents an ensemble approach to MSS, combining several state-of-the-art architectures to achieve superior separation performance across traditional Vocal, Drum, and Bass (VDB) stems, as well as expanding into second-level hierarchical separation for sub-stems like kick, snare, lead vocals, and background vocals. Our method addresses the limitations of relying on a single model by utilising the complementary strengths of various models, leading to more balanced results across stems. For stem selection, we used the harmonic mean of Signal-to-Noise Ratio (SNR) and Signal-to-Distortion Ratio (SDR), ensuring that extreme values do not skew the results and that both metrics are weighted effectively. In addition to consistently high performance across the VDB stems, we also explored second-level hierarchical separation, revealing important insights into the complexities of MSS and how factors like genre and instrumentation can influence model performance. While the second-level separation results show room for improvement, the ability to isolate sub-stems marks a significant advancement. Our findings pave the way for further research in MSS, particularly in expanding model capabilities beyond VDB and improving niche stem separations such as guitar and piano.</summary></entry><entry><title type="html">Analysis of Different Algorithmic Design Techniques for Seam Carving</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnalysisofDifferentAlgorithmicDesignTechniquesforSeamCarving.html" rel="alternate" type="text/html" title="Analysis of Different Algorithmic Design Techniques for Seam Carving" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnalysisofDifferentAlgorithmicDesignTechniquesforSeamCarving</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnalysisofDifferentAlgorithmicDesignTechniquesforSeamCarving.html">&lt;p&gt;Seam carving, a content-aware image resizing technique, has garnered significant attention for its ability to resize images while preserving important content. In this paper, we conduct a comprehensive analysis of four algorithmic design techniques for seam carving: brute-force, greedy, dynamic programming, and GPU-based parallel algorithms. We begin by presenting a theoretical overview of each technique, discussing their underlying principles and computational complexities. Subsequently, we delve into empirical evaluations, comparing the performance of these algorithms in terms of runtime efficiency. Our experimental results provide insights into the theoretical complexities of the design techniques.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.21207&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Owais Aijaz, Syed Muhammad Ali, Yousuf Uyghur</name></author><summary type="html">Seam carving, a content-aware image resizing technique, has garnered significant attention for its ability to resize images while preserving important content. In this paper, we conduct a comprehensive analysis of four algorithmic design techniques for seam carving: brute-force, greedy, dynamic programming, and GPU-based parallel algorithms. We begin by presenting a theoretical overview of each technique, discussing their underlying principles and computational complexities. Subsequently, we delve into empirical evaluations, comparing the performance of these algorithms in terms of runtime efficiency. Our experimental results provide insights into the theoretical complexities of the design techniques.</summary></entry><entry><title type="html">Analysis of Diurnal Air Temperature Trends and Pattern Similarities in Highland and Lowland Stations of Italy and UK</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnalysisofDiurnalAirTemperatureTrendsandPatternSimilaritiesinHighlandandLowlandStationsofItalyandUK.html" rel="alternate" type="text/html" title="Analysis of Diurnal Air Temperature Trends and Pattern Similarities in Highland and Lowland Stations of Italy and UK" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnalysisofDiurnalAirTemperatureTrendsandPatternSimilaritiesinHighlandandLowlandStationsofItalyandUK</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnalysisofDiurnalAirTemperatureTrendsandPatternSimilaritiesinHighlandandLowlandStationsofItalyandUK.html">&lt;p&gt;In this paper, an analysis of hourly air temperatures in four groups of 32 stations of the UK highland (five stations), UK lowland (four stations), Italian highland (eleven stations), and Italian lowland (twelve stations) at various altitudes was conducted over the period from 2002 to 2021. The study aimed to examine the trends of each hour of the day in that period, over different averaging time windows (-10 day, -30 day, and -60 day). The trends were computed using the Mann-Kendall trend test and Sen’s slope estimator. The similarity of trends within and across the groups of stations was assessed using the hierarchical clustering with dynamic time warping technique. An additional analysis was conducted to show the correlation of trends among the group of stations using the correlation distance matrix. Hierarchical clustering and distance correlation analysis show trend similarities and correlations, also indicating dissimilarities among different groups. Using 30 day averages, significant warming trends in specific months at the Italian stations are evident, especially in February, July, August, and December. The UK highland stations did not show statistically significant trends, but clear pattern similarities were found within the groups, especially in certain months. The ultimate goal of this paper is to provide insights into temperature dynamics and climate change characteristics on regional and diurnal scales.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20726&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Chalachew Muluken Liyew. Rosa Meo, Stefano Ferraris, Elvira Di Nardo</name></author><category term="stat.AP" /><summary type="html">In this paper, an analysis of hourly air temperatures in four groups of 32 stations of the UK highland (five stations), UK lowland (four stations), Italian highland (eleven stations), and Italian lowland (twelve stations) at various altitudes was conducted over the period from 2002 to 2021. The study aimed to examine the trends of each hour of the day in that period, over different averaging time windows (-10 day, -30 day, and -60 day). The trends were computed using the Mann-Kendall trend test and Sen’s slope estimator. The similarity of trends within and across the groups of stations was assessed using the hierarchical clustering with dynamic time warping technique. An additional analysis was conducted to show the correlation of trends among the group of stations using the correlation distance matrix. Hierarchical clustering and distance correlation analysis show trend similarities and correlations, also indicating dissimilarities among different groups. Using 30 day averages, significant warming trends in specific months at the Italian stations are evident, especially in February, July, August, and December. The UK highland stations did not show statistically significant trends, but clear pattern similarities were found within the groups, especially in certain months. The ultimate goal of this paper is to provide insights into temperature dynamics and climate change characteristics on regional and diurnal scales.</summary></entry><entry><title type="html">Analysis of Hopfield Model as Associative Memory</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnalysisofHopfieldModelasAssociativeMemory.html" rel="alternate" type="text/html" title="Analysis of Hopfield Model as Associative Memory" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnalysisofHopfieldModelasAssociativeMemory</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnalysisofHopfieldModelasAssociativeMemory.html">&lt;p&gt;This article delves into the Hopfield neural network model, drawing inspiration from biological neural systems. The exploration begins with an overview of the model’s foundations, incorporating insights from mechanical statistics to deepen our understanding. Focusing on audio retrieval, the study demonstrates the Hopfield model’s associative memory capabilities. Through practical implementation, the network is trained to retrieve different patterns.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.04264&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Matteo Silvestri</name></author><summary type="html">This article delves into the Hopfield neural network model, drawing inspiration from biological neural systems. The exploration begins with an overview of the model’s foundations, incorporating insights from mechanical statistics to deepen our understanding. Focusing on audio retrieval, the study demonstrates the Hopfield model’s associative memory capabilities. Through practical implementation, the network is trained to retrieve different patterns.</summary></entry><entry><title type="html">Analyzing Multi-Stage Loss Curve: Plateau and Descent Mechanisms in Neural Networks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnalyzingMultiStageLossCurvePlateauandDescentMechanismsinNeuralNetworks.html" rel="alternate" type="text/html" title="Analyzing Multi-Stage Loss Curve: Plateau and Descent Mechanisms in Neural Networks" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnalyzingMultiStageLossCurvePlateauandDescentMechanismsinNeuralNetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnalyzingMultiStageLossCurvePlateauandDescentMechanismsinNeuralNetworks.html">&lt;p&gt;The multi-stage phenomenon in the training loss curves of neural networks has been widely observed, reflecting the non-linearity and complexity inherent in the training process. In this work, we investigate the training dynamics of neural networks (NNs), with particular emphasis on the small initialization regime and identify three distinct stages observed in the loss curve during training: initial plateau stage, initial descent stage, and secondary plateau stage. Through rigorous analysis, we reveal the underlying challenges causing slow training during the plateau stages. Building on existing work, we provide a more detailed proof for the initial plateau. This is followed by a comprehensive analysis of the dynamics in the descent stage. Furthermore, we explore the mechanisms that enable the network to overcome the prolonged secondary plateau stage, supported by both experimental evidence and heuristic reasoning. Finally, to better understand the relationship between global training trends and local parameter adjustments, we employ the Wasserstein distance to capture the microscopic evolution of weight amplitude distribution.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20119&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zheng-An Chen, Tao Luo, GuiHong Wang</name></author><summary type="html">The multi-stage phenomenon in the training loss curves of neural networks has been widely observed, reflecting the non-linearity and complexity inherent in the training process. In this work, we investigate the training dynamics of neural networks (NNs), with particular emphasis on the small initialization regime and identify three distinct stages observed in the loss curve during training: initial plateau stage, initial descent stage, and secondary plateau stage. Through rigorous analysis, we reveal the underlying challenges causing slow training during the plateau stages. Building on existing work, we provide a more detailed proof for the initial plateau. This is followed by a comprehensive analysis of the dynamics in the descent stage. Furthermore, we explore the mechanisms that enable the network to overcome the prolonged secondary plateau stage, supported by both experimental evidence and heuristic reasoning. Finally, to better understand the relationship between global training trends and local parameter adjustments, we employ the Wasserstein distance to capture the microscopic evolution of weight amplitude distribution.</summary></entry><entry><title type="html">An approach to hummed-tune and song sequences matching</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/Anapproachtohummedtuneandsongsequencesmatching.html" rel="alternate" type="text/html" title="An approach to hummed-tune and song sequences matching" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/Anapproachtohummedtuneandsongsequencesmatching</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/Anapproachtohummedtuneandsongsequencesmatching.html">&lt;p&gt;Melody stuck in your head, also known as “earworm”, is tough to get rid of, unless you listen to it again or sing it out loud. But what if you can not find the name of that song? It must be an intolerable feeling. Recognizing a song name base on humming sound is not an easy task for a human being and should be done by machines. However, there is no research paper published about hum tune recognition. Adapting from Hum2Song Zalo AI Challenge 2021 - a competition about querying the name of a song by user’s giving humming tune, which is similar to Google’s Hum to Search. This paper covers details about the pre-processed data from the original type (mp3) to usable form for training and inference. In training an embedding model for the feature extraction phase, we ran experiments with some states of the art, such as ResNet, VGG, AlexNet, MobileNetV2. And for the inference phase, we use the Faiss module to effectively search for a song that matched the sequence of humming sound. The result comes at nearly 94\% in MRR@10 metric on the public test set, along with the top 1 result on the public leaderboard.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20352&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Loc Bao Pham, Huong Hoang Luong, Phu Thien Tran, Phuc Hoang Ngo, Vi Hoang Nguyen, Thinh Nguyen</name></author><summary type="html">Melody stuck in your head, also known as “earworm”, is tough to get rid of, unless you listen to it again or sing it out loud. But what if you can not find the name of that song? It must be an intolerable feeling. Recognizing a song name base on humming sound is not an easy task for a human being and should be done by machines. However, there is no research paper published about hum tune recognition. Adapting from Hum2Song Zalo AI Challenge 2021 - a competition about querying the name of a song by user’s giving humming tune, which is similar to Google’s Hum to Search. This paper covers details about the pre-processed data from the original type (mp3) to usable form for training and inference. In training an embedding model for the feature extraction phase, we ran experiments with some states of the art, such as ResNet, VGG, AlexNet, MobileNetV2. And for the inference phase, we use the Faiss module to effectively search for a song that matched the sequence of humming sound. The result comes at nearly 94\% in MRR@10 metric on the public test set, along with the top 1 result on the public leaderboard.</summary></entry><entry><title type="html">Anchored Learning for On-the-Fly Adaptation – Extended Technical Report</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnchoredLearningforOntheFlyAdaptationExtendedTechnicalReport.html" rel="alternate" type="text/html" title="Anchored Learning for On-the-Fly Adaptation – Extended Technical Report" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnchoredLearningforOntheFlyAdaptationExtendedTechnicalReport</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnchoredLearningforOntheFlyAdaptationExtendedTechnicalReport.html">&lt;p&gt;This study presents “anchor critics”, a novel strategy for enhancing the robustness of reinforcement learning (RL) agents in crossing the sim-to-real gap. While RL agents can be successfully trained in simulation, they often encounter difficulties such as unpredictability, inefficient power consumption, and operational failures when deployed in real-world scenarios. We identify that naive fine-tuning approaches lead to catastrophic forgetting, where policies maintain high rewards on frequently encountered states but lose performance on rarer, yet critical scenarios. Our method maximizes multiple Q-values across domains, ensuring high performance in both simulation and reality. Evaluations demonstrate that our approach enables behavior retention in sim-to-sim gymnasium tasks and in sim-to-real scenarios with racing quadrotors, achieving a near-50% reduction in power consumption while maintaining controllable, stable flight. We also contribute SwannFlight, an open-source firmware for testing adaptation techniques on real robots.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2301.06987&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bassel El Mabsout, Shahin Roozkhosh, Siddharth Mysore, Kate Saenko, Renato Mancuso</name></author><summary type="html">This study presents “anchor critics”, a novel strategy for enhancing the robustness of reinforcement learning (RL) agents in crossing the sim-to-real gap. While RL agents can be successfully trained in simulation, they often encounter difficulties such as unpredictability, inefficient power consumption, and operational failures when deployed in real-world scenarios. We identify that naive fine-tuning approaches lead to catastrophic forgetting, where policies maintain high rewards on frequently encountered states but lose performance on rarer, yet critical scenarios. Our method maximizes multiple Q-values across domains, ensuring high performance in both simulation and reality. Evaluations demonstrate that our approach enables behavior retention in sim-to-sim gymnasium tasks and in sim-to-real scenarios with racing quadrotors, achieving a near-50% reduction in power consumption while maintaining controllable, stable flight. We also contribute SwannFlight, an open-source firmware for testing adaptation techniques on real robots.</summary></entry><entry><title type="html">A new class of splitting methods that preserve ergodicity and exponential integrability for stochastic Langevin equation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnewclassofsplittingmethodsthatpreserveergodicityandexponentialintegrabilityforstochasticLangevinequation.html" rel="alternate" type="text/html" title="A new class of splitting methods that preserve ergodicity and exponential integrability for stochastic Langevin equation" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnewclassofsplittingmethodsthatpreserveergodicityandexponentialintegrabilityforstochasticLangevinequation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnewclassofsplittingmethodsthatpreserveergodicityandexponentialintegrabilityforstochasticLangevinequation.html">&lt;p&gt;In this paper, we propose a new class of splitting methods to solve the stochastic Langevin equation, which can simultaneously preserve the ergodicity and exponential integrability of the original equation. The central idea is to extract a stochastic subsystem that possesses the strict dissipation from the original equation, which is inspired by the inheritance of the Lyapunov structure for obtaining the ergodicity. We prove that the exponential moment of the numerical solution is bounded, thus validating the exponential integrability of the proposed methods. Further, we show that under moderate verifiable conditions, the methods have the first-order convergence in both strong and weak senses, and we present several concrete splitting schemes based on the methods. The splitting strategy of methods can be readily extended to construct conformal symplectic methods and high-order methods that preserve both the ergodicity and the exponential integrability, as demonstrated in numerical experiments. Our numerical experiments also show that the proposed methods have good performance in the long-time simulation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20938&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Chuchu Chen, Tonghe Dang, Jialin Hong, Fengshan Zhang</name></author><summary type="html">In this paper, we propose a new class of splitting methods to solve the stochastic Langevin equation, which can simultaneously preserve the ergodicity and exponential integrability of the original equation. The central idea is to extract a stochastic subsystem that possesses the strict dissipation from the original equation, which is inspired by the inheritance of the Lyapunov structure for obtaining the ergodicity. We prove that the exponential moment of the numerical solution is bounded, thus validating the exponential integrability of the proposed methods. Further, we show that under moderate verifiable conditions, the methods have the first-order convergence in both strong and weak senses, and we present several concrete splitting schemes based on the methods. The splitting strategy of methods can be readily extended to construct conformal symplectic methods and high-order methods that preserve both the ergodicity and the exponential integrability, as demonstrated in numerical experiments. Our numerical experiments also show that the proposed methods have good performance in the long-time simulation.</summary></entry><entry><title type="html">A new lower bound for the density of planar Sets avoiding Unit Distances</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnewlowerboundforthedensityofplanarSetsavoidingUnitDistances.html" rel="alternate" type="text/html" title="A new lower bound for the density of planar Sets avoiding Unit Distances" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnewlowerboundforthedensityofplanarSetsavoidingUnitDistances</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnewlowerboundforthedensityofplanarSetsavoidingUnitDistances.html">&lt;p&gt;In a recently published article by G. Ambrus et al. a new upper bound for the density of an unit avoiding, periodic set is given as $0.2470$, the first upper bound $&amp;lt; 1/4$. A construction of Croft 1967 gave a lower bound $\delta_C = 0.22936$ for the density. To this date, no better construction with a higher lower bound has been given. In this article I give a construction planar sets with a higher density than Croft’s tortoises. No explicit value for this density is given, it’s just shown that Croft’s density is a local minima of the density of a here constructed 1-parameter family of planar sets. So the densities are $&amp;gt; \delta_C$.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2408.10076&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Helmut Ruhland</name></author><summary type="html">In a recently published article by G. Ambrus et al. a new upper bound for the density of an unit avoiding, periodic set is given as $0.2470$, the first upper bound $&amp;lt; 1/4$. A construction of Croft 1967 gave a lower bound $\delta_C = 0.22936$ for the density. To this date, no better construction with a higher lower bound has been given. In this article I give a construction planar sets with a higher density than Croft’s tortoises. No explicit value for this density is given, it’s just shown that Croft’s density is a local minima of the density of a here constructed 1-parameter family of planar sets. So the densities are $&amp;gt; \delta_C$.</summary></entry><entry><title type="html">Annotation Efficiency: Identifying Hard Samples via Blocked Sparse Linear Bandits</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnnotationEfficiencyIdentifyingHardSamplesviaBlockedSparseLinearBandits.html" rel="alternate" type="text/html" title="Annotation Efficiency: Identifying Hard Samples via Blocked Sparse Linear Bandits" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnnotationEfficiencyIdentifyingHardSamplesviaBlockedSparseLinearBandits</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AnnotationEfficiencyIdentifyingHardSamplesviaBlockedSparseLinearBandits.html">&lt;p&gt;This paper considers the problem of annotating datapoints using an expert with only a few annotation rounds in a label-scarce setting. We propose soliciting reliable feedback on difficulty in annotating a datapoint from the expert in addition to ground truth label. Existing literature in active learning or coreset selection turns out to be less relevant to our setting since they presume the existence of a reliable trained model, which is absent in the label-scarce regime. However, the literature on coreset selection emphasizes the presence of difficult data points in the training set to perform supervised learning in downstream tasks (Mindermann et al., 2022). Therefore, for a given fixed annotation budget of $\mathsf{T}$ rounds, we model the sequential decision-making problem of which (difficult) datapoints to choose for annotation in a sparse linear bandits framework with the constraint that no arm can be pulled more than once (blocking constraint). With mild assumptions on the datapoints, our (computationally efficient) Explore-Then-Commit algorithm BSLB achieves a regret guarantee of $\widetilde{\mathsf{O}}(k^{\frac{1}{3}} \mathsf{T}^{\frac{2}{3}} +k^{-\frac{1}{2}} \beta_k + k^{-\frac{1}{12}} \beta_k^{\frac{1}{2}}\mathsf{T}^{\frac{5}{6}})$ where the unknown parameter vector has tail magnitude $\beta_k$ at sparsity level $k$. To this end, we show offline statistical guarantees of Lasso estimator with mild Restricted Eigenvalue (RE) condition that is also robust to sparsity. Finally, we propose a meta-algorithm C-BSLB that does not need knowledge of the optimal sparsity parameters at a no-regret cost. We demonstrate the efficacy of our BSLB algorithm for annotation in the label-scarce setting for an image classification task on the PASCAL-VOC dataset, where we use real-world annotation difficulty scores.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.20041&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Adit Jain, Soumyabrata Pal, Sunav Choudhary, Ramasuri Narayanam, Vikram Krishnamurthy</name></author><summary type="html">This paper considers the problem of annotating datapoints using an expert with only a few annotation rounds in a label-scarce setting. We propose soliciting reliable feedback on difficulty in annotating a datapoint from the expert in addition to ground truth label. Existing literature in active learning or coreset selection turns out to be less relevant to our setting since they presume the existence of a reliable trained model, which is absent in the label-scarce regime. However, the literature on coreset selection emphasizes the presence of difficult data points in the training set to perform supervised learning in downstream tasks (Mindermann et al., 2022). Therefore, for a given fixed annotation budget of $\mathsf{T}$ rounds, we model the sequential decision-making problem of which (difficult) datapoints to choose for annotation in a sparse linear bandits framework with the constraint that no arm can be pulled more than once (blocking constraint). With mild assumptions on the datapoints, our (computationally efficient) Explore-Then-Commit algorithm BSLB achieves a regret guarantee of $\widetilde{\mathsf{O}}(k^{\frac{1}{3}} \mathsf{T}^{\frac{2}{3}} +k^{-\frac{1}{2}} \beta_k + k^{-\frac{1}{12}} \beta_k^{\frac{1}{2}}\mathsf{T}^{\frac{5}{6}})$ where the unknown parameter vector has tail magnitude $\beta_k$ at sparsity level $k$. To this end, we show offline statistical guarantees of Lasso estimator with mild Restricted Eigenvalue (RE) condition that is also robust to sparsity. Finally, we propose a meta-algorithm C-BSLB that does not need knowledge of the optimal sparsity parameters at a no-regret cost. We demonstrate the efficacy of our BSLB algorithm for annotation in the label-scarce setting for an image classification task on the PASCAL-VOC dataset, where we use real-world annotation difficulty scores.</summary></entry><entry><title type="html">An operator preconditioned combined field integral equation for electromagnetic scattering</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/Anoperatorpreconditionedcombinedfieldintegralequationforelectromagneticscattering.html" rel="alternate" type="text/html" title="An operator preconditioned combined field integral equation for electromagnetic scattering" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/Anoperatorpreconditionedcombinedfieldintegralequationforelectromagneticscattering</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/Anoperatorpreconditionedcombinedfieldintegralequationforelectromagneticscattering.html">&lt;p&gt;This paper aims to address two issues of integral equations for the scattering of time-harmonic electromagnetic waves by a perfect electric conductor with Lipschitz continuous boundary: ill-conditioned {boundary element Galerkin matrices} on fine meshes and instability at spurious resonant frequencies. The remedy to ill-conditioned matrices is operator preconditioning, and resonant instability is eliminated by means of a combined field integral equation. Exterior traces of single and double layer potentials are complemented by their interior counterparts for a purely imaginary wave number. We derive the corresponding variational formulation in the natural trace space for electromagnetic fields and establish its well-posedness for all wave numbers. A Galerkin discretization scheme is employed using conforming edge boundary elements on dual meshes, which produces well-conditioned discrete linear systems of the variational formulation. Some numerical results are also provided to support the numerical analysis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.02289&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Van Chien Le, Kristof Cools</name></author><summary type="html">This paper aims to address two issues of integral equations for the scattering of time-harmonic electromagnetic waves by a perfect electric conductor with Lipschitz continuous boundary: ill-conditioned {boundary element Galerkin matrices} on fine meshes and instability at spurious resonant frequencies. The remedy to ill-conditioned matrices is operator preconditioning, and resonant instability is eliminated by means of a combined field integral equation. Exterior traces of single and double layer potentials are complemented by their interior counterparts for a purely imaginary wave number. We derive the corresponding variational formulation in the natural trace space for electromagnetic fields and establish its well-posedness for all wave numbers. A Galerkin discretization scheme is employed using conforming edge boundary elements on dual meshes, which produces well-conditioned discrete linear systems of the variational formulation. Some numerical results are also provided to support the numerical analysis.</summary></entry><entry><title type="html">A novel decomposition to explain heterogeneity in observational and randomized studies of causality</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/Anoveldecompositiontoexplainheterogeneityinobservationalandrandomizedstudiesofcausality.html" rel="alternate" type="text/html" title="A novel decomposition to explain heterogeneity in observational and randomized studies of causality" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/Anoveldecompositiontoexplainheterogeneityinobservationalandrandomizedstudiesofcausality</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/Anoveldecompositiontoexplainheterogeneityinobservationalandrandomizedstudiesofcausality.html">&lt;p&gt;This paper introduces a novel decomposition framework to explain heterogeneity in causal effects observed across different studies, considering both observational and randomized settings. We present a formal decomposition of between-study heterogeneity, identifying sources of variability in treatment effects across studies. The proposed methodology allows for robust estimation of causal parameters under various assumptions, addressing differences in pre-treatment covariate distributions, mediating variables, and the outcome mechanism. Our approach is validated through a simulation study and applied to data from the Moving to Opportunity (MTO) study, demonstrating its practical relevance. This work contributes to the broader understanding of causal inference in multi-study environments, with potential applications in evidence synthesis and policy-making.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2208.05543&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Brian Gilbert, Ivan D{\i}az, Kara E. Rudolph, Tat-Thang Vo</name></author><category term="stat.ME" /><summary type="html">This paper introduces a novel decomposition framework to explain heterogeneity in causal effects observed across different studies, considering both observational and randomized settings. We present a formal decomposition of between-study heterogeneity, identifying sources of variability in treatment effects across studies. The proposed methodology allows for robust estimation of causal parameters under various assumptions, addressing differences in pre-treatment covariate distributions, mediating variables, and the outcome mechanism. Our approach is validated through a simulation study and applied to data from the Moving to Opportunity (MTO) study, demonstrating its practical relevance. This work contributes to the broader understanding of causal inference in multi-study environments, with potential applications in evidence synthesis and policy-making.</summary></entry><entry><title type="html">A novel polyhedral scaled boundary finite element method solving three-dimensional heat conduction problems</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/Anovelpolyhedralscaledboundaryfiniteelementmethodsolvingthreedimensionalheatconductionproblems.html" rel="alternate" type="text/html" title="A novel polyhedral scaled boundary finite element method solving three-dimensional heat conduction problems" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/Anovelpolyhedralscaledboundaryfiniteelementmethodsolvingthreedimensionalheatconductionproblems</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/Anovelpolyhedralscaledboundaryfiniteelementmethodsolvingthreedimensionalheatconductionproblems.html">&lt;p&gt;In this work, we derived the three-dimensional scaled boundary finite element formulation for thermal conduction problems. By introducing Wachspress shape functions, we proposed a novel polyhedral scaled boundary finite element method (PSBFEM) to address thermal conduction problems. The proposed method effectively addresses the challenges associated with complex geometries by integrating the polyhedral mesh and the octree mesh. The presented formulation handles both steady-state and transient thermal conduction analyses. Through a series of numerical examples, the accuracy and convergence of the proposed method were validated. The results demonstrate that mesh refinement leads to superior accuracy for the PSBFEM compared to the FEM. Moreover, Polyhedral elements provide an effective and efficient approach for complex simulations that substantially reduces computational costs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.15331&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mingjiao Yan, Yang Yang, Chao Su, Zongliang Zhang, Qingsong Duan, Dengmiao Hao</name></author><summary type="html">In this work, we derived the three-dimensional scaled boundary finite element formulation for thermal conduction problems. By introducing Wachspress shape functions, we proposed a novel polyhedral scaled boundary finite element method (PSBFEM) to address thermal conduction problems. The proposed method effectively addresses the challenges associated with complex geometries by integrating the polyhedral mesh and the octree mesh. The presented formulation handles both steady-state and transient thermal conduction analyses. Through a series of numerical examples, the accuracy and convergence of the proposed method were validated. The results demonstrate that mesh refinement leads to superior accuracy for the PSBFEM compared to the FEM. Moreover, Polyhedral elements provide an effective and efficient approach for complex simulations that substantially reduces computational costs.</summary></entry><entry><title type="html">Antigen-Specific Antibody Design via Direct Energy-based Preference Optimization</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AntigenSpecificAntibodyDesignviaDirectEnergybasedPreferenceOptimization.html" rel="alternate" type="text/html" title="Antigen-Specific Antibody Design via Direct Energy-based Preference Optimization" /><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AntigenSpecificAntibodyDesignviaDirectEnergybasedPreferenceOptimization</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/10/29/AntigenSpecificAntibodyDesignviaDirectEnergybasedPreferenceOptimization.html">&lt;p&gt;Antibody design, a crucial task with significant implications across various disciplines such as therapeutics and biology, presents considerable challenges due to its intricate nature. In this paper, we tackle antigen-specific antibody sequence-structure co-design as an optimization problem towards specific preferences, considering both rationality and functionality. Leveraging a pre-trained conditional diffusion model that jointly models sequences and structures of antibodies with equivariant neural networks, we propose direct energy-based preference optimization to guide the generation of antibodies with both rational structures and considerable binding affinities to given antigens. Our method involves fine-tuning the pre-trained diffusion model using a residue-level decomposed energy preference. Additionally, we employ gradient surgery to address conflicts between various types of energy, such as attraction and repulsion. Experiments on RAbD benchmark show that our approach effectively optimizes the energy of generated antibodies and achieves state-of-the-art performance in designing high-quality antibodies with low total energy and high binding affinity simultaneously, demonstrating the superiority of our approach.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.16576&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xiangxin Zhou, Dongyu Xue, Ruizhe Chen, Zaixiang Zheng, Liang Wang, Quanquan Gu</name></author><summary type="html">Antibody design, a crucial task with significant implications across various disciplines such as therapeutics and biology, presents considerable challenges due to its intricate nature. In this paper, we tackle antigen-specific antibody sequence-structure co-design as an optimization problem towards specific preferences, considering both rationality and functionality. Leveraging a pre-trained conditional diffusion model that jointly models sequences and structures of antibodies with equivariant neural networks, we propose direct energy-based preference optimization to guide the generation of antibodies with both rational structures and considerable binding affinities to given antigens. Our method involves fine-tuning the pre-trained diffusion model using a residue-level decomposed energy preference. Additionally, we employ gradient surgery to address conflicts between various types of energy, such as attraction and repulsion. Experiments on RAbD benchmark show that our approach effectively optimizes the energy of generated antibodies and achieves state-of-the-art performance in designing high-quality antibodies with low total energy and high binding affinity simultaneously, demonstrating the superiority of our approach.</summary></entry></feed>